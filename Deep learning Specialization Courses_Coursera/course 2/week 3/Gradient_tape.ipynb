{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdcde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant imports \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96ba88",
   "metadata": {},
   "source": [
    "- gradient tape is a way that let us optimize a function with respect to certain parameter(s)\n",
    "    - its name is derived from the old casette tapes where you record the events and then it has the ability to navigate forward or backward \n",
    "    - so we only record the forward propagation and it will record the operations in order and figure out by itself how to perform backprop (mine: using computation graph) and compute the gradients in the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40480219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n"
     ]
    }
   ],
   "source": [
    "# define the parameters we have \n",
    "w = tf.Variable(0, dtype = tf.float32) # we defined a variable with type float and initialized it with a value of 0\n",
    "# define the optimization algorithm that we will use \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c4fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfine the loop function \n",
    "def single_train_loop():\n",
    "    # open the tape to record the sequence of opetations needed to compute the function (forward prop)\n",
    "    with tf.GradientTape() as tape:\n",
    "        # in here we write how the function is calculated w.r.t its parameters (the forward prop step) for the gradient_tape to record them \n",
    "        cost = w ** 2 - 10 * w + 25\n",
    "    # we have to define what are the trainable variables (that we want to calculate their gradients)\n",
    "    trainable_variables = [w] # that  is why we had to define the w as a variable above \n",
    "    # Then we compute the gradients using the tape we opened, we give to it both the function and its trainable variables  \n",
    "    grads = tape.gradient(cost, trainable_variables)\n",
    "    # After calculating the gradients, we use the optimizer to carry out the update on the variables \n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables)) # we zip them to pair the list of gradients with their corresponding list of variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a94fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'a')\n",
      "(2, 'b')\n",
      "(3, 'c')\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = ['a','b','c']\n",
    "for item in zip(a,b):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d8e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999931>\n"
     ]
    }
   ],
   "source": [
    "# let's run one step of the loop and print the w\n",
    "single_train_loop()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5e46f",
   "metadata": {},
   "source": [
    "- we see that it increased a little bit (mine: the gradient tape carried out the backward prop to calculate the gradients based on the trainable parameters and update them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2514bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "# lets run it for 1000 steps \n",
    "num_iterations = 1000\n",
    "for i in range(num_iterations):\n",
    "    single_train_loop()\n",
    "    \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3eabcc",
   "metadata": {},
   "source": [
    "- we see that w is nearly 5 (which we knew was the minimum of the cost function)\n",
    "    - all we needed is to specify the cost function and the sequence in which the cost is calculated for the gradient tape -forward prop- and it will carry out the backward prop to calculate the gradients then update the parameters based on our specified optimization algorithm \n",
    "- that is why in TF we only have to implement the forward prop step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98c6c0",
   "metadata": {},
   "source": [
    "### another method \n",
    "- we will see another syntax to do the exact same thing above \n",
    "- and this time our cost function will not only be a function of the learnable parameters, but also of other parameters (like in real life where the cost function is a function of the learnable parameters, hyper parameters and the data)\n",
    "    - so if they change they will affect the cost function and we may end up with another values that minimize the cost (just like real life)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49959df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n"
     ]
    }
   ],
   "source": [
    "# define the parameters we have \n",
    "w = tf.Variable(0, dtype = tf.float32) # we defined a variable with type float and initialized it with a value of 0\n",
    "# define the optimization algorithm that we will use \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "# define another variable that exists in the cost function \n",
    "x = np.array([1.0, -10.0, 25.0], dtype = np.float32) # we defined it as a numpy array and not a tf variable \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793efae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999931>\n"
     ]
    }
   ],
   "source": [
    "def cost_fun():\n",
    "    return x[0] * w**2 + x[1] * w + x[2]\n",
    "    \n",
    "optimizer.minimize(cost_fun, [w]) # this is equivalent to the above single_train_loop\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b31db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets encapsulate the above in a function \n",
    "def training(x,w,optimizer,num_iterations = 1000):\n",
    "    def cost_fun():\n",
    "        return x[0] * w**2 + x[1] * w + x[2]\n",
    "    for i in range(num_iterations):\n",
    "        optimizer.minimize(cost_fun,[w])\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8b6331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = training(x,w,optimizer)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a48ac",
   "metadata": {},
   "source": [
    "we see it worked, let's try different values of x and see how they affect the minimum of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2022b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(0, dtype = tf.float32) # we defined a variable with type float and initialized it with a value of 0\n",
    "# define the optimization algorithm that we will use \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "# define another variable that exists in the cost function \n",
    "x = np.array([5.0, -7.0, 13.0], dtype = np.float32) # we defined it as a numpy array and not a tf variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d40775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.7>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = training(x,w,optimizer)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306d48b",
   "metadata": {},
   "source": [
    "- hopefully we have a sense of what TF can do, and all what we need is to specify the cost function (or the forward prop steps in general) and what are the variables that we want to optimise the function with respect to them, and it will figure out the backward prop \n",
    "- what actually happens is that it constructs a computation graph when we specify the forward prop sequence of operations "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
