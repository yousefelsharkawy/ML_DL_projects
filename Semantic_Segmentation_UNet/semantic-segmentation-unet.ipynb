{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9441013d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:15.940317Z",
     "iopub.status.busy": "2024-08-09T20:29:15.940051Z",
     "iopub.status.idle": "2024-08-09T20:29:25.788642Z",
     "shell.execute_reply": "2024-08-09T20:29:25.787640Z"
    },
    "papermill": {
     "duration": 9.859308,
     "end_time": "2024-08-09T20:29:25.790725",
     "exception": false,
     "start_time": "2024-08-09T20:29:15.931417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_masks.zip', '29bb3ece3180_11.jpg', 'train_masks.csv.zip', 'train.zip', 'metadata.csv.zip', 'sample_submission.csv.zip', 'test.zip', 'test_hq.zip', 'train_hq.zip']\n",
      "5088 5088\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input/carvana-image-masking-challenge/\"))\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "DATASET_DIR = '../input/carvana-image-masking-challenge/'\n",
    "WORKING_DIR = '/kaggle/working/'\n",
    "\n",
    "if len(os.listdir(WORKING_DIR)) <= 1:\n",
    "\n",
    "    with zipfile.ZipFile(DATASET_DIR + 'train.zip', 'r') as zip_file:\n",
    "        zip_file.extractall(WORKING_DIR)\n",
    "\n",
    "    with zipfile.ZipFile(DATASET_DIR + 'train_masks.zip', 'r') as zip_file:\n",
    "        zip_file.extractall(WORKING_DIR)\n",
    "    \n",
    "    print(\n",
    "        len(os.listdir(WORKING_DIR + 'train')),\n",
    "        len(os.listdir(WORKING_DIR + 'train_masks'))\n",
    "    )\n",
    "    \n",
    "    # Move some of the images (5088-4600) to the validation directory\n",
    "    train_dir = WORKING_DIR + 'train/'\n",
    "    val_dir = WORKING_DIR + 'val/'\n",
    "    os.mkdir(val_dir)\n",
    "    for file in sorted(os.listdir(train_dir))[4600:]:\n",
    "      shutil.move(train_dir + file, val_dir)\n",
    "    \n",
    "    # move their masks as well\n",
    "    masks_dir = WORKING_DIR + 'train_masks/'\n",
    "    val_masks_dir = WORKING_DIR + 'val_masks/'\n",
    "    os.mkdir(val_masks_dir)\n",
    "    for file in sorted(os.listdir(masks_dir))[4600:]:\n",
    "      shutil.move(masks_dir + file, val_masks_dir)\n",
    "\n",
    "    os.mkdir(WORKING_DIR + 'saved_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877ed04",
   "metadata": {
    "papermill": {
     "duration": 0.007012,
     "end_time": "2024-08-09T20:29:25.805221",
     "exception": false,
     "start_time": "2024-08-09T20:29:25.798209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48175efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:25.820462Z",
     "iopub.status.busy": "2024-08-09T20:29:25.820187Z",
     "iopub.status.idle": "2024-08-09T20:29:30.985440Z",
     "shell.execute_reply": "2024-08-09T20:29:30.984673Z"
    },
    "papermill": {
     "duration": 5.175433,
     "end_time": "2024-08-09T20:29:30.987681",
     "exception": false,
     "start_time": "2024-08-09T20:29:25.812248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_accuracy_binary(loader,model,device):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
    "\n",
    "    print(\n",
    "        f'Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}'\n",
    "    )\n",
    "    print(f'Dice score: {dice_score/len(loader)}')\n",
    "    model.train()\n",
    "    return dice_score/len(loader)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def save_predictions_as_imgs(loader, model, device, folder=\"saved_images/\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    num_examples = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        \n",
    "        for i in range(preds.size(0)):  # Iterate over each image in the batch\n",
    "            torchvision.utils.save_image(preds[i], os.path.join(folder, f\"pred_{idx}_{i}.png\"))\n",
    "            torchvision.utils.save_image(y[i].unsqueeze(0), os.path.join(folder, f\"mask_{idx}_{i}.png\"))  # unsqueeze adds a channel dimension to the tensor\n",
    "            num_examples += 1\n",
    "            if num_examples == 10:\n",
    "                model.train()\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21995003",
   "metadata": {
    "papermill": {
     "duration": 0.006905,
     "end_time": "2024-08-09T20:29:31.002232",
     "exception": false,
     "start_time": "2024-08-09T20:29:30.995327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Tips\n",
    "    - What is dice score?\n",
    "        - Dice score is a metric to evaluate the performance of a segmentation model. It is defined as the intersection of the predicted mask and the ground truth mask divided by the average of the number of pixels in the predicted mask and the ground truth mask.\n",
    "        - formula : $Dice = \\frac{2 \\times |X \\cap Y|}{|X| + |Y|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9fd81",
   "metadata": {
    "papermill": {
     "duration": 0.006881,
     "end_time": "2024-08-09T20:29:31.016123",
     "exception": false,
     "start_time": "2024-08-09T20:29:31.009242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Semantic Segmentation with UNet\n",
    "\n",
    "- We will build a model from scratch, and set up the data loading pipeline which will contain data augmentation using `albumentations` library.\n",
    "\n",
    "- we will train the model on the [Carvana Image Masking Challenge](https://www.kaggle.com/c/carvana-image-masking-challenge) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b022db",
   "metadata": {
    "papermill": {
     "duration": 0.006862,
     "end_time": "2024-08-09T20:29:31.029958",
     "exception": false,
     "start_time": "2024-08-09T20:29:31.023096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Unet Architecture\n",
    "\n",
    "- The U-Net architecture is introduced in the paper titled [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) by Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\n",
    "    - they designed this architecture for biomedical image segmentation, but it can be used for any image segmentation task.\n",
    "\n",
    "- The U-Net architecture is symmetric, and it consists of two parts:\n",
    "    - Contracting path (Encoder)\n",
    "        - The contracting path is a typical convolutional network that consists of repeated application of convolutions, downsampling, and ReLU activations.\n",
    "    - Expansive path (Decoder)\n",
    "        - The expansive path consists of upsampling, concatenation with the corresponding cropped feature map from the contracting path, followed by convolution, and ReLU activation.\n",
    "    - Skip connections\n",
    "        - The skip connections are the concatenation of feature maps from contracting path with the corresponding feature maps in the expansive path.\n",
    "\n",
    "- The U-Net architecture is shown below:\n",
    "    - ![unet](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
    "\n",
    "- the pattern they followed in the paper is:\n",
    "    - 2 3x3 convolutions with ReLU activation followed by 2x2 max pooling with stride 2 for downsampling. (this is repeated 4 times)\n",
    "    - 2 3x3 convolutions with ReLU activation for upsampling. (this is repeated 4 times)\n",
    "    - skip connections are added between the corresponding feature maps in the contracting and expansive paths.\n",
    "    - There is a 1x1 convolution at the end of the network, which is used to map each 64-component feature vector to the desired number of classes (1x1 convolution preserves the area and changes the depth).\n",
    "\n",
    "- Some things we will do that are different from the original U-Net architecture:\n",
    "    - We will use `Same` padding in the convolution layers to keep the spatial dimensions the same, unlike the original architecture which uses `Valid` padding.\n",
    "        - as a result, They did cropping in the skip connections to concatenate feature maps of the same spatial level (because the dimensions did not match)\n",
    "        - but for use the dimensions will match, so we will not need cropping and we will simply add the feature maps from the contracting path to the expansive path.\n",
    "        - The carvana winners used `Same` padding in their implementation (so it doesn't seem to affect the performance much).\n",
    "    - Instead of the transposed convolution, we might use a bilinear upsampling layer\n",
    "        - This layer will upsample the input by a factor of 2. It uses bilinear interpolation to upsample the input.\n",
    "        - in Gans, in Pro Gans, they used bilinear upsampling instead of transposed convolution because it produces better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06305dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:31.045510Z",
     "iopub.status.busy": "2024-08-09T20:29:31.044918Z",
     "iopub.status.idle": "2024-08-09T20:29:31.049238Z",
     "shell.execute_reply": "2024-08-09T20:29:31.048361Z"
    },
    "papermill": {
     "duration": 0.014101,
     "end_time": "2024-08-09T20:29:31.051142",
     "exception": false,
     "start_time": "2024-08-09T20:29:31.037041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6485d34",
   "metadata": {
    "papermill": {
     "duration": 0.006952,
     "end_time": "2024-08-09T20:29:31.065148",
     "exception": false,
     "start_time": "2024-08-09T20:29:31.058196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "- we will use the class `DoubleConv` to define a block of two 3x3 convolutions with ReLU activation. because this block is used multiple times in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0054d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:31.080476Z",
     "iopub.status.busy": "2024-08-09T20:29:31.080221Z",
     "iopub.status.idle": "2024-08-09T20:29:32.383837Z",
     "shell.execute_reply": "2024-08-09T20:29:32.382903Z"
    },
    "papermill": {
     "duration": 1.313983,
     "end_time": "2024-08-09T20:29:32.386232",
     "exception": false,
     "start_time": "2024-08-09T20:29:31.072249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 160, 160])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1,bias=False), # same padding, we set bias to False because we will use a batchnorm layer after this (which cancels out the bias)\n",
    "            nn.BatchNorm2d(out_channels), # batchnorm layer, not in the original U-Net\n",
    "            nn.ReLU(inplace=True), # inplace true means it will modify the input directly, without allocating any additional output. It can sometimes slightly decrease the memory usage, but may not always be a valid operation (because the original input is destroyed)\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        # the channels of the Unet are hardcoded, but you can change them to your needs\n",
    "        self.features = [64, 128, 256, 512] \n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # max pooling layer\n",
    "\n",
    "        # Encoder\n",
    "        for feature in self.features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            # update in_channels for the next layer\n",
    "            in_channels = feature\n",
    "            # we will not add the max pooling layer here, because we want to save the outputs before the max pooling layer (for the skip connections)\n",
    "\n",
    "        # the bottleneck layer (which is the bottom of the U)\n",
    "        # this is a single level with no symmetrical upsampling (that is why we did not include 1024 in the features list) and we did it outside the loops\n",
    "        self.bottleneck = DoubleConv(self.features[-1], self.features[-1]*2) \n",
    "\n",
    "        # Decoder\n",
    "        for feature in reversed(self.features):\n",
    "            # the shift introduced because of the bottleneck layer will cause each transpose convolution to take 2*feature as input (check the diagram)\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))  # kernel size 2, stride will double the width and height\n",
    "            # the double conv will take double the number of features as input because of the concatenation of the skip connection\n",
    "            self.ups.append(DoubleConv(feature*2, feature)) \n",
    "\n",
    "        # the final layer, which is a 1x1 convolution\n",
    "        self.final_layer = nn.Conv2d(self.features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Encoder\n",
    "        for level in self.downs:\n",
    "            x = level(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # reverse the skip connections (because we want to concatenate the layers in the reverse order)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(0, len(self.ups), 2): # iterate with a step of 2 because we have 2 layers for each level\n",
    "            x = self.ups[i](x)\n",
    "            # add the skip connection\n",
    "            skip_connection = skip_connections[i//2] # i//2 because we move here with a step of 2, but we want to move with a step of 1 in the skip connections list\n",
    "            \n",
    "            # Defensive Act: check if the dimensions of the skip connection and the x are not the same (that might result when choosing an input image which size is not divisible by 2^4), which will lead to an odd number of pixels, which max pool will floor to the nearest integer, and the transpose convolution will double the width and height to that integer, which will cause the dimensions to be different by 1 pixel\n",
    "            if x.shape != skip_connection.shape:\n",
    "                # interpolate x to the size of the skip connection\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            # concatenate the skip connection\n",
    "            x = torch.cat((skip_connection, x), dim=1) # dim=1 because we want to concatenate along the channels\n",
    "            x = self.ups[i+1](x) # the double conv layer\n",
    "\n",
    "        # Final layer\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# test the model\n",
    "dummy = torch.randn((3, 3, 160, 160))                        \n",
    "model = UNet(in_channels=3, out_channels=1)\n",
    "model(dummy).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877cc7c",
   "metadata": {
    "papermill": {
     "duration": 0.009107,
     "end_time": "2024-08-09T20:29:32.404304",
     "exception": false,
     "start_time": "2024-08-09T20:29:32.395197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Tips\n",
    "    - We used something called ModuleList to store the layers in the network. this is different from a regular python list because it registers the layers in the network, so they can be used in the forward pass.\n",
    "        - long story short, it is compatible with the PyTorch model functions and used when we want to store layers in a list.\n",
    "\n",
    "    - ConvTranspose2d is used for upsampling in the expansive path. it takes the following arguments:\n",
    "\n",
    "    - We had to choose an input size that is divisible by 16 (because we will divide by 2 four times in the contracting path).\n",
    "        - so 2*2*2*2 = 16\n",
    "        - the reason for that is that if it is not divisible by 16, we will have an odd number of pixels, and the max pool layer will floor it when it reduces the size by half, leading to a mismatch in the dimensions when we add the skip connections later\n",
    "        - other thing we can do is to resize the skip connections or the upsampled feature maps to match the dimensions. (which we did above)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ba760",
   "metadata": {
    "papermill": {
     "duration": 0.006971,
     "end_time": "2024-08-09T20:29:32.419294",
     "exception": false,
     "start_time": "2024-08-09T20:29:32.412323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b38805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:32.434982Z",
     "iopub.status.busy": "2024-08-09T20:29:32.434659Z",
     "iopub.status.idle": "2024-08-09T20:29:32.443744Z",
     "shell.execute_reply": "2024-08-09T20:29:32.442899Z"
    },
    "papermill": {
     "duration": 0.019192,
     "end_time": "2024-08-09T20:29:32.445734",
     "exception": false,
     "start_time": "2024-08-09T20:29:32.426542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        # read the image and the mask (label), and store them in the form of numpy arrays (for the albumentations library)\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\")) # we might not need to do this because the images are loaded as RGB by default\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) # convert(\"L\") will convert the image to grayscale\n",
    "        # binarize the mask\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        # apply the transformations if they exist\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "            \n",
    "        return image, mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f237e6da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:32.461117Z",
     "iopub.status.busy": "2024-08-09T20:29:32.460822Z",
     "iopub.status.idle": "2024-08-09T20:29:33.806555Z",
     "shell.execute_reply": "2024-08-09T20:29:33.805574Z"
    },
    "papermill": {
     "duration": 1.355985,
     "end_time": "2024-08-09T20:29:33.808891",
     "exception": false,
     "start_time": "2024-08-09T20:29:32.452906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_image_dir = '/kaggle/working/train'\n",
    "train_mask_dir = '/kaggle/working/train_masks'\n",
    "val_image_dir = '/kaggle/working/val'\n",
    "val_mask_dir = '/kaggle/working/val_masks'\n",
    "## Dataset hyperparameters\n",
    "batch_size = 16\n",
    "image_height = 320\n",
    "image_width = 480\n",
    "pin_memory = True\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=image_height, width=image_width),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        # this will only divide by 255 (since mean = 0 and std = 1)\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# for the validation transforms, we will only resize and normalize without any augmentations\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=image_height, width=image_width),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = CarvanaDataset(image_dir=train_image_dir,mask_dir=train_mask_dir, transform=train_transform)\n",
    "val_dataset = CarvanaDataset(image_dir=val_image_dir,mask_dir=val_mask_dir, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e13059f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:33.825204Z",
     "iopub.status.busy": "2024-08-09T20:29:33.824767Z",
     "iopub.status.idle": "2024-08-09T20:29:33.830696Z",
     "shell.execute_reply": "2024-08-09T20:29:33.829811Z"
    },
    "papermill": {
     "duration": 0.015951,
     "end_time": "2024-08-09T20:29:33.832495",
     "exception": false,
     "start_time": "2024-08-09T20:29:33.816544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4600, 488)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3a1b81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:33.848177Z",
     "iopub.status.busy": "2024-08-09T20:29:33.847913Z",
     "iopub.status.idle": "2024-08-09T20:29:33.953207Z",
     "shell.execute_reply": "2024-08-09T20:29:33.952279Z"
    },
    "papermill": {
     "duration": 0.115478,
     "end_time": "2024-08-09T20:29:33.955273",
     "exception": false,
     "start_time": "2024-08-09T20:29:33.839795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 320, 480]), torch.Size([320, 480]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape, train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cee045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:33.972220Z",
     "iopub.status.busy": "2024-08-09T20:29:33.971486Z",
     "iopub.status.idle": "2024-08-09T20:29:33.976208Z",
     "shell.execute_reply": "2024-08-09T20:29:33.975392Z"
    },
    "papermill": {
     "duration": 0.015027,
     "end_time": "2024-08-09T20:29:33.978070",
     "exception": false,
     "start_time": "2024-08-09T20:29:33.963043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a65225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:33.993948Z",
     "iopub.status.busy": "2024-08-09T20:29:33.993671Z",
     "iopub.status.idle": "2024-08-09T20:29:34.715978Z",
     "shell.execute_reply": "2024-08-09T20:29:34.714857Z"
    },
    "papermill": {
     "duration": 0.732586,
     "end_time": "2024-08-09T20:29:34.718133",
     "exception": false,
     "start_time": "2024-08-09T20:29:33.985547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 320, 480]) torch.Size([16, 320, 480])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape, masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017c805",
   "metadata": {
    "papermill": {
     "duration": 0.007447,
     "end_time": "2024-08-09T20:29:34.734644",
     "exception": false,
     "start_time": "2024-08-09T20:29:34.727197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d483f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:34.750917Z",
     "iopub.status.busy": "2024-08-09T20:29:34.750565Z",
     "iopub.status.idle": "2024-08-09T20:29:35.068905Z",
     "shell.execute_reply": "2024-08-09T20:29:35.067910Z"
    },
    "papermill": {
     "duration": 0.329216,
     "end_time": "2024-08-09T20:29:35.071323",
     "exception": false,
     "start_time": "2024-08-09T20:29:34.742107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_accuracy = 0\n",
    "\n",
    "## Model hyperparameters\n",
    "in_channels = 3\n",
    "out_channels = 1\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 20\n",
    "\n",
    "model = UNet(in_channels=in_channels, out_channels=out_channels).to(device)\n",
    "# criterion = nn.CrossEntropyLoss() # we will use this loss function if we have multiple classes (out_channels > 1)\n",
    "criterion = nn.BCEWithLogitsLoss() # binary cross entropy with logits loss (it expects the logits, and it will apply the sigmoid function by itself), the sigmoid is applied because we have only 1 channel, and each pixel will have a value between 0 and 1 that represents the probability of that pixel being class 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scalar\n",
    "scaler = torch.cuda.amp.GradScaler() # this will help us to use mixed precision training\n",
    "\n",
    "use_scheduler = True\n",
    "if use_scheduler:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1)\n",
    "\n",
    "\n",
    "# if we want to load a model and continue training\n",
    "load_model = False\n",
    "if load_model:\n",
    "    checkpoint = torch.load(\"model.pth\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    best_accuracy = checkpoint[\"best_accuracy\"]\n",
    "    print(\"=> Loaded model with accuracy {:.2f}\".format(best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3af951",
   "metadata": {
    "papermill": {
     "duration": 0.007326,
     "end_time": "2024-08-09T20:29:35.086546",
     "exception": false,
     "start_time": "2024-08-09T20:29:35.079220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e956596c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T20:29:35.102932Z",
     "iopub.status.busy": "2024-08-09T20:29:35.102316Z",
     "iopub.status.idle": "2024-08-09T22:59:09.865914Z",
     "shell.execute_reply": "2024-08-09T22:59:09.864823Z"
    },
    "papermill": {
     "duration": 8975.824191,
     "end_time": "2024-08-09T22:59:10.918151",
     "exception": false,
     "start_time": "2024-08-09T20:29:35.093960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 288/288 [06:46<00:00,  1.41s/it, loss=0.0136]\n",
      "Epoch 1 Validation: 100%|██████████| 31/31 [00:22<00:00,  1.35it/s, loss=0.0131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 72261742/74956800 with acc 96.40\n",
      "Dice score: 0.9224391579627991\n",
      "Epoch 1, train loss: 0.013601007944215898, val loss: 0.013110907656736061, val accuracy: 0.9224391579627991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 288/288 [06:47<00:00,  1.42s/it, loss=0.00719]\n",
      "Epoch 2 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.00577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74513868/74956800 with acc 99.41\n",
      "Dice score: 0.9863258004188538\n",
      "Epoch 2, train loss: 0.007192053386698599, val loss: 0.0057733867653324954, val accuracy: 0.9863258004188538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 288/288 [06:48<00:00,  1.42s/it, loss=0.00476]\n",
      "Epoch 3 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.32it/s, loss=0.00416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74470110/74956800 with acc 99.35\n",
      "Dice score: 0.9849904775619507\n",
      "Epoch 3, train loss: 0.00476276604215736, val loss: 0.00415501083232096, val accuracy: 0.9849904775619507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 288/288 [06:48<00:00,  1.42s/it, loss=0.00338]\n",
      "Epoch 4 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.00295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74577623/74956800 with acc 99.49\n",
      "Dice score: 0.9882967472076416\n",
      "Epoch 4, train loss: 0.0033772764622193316, val loss: 0.0029505078726616064, val accuracy: 0.9882967472076416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 288/288 [06:48<00:00,  1.42s/it, loss=0.00265]\n",
      "Epoch 5 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.00242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74639942/74956800 with acc 99.58\n",
      "Dice score: 0.9901397824287415\n",
      "Epoch 5, train loss: 0.002651454665414665, val loss: 0.002415576781772199, val accuracy: 0.9901397824287415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 288/288 [06:51<00:00,  1.43s/it, loss=0.00218]\n",
      "Epoch 6 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.32it/s, loss=0.00201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74675280/74956800 with acc 99.62\n",
      "Dice score: 0.9912691116333008\n",
      "Epoch 6, train loss: 0.0021785037041358326, val loss: 0.0020084960813649367, val accuracy: 0.9912691116333008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 288/288 [06:50<00:00,  1.43s/it, loss=0.0019]\n",
      "Epoch 7 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.32it/s, loss=0.00184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74686774/74956800 with acc 99.64\n",
      "Dice score: 0.9916195869445801\n",
      "Epoch 7, train loss: 0.001904298901638907, val loss: 0.0018442287987678267, val accuracy: 0.9916195869445801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 288/288 [06:48<00:00,  1.42s/it, loss=0.00175]\n",
      "Epoch 8 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.00171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74682511/74956800 with acc 99.63\n",
      "Dice score: 0.9915029406547546\n",
      "Epoch 8, train loss: 0.001752724326013223, val loss: 0.001714758479540221, val accuracy: 0.9915029406547546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 288/288 [06:46<00:00,  1.41s/it, loss=0.00164]\n",
      "Epoch 9 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.00163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74694405/74956800 with acc 99.65\n",
      "Dice score: 0.9918637871742249\n",
      "Epoch 9, train loss: 0.0016440162127432617, val loss: 0.0016264824662357569, val accuracy: 0.9918637871742249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 288/288 [06:48<00:00,  1.42s/it, loss=0.0016]\n",
      "Epoch 10 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.00162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74697385/74956800 with acc 99.65\n",
      "Dice score: 0.991949737071991\n",
      "Epoch 10, train loss: 0.001601790799556867, val loss: 0.0016157866692262106, val accuracy: 0.991949737071991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 288/288 [06:48<00:00,  1.42s/it, loss=0.00194]\n",
      "Epoch 11 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.0015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74572095/74956800 with acc 99.49\n",
      "Dice score: 0.9880622625350952\n",
      "Epoch 11, train loss: 0.0019438471997399692, val loss: 0.0014950417089047003, val accuracy: 0.9880622625350952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 288/288 [06:49<00:00,  1.42s/it, loss=0.00127]\n",
      "Epoch 12 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.00106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74676511/74956800 with acc 99.63\n",
      "Dice score: 0.9913032054901123\n",
      "Epoch 12, train loss: 0.0012730310189173273, val loss: 0.0010636865817865388, val accuracy: 0.9913032054901123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 288/288 [06:48<00:00,  1.42s/it, loss=0.00102]\n",
      "Epoch 13 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.000932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74672365/74956800 with acc 99.62\n",
      "Dice score: 0.9911538362503052\n",
      "Epoch 13, train loss: 0.0010211100536601052, val loss: 0.0009315157095428373, val accuracy: 0.9911538362503052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 288/288 [06:49<00:00,  1.42s/it, loss=0.000889]\n",
      "Epoch 14 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.33it/s, loss=0.000817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74706178/74956800 with acc 99.67\n",
      "Dice score: 0.9922254085540771\n",
      "Epoch 14, train loss: 0.0008886557240444033, val loss: 0.0008171223901731313, val accuracy: 0.9922254085540771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 288/288 [06:49<00:00,  1.42s/it, loss=0.00087]\n",
      "Epoch 15 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.32it/s, loss=0.000765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74710090/74956800 with acc 99.67\n",
      "Dice score: 0.9923355579376221\n",
      "Epoch 15, train loss: 0.0008698460360503067, val loss: 0.0007648017986479109, val accuracy: 0.9923355579376221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 288/288 [06:49<00:00,  1.42s/it, loss=0.000744]\n",
      "Epoch 16 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.31it/s, loss=0.000705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74723318/74956800 with acc 99.69\n",
      "Dice score: 0.9927541017532349\n",
      "Epoch 16, train loss: 0.0007443755142309743, val loss: 0.0007048305257635771, val accuracy: 0.9927541017532349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 288/288 [06:54<00:00,  1.44s/it, loss=0.000702]\n",
      "Epoch 17 Validation: 100%|██████████| 31/31 [00:24<00:00,  1.28it/s, loss=0.000679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74720271/74956800 with acc 99.68\n",
      "Dice score: 0.9926553964614868\n",
      "Epoch 17, train loss: 0.0007019807165488601, val loss: 0.0006793220603808028, val accuracy: 0.9926553964614868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 288/288 [06:49<00:00,  1.42s/it, loss=0.000671]\n",
      "Epoch 18 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.31it/s, loss=0.00068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74720166/74956800 with acc 99.68\n",
      "Dice score: 0.9926551580429077\n",
      "Epoch 18, train loss: 0.0006712732379041288, val loss: 0.0006799222748787677, val accuracy: 0.9926551580429077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|██████████| 288/288 [06:51<00:00,  1.43s/it, loss=0.000647]\n",
      "Epoch 19 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.29it/s, loss=0.000645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74730316/74956800 with acc 99.70\n",
      "Dice score: 0.9929762482643127\n",
      "Epoch 19, train loss: 0.0006469412642004697, val loss: 0.0006449894166597333, val accuracy: 0.9929762482643127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|██████████| 288/288 [06:52<00:00,  1.43s/it, loss=0.000639]\n",
      "Epoch 20 Validation: 100%|██████████| 31/31 [00:23<00:00,  1.32it/s, loss=0.000643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 74729886/74956800 with acc 99.70\n",
      "Dice score: 0.9929567575454712\n",
      "Epoch 20, train loss: 0.0006387507787946126, val loss: 0.0006434852066526159, val accuracy: 0.9929567575454712\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    ## Training phase \n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1} Training\")\n",
    "\n",
    "    train_loss = 0\n",
    "    train_exapmles = 0\n",
    "\n",
    "    # loop on the train loader\n",
    "    for batch_idx, (images, masks) in enumerate(tk0):\n",
    "        images = images.to(device)\n",
    "        masks = masks.float().unsqueeze(1).to(device) # add a channel dimension to the mask (since it is a single channel image)\n",
    "\n",
    "        \n",
    "        # we will use Float16 training to speed up the training process\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # forward pass\n",
    "            preds = model(images)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(preds, masks)\n",
    "            train_loss += loss.item()\n",
    "            train_exapmles += images.size(0)\n",
    "        \n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward() # scale the loss to avoid underflow or overflow\n",
    "        \n",
    "        # update the weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update the progress bar\n",
    "        tk0.set_postfix(loss=(train_loss/train_exapmles))\n",
    "\n",
    "    ## Validation phase\n",
    "    model.eval()\n",
    "    tk1 = tqdm(val_loader, total=len(val_loader), desc=f\"Epoch {epoch+1} Validation\")\n",
    "\n",
    "    val_loss = 0\n",
    "    val_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tk1):\n",
    "            images = images.to(device)\n",
    "            masks = masks.float().unsqueeze(1).to(device)\n",
    "\n",
    "            # forward pass\n",
    "            preds = model(images)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(preds, masks)\n",
    "            val_loss += loss.item()\n",
    "            val_examples += images.size(0)\n",
    "\n",
    "            # update the progress bar\n",
    "            tk1.set_postfix(loss=(val_loss/val_examples))\n",
    "\n",
    "    # save the model if the accuracy is improved\n",
    "    accuracy = check_accuracy_binary(val_loader, model, device)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"best_accuracy\": best_accuracy,\n",
    "        }\n",
    "        torch.save(checkpoint, \"model.pth\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, train loss: {train_loss/train_exapmles}, val loss: {val_loss/val_examples}, val accuracy: {accuracy}\")\n",
    "\n",
    "    # save the predictions as images every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        save_predictions_as_imgs(val_loader, model, device, folder=\"saved_images/\")\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler.step()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4db7e",
   "metadata": {
    "papermill": {
     "duration": 1.118643,
     "end_time": "2024-08-09T22:59:13.048926",
     "exception": false,
     "start_time": "2024-08-09T22:59:11.930283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef649f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T22:59:15.087460Z",
     "iopub.status.busy": "2024-08-09T22:59:15.087110Z",
     "iopub.status.idle": "2024-08-09T22:59:15.897737Z",
     "shell.execute_reply": "2024-08-09T22:59:15.896744Z"
    },
    "papermill": {
     "duration": 1.825565,
     "end_time": "2024-08-09T22:59:15.900196",
     "exception": false,
     "start_time": "2024-08-09T22:59:14.074631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[:-1:2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e13baa08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T22:59:18.009134Z",
     "iopub.status.busy": "2024-08-09T22:59:18.008568Z",
     "iopub.status.idle": "2024-08-09T23:01:42.878336Z",
     "shell.execute_reply": "2024-08-09T23:01:42.877414Z"
    },
    "papermill": {
     "duration": 145.89107,
     "end_time": "2024-08-09T23:01:42.881281",
     "exception": false,
     "start_time": "2024-08-09T22:59:16.990211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'test' not in os.listdir(WORKING_DIR):\n",
    "    if os.path.isfile(WORKING_DIR + 'submission.csv'):\n",
    "        os.remove(WORKING_DIR + 'submission.csv')\n",
    "    with zipfile.ZipFile(DATASET_DIR + 'test.zip', 'r') as zip_file:\n",
    "        zip_file.extractall(WORKING_DIR)\n",
    "if 'test_images' not in os.listdir(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR + 'test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0082170e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T23:01:44.991763Z",
     "iopub.status.busy": "2024-08-09T23:01:44.990765Z",
     "iopub.status.idle": "2024-08-10T00:07:07.685313Z",
     "shell.execute_reply": "2024-08-10T00:07:07.683617Z"
    },
    "papermill": {
     "duration": 3923.711592,
     "end_time": "2024-08-10T00:07:07.687348",
     "exception": false,
     "start_time": "2024-08-09T23:01:43.975756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6254/6254 [1:05:20<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR = WORKING_DIR + 'test'\n",
    "THRESHOLD = 0.5\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Dataset\n",
    "class CarvanaTestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.images[index]\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image)\n",
    "            image = augmentations['image']\n",
    "\n",
    "        return img_name, image\n",
    "\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=image_height, width=image_width),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(), \n",
    "    ]\n",
    ")\n",
    "   \n",
    "test_set = CarvanaTestDataset(\n",
    "    image_dir=TEST_DIR,\n",
    "    transform=test_transform\n",
    ")    \n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "    \n",
    "# Model\n",
    "checkpoint = torch.load(WORKING_DIR + 'model.pth')\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Predictions\n",
    "all_predictions = []\n",
    "for img_names, x in tqdm(test_loader):\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = torch.sigmoid(model(x))\n",
    "        preds = (preds > THRESHOLD).float()   \n",
    "    preds = TF.resize(\n",
    "        preds, size=(1280, 1918), interpolation=TF.InterpolationMode.NEAREST\n",
    "    )\n",
    "    \n",
    "    # Encoding\n",
    "    for idx in range(len(img_names)):\n",
    "        encoding = rle_encode(preds[idx].squeeze().cpu())\n",
    "        all_predictions.append([img_names[idx], encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e3c345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T00:07:10.805817Z",
     "iopub.status.busy": "2024-08-10T00:07:10.804868Z",
     "iopub.status.idle": "2024-08-10T00:07:48.446545Z",
     "shell.execute_reply": "2024-08-10T00:07:48.445693Z"
    },
    "papermill": {
     "duration": 39.188056,
     "end_time": "2024-08-10T00:07:48.449020",
     "exception": false,
     "start_time": "2024-08-10T00:07:09.260964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(WORKING_DIR + 'test')\n",
    "sub = pd.DataFrame(all_predictions)\n",
    "sub.columns = ['img', 'rle_mask']\n",
    "sub.to_csv(os.path.join(WORKING_DIR, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d436a787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T00:07:51.522360Z",
     "iopub.status.busy": "2024-08-10T00:07:51.521398Z",
     "iopub.status.idle": "2024-08-10T00:07:51.528183Z",
     "shell.execute_reply": "2024-08-10T00:07:51.527331Z"
    },
    "papermill": {
     "duration": 1.508742,
     "end_time": "2024-08-10T00:07:51.530185",
     "exception": false,
     "start_time": "2024-08-10T00:07:50.021443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission.csv' target='_blank'>submission.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/submission.csv"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "os.chdir('/kaggle/working')\n",
    "FileLink('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 45059,
     "sourceId": 6927,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13122.36589,
   "end_time": "2024-08-10T00:07:55.523305",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-09T20:29:13.157415",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
