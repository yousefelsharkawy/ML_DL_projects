{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12949831",
   "metadata": {
    "papermill": {
     "duration": 0.003472,
     "end_time": "2024-08-08T16:27:45.153593",
     "exception": false,
     "start_time": "2024-08-08T16:27:45.150121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "Neural Style Transfer is a technique that uses a neural network to generate an image that is a combination of the content of one image and the style of another image. The technique was introduced by Gatys et al. in the paper [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576).\n",
    "\n",
    "\n",
    "- The paper uses the VGG19 model\n",
    "\n",
    "![VGG19](assets/vgg19.png)\n",
    "\n",
    "- what we will do is to take that model and freeze all of it, we will pass the content image, the style image and a random image through the model, then we will calculate the content loss (the difference between the content image and the random image) and the style loss (the difference between the style image and the random image), then we use these losses to update the random image to minimize the content and style losses.\n",
    "    - the content loss is designed so that the random image will have the same content as the content image\n",
    "    - the style loss is designed so that the random image will have the same style as the style image\n",
    "\n",
    "\n",
    "- Content Loss:\n",
    "    - The content loss is the mean squared error between the feature maps of the content image and the random image at a certain layer of the model. \n",
    "    - Andrew Ng explains that this layer is chosen in intermediate layers of the model because the early layers capture low-level features like edges and textures, and the later layers capture high-level features like objects and scenes. but the intermediate layers captures something in between.\n",
    "    - the content loss is calculated as follows:\n",
    "        - $L_{content} = \\frac{1}{2} \\sum (F_{content} - F_{random})^2$ where $F_{content}$ and $F_{random}$ are the feature maps (activations) of the content image and the random image at a certain layer of the model.\n",
    "\n",
    "- Style Loss:\n",
    "    - The style loss is the mean squared error (L2 distance in Vectorized form) between the Gram matrices of the feature maps of the style image and the random image\n",
    "    - the gram matrix is simply the correlation matrix of the feature maps\n",
    "        - if we have a feature map of shape (H, W, C) then the gram matrix will be of shape (C, C) where C is the number of channels, each element in the gram matrix (say $G_{ij}$) is the dot product between the vectorized version of the $i^{th}$ channel and the $j^{th}$ channel of the feature map\n",
    "    - we calculate the style loss for each layer of the model and sum them up to get the total style loss (the paper uses 5 layers to calculate the style loss, they are: 'block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1') basically the first convolutional layer of each block in the model.\n",
    "    - the style loss is calculated as follows:\n",
    "        - $L_{style} = \\sum \\frac{1}{4 \\times C^2 \\times H^2 \\times W^2} \\sum (G_{style} - G_{random})^2$ where $G_{style}$ and $G_{random}$ are the gram matrices of the style image and the random image at a certain layer of the model.\n",
    "        - the $\\frac{1}{4 \\times C^2 \\times H^2 \\times W^2}$ is a normalization factor to make the style loss independent of the size of the feature maps.\n",
    "        - The total style loss is the sum of the style losses of the 5 layers.\n",
    "\n",
    "- Total Loss:\n",
    "    - The total loss is the sum of the content loss and the style loss multiplied by their respective weights ($\\alpha$ and $\\beta$)\n",
    "    - $L_{total} = \\alpha \\times L_{content} + \\beta \\times L_{style}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7599c413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:27:45.161093Z",
     "iopub.status.busy": "2024-08-08T16:27:45.160762Z",
     "iopub.status.idle": "2024-08-08T16:27:50.461829Z",
     "shell.execute_reply": "2024-08-08T16:27:50.460973Z"
    },
    "papermill": {
     "duration": 5.307579,
     "end_time": "2024-08-08T16:27:50.464250",
     "exception": false,
     "start_time": "2024-08-08T16:27:45.156671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7548a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:27:50.472803Z",
     "iopub.status.busy": "2024-08-08T16:27:50.471751Z",
     "iopub.status.idle": "2024-08-08T16:27:50.475966Z",
     "shell.execute_reply": "2024-08-08T16:27:50.475127Z"
    },
    "papermill": {
     "duration": 0.01036,
     "end_time": "2024-08-08T16:27:50.477882",
     "exception": false,
     "start_time": "2024-08-08T16:27:50.467522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg19(weights=None)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4216d",
   "metadata": {
    "papermill": {
     "duration": 0.003679,
     "end_time": "2024-08-08T16:27:50.485341",
     "exception": false,
     "start_time": "2024-08-08T16:27:50.481662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- the first Convolutional layer of each block in the VGG19 model is the one after the max pooling layer\n",
    "    - from the above model description we see that their indices are: 0, 5, 10, 19, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e820f5db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:27:50.493561Z",
     "iopub.status.busy": "2024-08-08T16:27:50.493202Z",
     "iopub.status.idle": "2024-08-08T16:27:50.563585Z",
     "shell.execute_reply": "2024-08-08T16:27:50.562493Z"
    },
    "papermill": {
     "duration": 0.076903,
     "end_time": "2024-08-08T16:27:50.565659",
     "exception": false,
     "start_time": "2024-08-08T16:27:50.488756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.chosen_layers = ['0', '5', '10', '19', '28']\n",
    "        self.model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features[:29] # we remove all layers after 28th layer (we don't need them)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        # loop on the model features (layers)\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            # run the input through the layers \n",
    "            x = layer(x)\n",
    "            # if the layer was one of the chosen layers, add its output to the activations list\n",
    "            if str(layer_num) in self.chosen_layers:\n",
    "                activations.append(x)\n",
    "        \n",
    "        return activations # will contain the outputs of the chosen layers\n",
    "    \n",
    "\n",
    "def load_image(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = transform(image).unsqueeze(0) # add a dimension at the beginning of the tensor (batch size)\n",
    "    return image.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# specify the image size (the better the GPU, the bigger the image size can be)\n",
    "imsize = 512 # this is important as we need Content, Style and Generated images to have the same size (in order to perform the loss calculations)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imsize, imsize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalization for VGG19, he said he tried it without normalization and it worked well as well, but if we normalize, we need to denormalize the generated image before saving it (multiply by std and add mean)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d06b18",
   "metadata": {
    "papermill": {
     "duration": 0.002735,
     "end_time": "2024-08-08T16:27:50.571700",
     "exception": false,
     "start_time": "2024-08-08T16:27:50.568965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- some people start with the original content image as the random image (instead of starting with a random image), this is called the \"fast neural style transfer\" technique, it is faster because the random image will start with the content image and will be updated to minimize the style loss only, this is useful when we want to apply the style of the style image to the content image.\n",
    "     - this is what we will do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbb6106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:27:50.578704Z",
     "iopub.status.busy": "2024-08-08T16:27:50.578386Z",
     "iopub.status.idle": "2024-08-08T18:05:19.469046Z",
     "shell.execute_reply": "2024-08-08T18:05:19.468120Z"
    },
    "papermill": {
     "duration": 5848.896608,
     "end_time": "2024-08-08T18:05:19.471179",
     "exception": false,
     "start_time": "2024-08-08T16:27:50.574571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:03<00:00, 165MB/s]\n",
      "100%|██████████| 6000/6000 [16:14<00:00,  6.16it/s, content_loss=75.9, style_loss=1.85e+7, total_loss=1.85e+8]\n",
      "100%|██████████| 6000/6000 [16:12<00:00,  6.17it/s, content_loss=71.3, style_loss=6.71e+7, total_loss=6.71e+8]\n",
      "100%|██████████| 6000/6000 [16:10<00:00,  6.18it/s, content_loss=69.1, style_loss=1.34e+7, total_loss=1.34e+8]\n",
      "100%|██████████| 6000/6000 [16:12<00:00,  6.17it/s, content_loss=70, style_loss=9.5e+7, total_loss=9.5e+8]\n",
      "100%|██████████| 6000/6000 [16:12<00:00,  6.17it/s, content_loss=84.8, style_loss=7.46e+7, total_loss=7.46e+8]\n",
      "100%|██████████| 6000/6000 [16:11<00:00,  6.17it/s, content_loss=61.9, style_loss=3.22e+7, total_loss=3.22e+8]\n"
     ]
    }
   ],
   "source": [
    "original_img = load_image(\"/kaggle/input/nst-images/content2.jfif\")\n",
    "style_images = ['/kaggle/input/nst-images/style1.jfif','/kaggle/input/nst-images/style2.jpg','/kaggle/input/nst-images/style3.jfif','/kaggle/input/nst-images/style4.jfif','/kaggle/input/nst-images/style5.jfif','/kaggle/input/nst-images/style6.jfif']\n",
    "\n",
    "for i, path in enumerate(style_images):\n",
    "    #generated  = torch.randn(original_img.shape, device=device, requires_grad=True) # random image\n",
    "    generated = original_img.clone().requires_grad_(True) # clone the original image and set requires_grad to True to optimize it\n",
    "\n",
    "    # Hyper parameters\n",
    "    total_steps = 6000\n",
    "    learning_rate = 0.001\n",
    "    alpha = 1 # content loss hyperparameter\n",
    "    beta = 10 # style loss hyperparameter\n",
    "\n",
    "    # we set the optimizer to optimize the generated image\n",
    "    optimizer = optim.Adam([generated], lr=learning_rate)\n",
    "\n",
    "\n",
    "    model = VGG().to(device).eval() # we don't want to train the model, just use it to get the activations of the chosen layers\n",
    "    style_img = load_image(path)\n",
    "\n",
    "    \n",
    "    tk0 = tqdm(range(total_steps), total=total_steps)\n",
    "    for step in tk0:\n",
    "        # pass the 3 images through the model\n",
    "        generated_features = model(generated)\n",
    "        original_img_features = model(original_img)\n",
    "        style_features = model(style_img)\n",
    "        # the outputs above are a list of 5 activations (one for each chosen layer)\n",
    "\n",
    "        # initialize the losses to 0 at the beginning of each step\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "\n",
    "        for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_features):\n",
    "            # get the dimensions of the current activations (feature maps)\n",
    "            batch_size, channel, height, width = gen_feature.shape \n",
    "\n",
    "            ## compute the content loss\n",
    "            content_loss += torch.mean((gen_feature - orig_feature) ** 2) #/ 2 \n",
    "            ## compute the style loss\n",
    "            # compute the gram matrix of the generated image\n",
    "            G_generated = gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t())\n",
    "            # compute the gram matrix of the style image\n",
    "            G_style = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "            style_loss += torch.mean((G_generated - G_style) ** 2) #/ (4 * (channel ** 2) * (width * height) ** 2)\n",
    "\n",
    "        # compute the total loss\n",
    "        total_loss = alpha * content_loss + beta * style_loss\n",
    "\n",
    "        # back propagation \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # update the generated image\n",
    "        optimizer.step()\n",
    "\n",
    "        tk0.set_postfix(total_loss=total_loss.item(), content_loss=content_loss.item(), style_loss=style_loss.item())\n",
    "        if step % 200 == 0:\n",
    "            #print(total_loss.item())\n",
    "            # detach the generated image from the graph and denormalize it\n",
    "            denormalized = generated * torch.tensor([0.229, 0.224, 0.225],device=device).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406],device=device).view(3, 1, 1)\n",
    "            save_image(denormalized.clamp(0, 1), f\"output{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7decf",
   "metadata": {
    "papermill": {
     "duration": 5.876283,
     "end_time": "2024-08-08T18:05:31.217681",
     "exception": false,
     "start_time": "2024-08-08T18:05:25.341398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Tips \n",
    "    - we calculated the gram matrix as follows \n",
    "        - first we shaped the activations to be of shape (C, H*W)\n",
    "        - then we multiplied the activations by their transpose (the transpose will be of shape (H*W, C) and the multiplication will be of shape (C, C)) and we will do that for both the style image and the random image\n",
    "    - mine: changes from tha peper\n",
    "        - we started with the content image as the random image\n",
    "        - we used the first convolutional layer of each block to calculate the content loss as well (not just 1 layer as in the paper)\n",
    "        - we didnt weight the style loss for the different layers (the paper used different weights for the different layers)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5512936,
     "sourceId": 9132393,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5876.074754,
   "end_time": "2024-08-08T18:05:38.368967",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-08T16:27:42.294213",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
