{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/2020_02_CNN_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_bb6VjPehhk"
   },
   "source": [
    "- in images, let us assume that we want to classify the image \n",
    "    - in early labs of deep learning, we used Dense layers to do that \n",
    "        - flatten the 2D image to a very long vector \n",
    "        - input it to the NN \n",
    "        - but we said that this procedure has a lot of weights that need to be trained \n",
    "- then the idea of convolution from image processing got introduced into deep learning , remeber in image processing we used to have certain filters that do certain operations (detecting horiontal edges, vertical edges, sharpining and so on) and convolute the filters on the image \n",
    "\n",
    "- so people thought about making the NN do convolution on the image and through a series of cascaded convolutions -and applying non linear function to each result- we may get the NN to learn the weignts of each layer's filters that will enable us to classiy the image or do the required task \n",
    "\n",
    "- so , what we want here is to learn the weights  that do the required mapping between the inputs and the label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uoxdNjkDw2K"
   },
   "source": [
    "# Introduction to Convolutional Neural Networks\n",
    "\n",
    "This lB addresses the basic concepts of Convolutional Neural Networks and their implementation using the Keras framework.\n",
    "Convolutional Neural Networks (CNNs) are a class of feed-forward artificial neural networks that are applied to analyze visual 2D imagery, meaning that we can feed images directly to a CNN without the need to flatten them into a 1D vector beforehand.\n",
    "CNNs have revolutionized the field of computer vision in the last decade. In 2012 Alex Krizhevsky introduced the AlexNet architecture to win the ImageNet Challenge (one of the most important competitions on image classification within the Computer Vision community), by reducing the top-5 error more than 10 percentage points, which was an incredible improvement. As of now, CNNs are used not only on image classification but in many other computer vision tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ2d_oQ2UEPz"
   },
   "source": [
    "# CNN Structure\n",
    "\n",
    "The basic pipeline of CNNs consists of an image that undergoes through a series of convolutional layers to obtain a final representation. This final representation depends on the kind of problem that the architecture is facing. For instance, the output of the final layer in a classification setup will be the probabilities of each of the classes. However, the network's outputs take any shape that is desired; we could use a single value for regression or a newly generated image for semantic segmentation or image transformation purposes. Layers within CNNs are similar to those that we have already seen on 1D neural networks, but having this time 2D matrices as inputs instead of 1D vectors. Now, we will introduce specific layers that are used in CNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvNKve9GZL4M"
   },
   "source": [
    "# 2D Convolutional Layer\n",
    "\n",
    "The most common layer in any CNN architecture is the 2D convolutional layer. Convolutional layers are specifically designed to extract features from images or even extract features from previously extracted features. Therefore, in CNN networks, we can stack several convolutional layers to obtain more and more complex representations -features- of an image.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n",
    "\n",
    "Image [source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1).\n",
    "\n",
    "The latest deep learning frameworks have made possible the integration of convolutional layers easily on our architectures with only a single line of code. We will address here how 2D convolutions work since full understanding is needed to comprehend how any CNN operates. The following images and some explanations can also be found on [Irhum Shafkat's blog](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) and [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) book. Both are strongly recommended.\n",
    "\n",
    "To understand 2D convolutions, we need to define first what a kernel is. Kernels are simply matrices of numbers. The numbers on the kernels are the so-called weights and the weights on the kernels change as we train the network. Training the network, and therefore updating the kernel's weights, will improve the performance of CNN in solving specific tasks. \n",
    "\n",
    "The 2D convolution operation takes the network's kernels and follows a process similar to a sliding window over the input image (or feature map), performing an element-wise multiplication with the elements that are currently on. See the figure below. The results of this elementwise multiplication are summed up, giving, as a result, a single output value. This operation is repeated for all the positions of this sliding window, composing at the end the feature map. This generated feature map can go through another 2D convolutional layer and create more powerful features. \n",
    "\n",
    "> \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n",
    "\n",
    "> \n",
    "The previous image shows the 2D convolution operation. A new feature value is the weighted sum of all the elements in the sliding window after the elementwise multiplication between input and kernel.  The bigger the size of the kernel is, the more feature elements will contribute to the final output value. In contrast to fully connected layers, where a new feature value is a weighted sum over all input values, 2D convolutions compute features based on local areas. In other words, instead of looking at every input component, they consider only features coming from close locations. \n",
    "\n",
    "In the above example, the input image on the left has a size of 5x5 and the dimension of the resulting feature map is 3x3, showing that the size of the output maps is not always equal to the input size. The output size can be computed by doing:\n",
    "\n",
    "$O = W - K + 1$,\n",
    "\n",
    "where $O$ is the output height/length, $W$ is the input height/length and $K$ is the kernel size. The output size is not only conditioned on the input size but also on the kernel size. Check in the following code cell how the output feature map shape changes as you increase the kernel size. In Keras, we define the layer by using `Conv2D` from `keras.layers` (documentation [here](https://keras.io/layers/convolutional/#conv2d)).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LC3bDpttehhq",
    "outputId": "2c3a79f5-43d2-4f8d-dc3b-793b73266548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 98)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_width = 100 - 3 + 1\n",
    "output_height= 100 - 3 + 1\n",
    "output_width, output_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QA0MAQgehht"
   },
   "source": [
    "- in today's lab, we will do some convolutional layers, make sure of running them then study some architectures \n",
    "\n",
    "in keras the layer is \n",
    "\n",
    "`Conv2D(number of filters, kernel size, strides = 1,padding = 'valid')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ez90vQEksR9b",
    "outputId": "92323841-bf05-43c6-85b5-4249c45932ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 440ms/step\n",
      "Input size: (100, 100)\n",
      "Output size: (98, 98)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Generate dummy data -image- Â \n",
    "input_feature = np.random.random((1, 100, 100, 1)) # a patch of 1 image, each of size 100x100x1\n",
    "\n",
    "# input: 100x100 image with 1 channels -> (100, 100, 1) tensor.\n",
    "# this applies 1 convolution filter of size 3x3 each.\n",
    "model = Sequential()\n",
    "# conv2D(number of filters, kernel size, stride = 1)\n",
    "model.add(Conv2D(1, (3, 3), input_shape=(100, 100, 1))) # we don't write the patch in dimension \n",
    "\n",
    "\n",
    "output_feature = model.predict(input_feature)\n",
    "\n",
    "print('Input size: ({:}, {:})'.format(input_feature.shape[1], input_feature.shape[2]))\n",
    "print('Output size: ({:}, {:})'.format(output_feature.shape[1], output_feature.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_IRHLevehhv",
    "outputId": "9e1652b4-f768-40d9-b05c-6d2cb99809bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100, 100, 1), (1, 98, 98, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feature.shape,output_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNOhth4FuNob"
   },
   "source": [
    "Moreover, the kernel size and the input size are not the only parameters affecting the output size. Two extra elements that change the size of the output map are padding and striding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT6f_n5QiU8X"
   },
   "source": [
    "# Adding Padding to Input Features\n",
    "\n",
    "In some tasks, such as image translation, we need the output size to be equal to the input size. The solution to that is using padding, where extra edges are added to the input features so that the dimension is not reduced after the convolutional layer. Normally those pixels have $0$ value (termed zero-padding), but depending on the application other methods could be used, e.g., reflection or symmetric padding.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*1okwhewf5KCtIPaFib4XaA.gif)\n",
    "\n",
    "If padding is used, the new output size can be computed by doing:\n",
    "\n",
    "$O = W - K + 2P+ 1$, \n",
    "\n",
    "where $P$ is the padding value. $P$ must be set in concordance with the kernel size if dimensionality wants to be preserved. Padding can be added to the `Conv2d` layer in Keras by using the padding argument and selecting one of the two settings: `valid`, which is the default value and means no padding; and `same`, which adapts the padding value to have the same output size as input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7Z5G2O00Etd",
    "outputId": "e813abf3-966d-4271-c584-907d1d0ad447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n",
      "Input size: (100, 100)\n",
      "Output size: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Generate dummy data\n",
    "input_feature = np.random.random((1, 100, 100, 1))\n",
    "\n",
    "# input: 100x100 image with 1 channels -> (100, 100, 1) tensor.\n",
    "# this applies 1 convolution filter of size 3x3 each.\n",
    "# attribute padding='same' applies zero-padding to the input feature map\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3, 3), input_shape=(100, 100, 1), padding=\"same\")) # this will adjust the padding frames according to the kernel size, therefore roduces output of the SAME size\n",
    "\n",
    "output_feature = model.predict(input_feature)\n",
    "\n",
    "print('Input size: ({:}, {:})'.format(input_feature.shape[1], input_feature.shape[2]))\n",
    "print('Output size: ({:}, {:})'.format(output_feature.shape[1], output_feature.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1pCzVtxehhz",
    "outputId": "21831c38-c9f6-4f17-b3a7-a8c31d2fd295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_width = 100 - 3 + 2 + 1\n",
    "output_height = 100 - 3 + 2 + 1\n",
    "output_width, output_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdB4jMeswXVA"
   },
   "source": [
    "# Using Stride in Convolutional Layers\n",
    "\n",
    "The stride operation allows the convolutional layers to skip some of the sliding windows explained above. Therefore, instead of jumping one pixel apart, we can define the number of skipped elements before computing the weighting sum between the kernel's weights and input features. A stride of 1 means that features will be extracted from all windows a pixel apart, so basically, every single window will be computed, acting as a standard convolution. A stride of 2 means that we are selecting windows 2 pixels apart, skipping every other window in the process. Strides reduce the number of computations and consequently the size of the output map. In practice, as we go deeper into the CNN, the size of the feature map gets smaller while the number of channels increases. Moreover, we can further reduce the size of the feature map using pooling operations, which we introduce later in this tutorial.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*BMngs93_rm2_BpJFH2mS0Q.gif)\n",
    "\n",
    "If strides are used, the new output size can be computed as:\n",
    "\n",
    "$O = \\dfrac{W - K + 2P}{S}+ 1$, \n",
    "\n",
    "where $S$ is the stride value. The stride can be set in the layer by using the `strides` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8GjztiH0hnw",
    "outputId": "3dba8223-eb79-4558-96f8-56b8d3584957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "Input size: (100, 100)\n",
      "Output size: (50, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Generate dummy data\n",
    "input_feature = np.random.random((1, 100, 100, 1))\n",
    "\n",
    "# input: 100x100 image with 1 channels -> (100, 100, 1) tensor.\n",
    "# this applies 1 convolution filter of size 3x3 each.\n",
    "# attribute padding='same' applies zero-padding to the input feature map\n",
    "# attribute strides=2 applies applies stride of 2\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3, 3), input_shape=(100, 100, 1), padding=\"same\", strides=2))\n",
    "\n",
    "output_feature = model.predict(input_feature)\n",
    "\n",
    "print('Input size: ({:}, {:})'.format(input_feature.shape[1], input_feature.shape[2]))\n",
    "print('Output size: ({:}, {:})'.format(output_feature.shape[1], output_feature.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_DwZzRaehh2",
    "outputId": "28ab3ac3-ee42-49c7-80e6-c5d1b559f165"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_width = ((100 - 3 + 2) / 2) + 1\n",
    "output_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7h3Nepwehh3"
   },
   "source": [
    "# Differences Between Kernel and Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-fKvgJ3H2gr"
   },
   "source": [
    "Filter = kernel x no. of channels\n",
    "\n",
    "The examples above take as input a single-channel image and compute a feature map with also one channel. However, when dealing with RGB images or feature maps, the input is no longer a single-channel map but, instead, they can have multiple channels. In the case of an RGB image, for each 2D convolution, we will need to define 3 kernels to interact with each of the image's channel colors. This group of kernels is called a filter. Thus, a filter is a collection of kernels that produces a single output.\n",
    "\n",
    "As a regular practice when defining Deep Learning models, we increase the number of filters in each convolutional layer in order to obtain more channels from those layers that could extract more complex and meaningful features. When designing a neural network, each filter in a convolutional layer must have the same number of kernels as the number of channels of the input feature. Keras already deals with the number of kernels inside each filter by keeping track of the input size in each convolutional layer, unlike other frameworks such as Pytorch or TensorFlow. Hence, in Keras, we must only decide the number of filters (output channels) in each layer.\n",
    "\n",
    "The next figure shows how the convolution is performed when having three input channels. First, one filter uses its three independent kernels to convolve with the RGB channels of the input image:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*8dx6nxpUh2JqvYWPadTwMQ.gif)\n",
    "\n",
    "Next, each of the processed feature maps is summed together to obtain a single channel:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*CYB2dyR3EhFs1xNLK8ewiA.gif)\n",
    "\n",
    "Finally, we add the bias term to obtain the feature map. There is a single bias, therefore the bias gets added to the full output channel to produce a single-channel feature map. This operation is repeated for all filters on its own since each filter has a different set of kernels and a scalar bias, combining them at the end and building the final feature map. \n",
    "\n",
    "Now, we show how to use a `Conv2D` layer that takes an input image with 3 channels and generates an output map with 32 channels.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvu-1EB0ehh3"
   },
   "source": [
    "- let's try Conv2D with input of thickness > 1\n",
    "    - we will input RGB image (input of 3 channels)\n",
    "    - we want to output an image with 32 channel -which means we will use 32 filters- each filter will adjust its thickness to match the input thickness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqiEwpdROxga",
    "outputId": "26cee6f9-d7de-4010-c835-23480ace600a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n",
      "Input size: (100, 100, 3)\n",
      "Output size: (50, 50, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Generate dummy data\n",
    "input_feature = np.random.random((1, 100, 100, 3))\n",
    "\n",
    "# input: 100x100 image with 3 channels -> (100, 100, 3) tensor.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "# attribute padding='same' applies zero-padding to the input feature map\n",
    "# attribute strides=2 applies applies stride of 2\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(100, 100, 3), padding=\"same\", strides=2))\n",
    "\n",
    "output_feature = model.predict(input_feature)\n",
    "\n",
    "print('Input size: ({:}, {:}, {:})'.format(input_feature.shape[1], input_feature.shape[2], input_feature.shape[3]))\n",
    "print('Output size: ({:}, {:}, {:})'.format(output_feature.shape[1], output_feature.shape[2], output_feature.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYMWxrc-ehh4"
   },
   "source": [
    "- we go 50 because we used strides = 2 and we got 50 and not 49 because we used padding to be same and we got 32 channels in the output as we used 32 filters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmOsgOpnehh5"
   },
   "source": [
    "# Activation Functions\n",
    "\n",
    "- so far we have done only convolution on the layers (which is not what we do in the CNN) so we ALWAYS apply an activation function after we convolute  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S50cYwkhPqka"
   },
   "source": [
    "\n",
    "\n",
    "As seen in previous LABS, after `Dense` layers usually we can find activation functions, which we will also use after `Conv2D` layers. Those activation functions are a set of operators that will map the feature values to a new set of resulting values, depending on the function at hand. The main reason for using activation functions is that they add non-linearities to the network, giving more expressive power to the network, which will be able to reproduce more complex functions. \n",
    "\n",
    "\n",
    "The complete list of activation functions that Keras offers can be found [here](https://keras.io/activations/). We introduce here how some of them:\n",
    "\n",
    "*  **Sigmoid Function** sets the output in the range (0, 1). The sigmoid function is widely used in binary classification problems since its output can be taken as a probability value. `keras.activations.sigmoid(x)`:\n",
    "\n",
    ">![](https://i.ibb.co/Ph8dsTv/sigmoid.png)\n",
    "\n",
    "*  **Tanh Function** is a logistic function as sigmoid, but the range of the tanh function is (-1, 1). Contrary to sigmoid function, where the values close to 0 are set around 0.5, in the tanh function they will be still mapped around the 0 value. `keras.activations.tanh(x)`:\n",
    "\n",
    ">![](https://i.ibb.co/68g7LpL/tanh.png)\n",
    "\n",
    "*  **ReLU Function** is the most common activation function you can find in any current CNN as in general works better than the rest. The range of this function in \\[0, inf). Basically, it sets all negative values to 0, and hence is computationally easy to implement. As a drawback, during training some neurons will *die*, meaning that the output will be 0 for all available data points and no gradient will be propagated there. `keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)`:\n",
    "\n",
    ">![](https://i.ibb.co/Zd9H8Z4/relu.png)\n",
    "\n",
    "*  **LeakyReLU Function** is a modified version of the ReLU activation above, which attempts to solve the problem of dying neurons that ReLU has. While ReLU does not backpropagate negative values, Leaky ReLU smooths those values without setting them to 0. That allows the gradients to backpropagate through the network even for negative values. `keras.layers.LeakyReLU(alpha=0.3)`:\n",
    "\n",
    ">![](https://i.ibb.co/dmnJ6h1/leakyrelu.png)\n",
    "\n",
    "*  **Softmax Function** is another widely activation function for multi-class classification problems and usually is employed as the last activation function in a multi-class classification model. This function sets all of the output elements to the range (0, 1). However, the softmax function does not take independently the input values to map in their probability values, it takes an un-normalized vector, $s$, and normalizes it into a probability distribution, $p$. As the output is a probability, the output elements add up to 1. `keras.activations.softmax(x, axis=-1)`. The output value $p_i$ is computed as:\n",
    "\n",
    "> $p_{i} = \\dfrac{e^{s_i}}{\\sum_{\\substack{j}}^{N} e^{s_j}}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MWo2VsYr5T2"
   },
   "source": [
    "The following example shows the feature maps before and after of the ReLU activation function. All values that are negative are set to 0 after the activation function as explained above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MtQTdM8ehh6"
   },
   "source": [
    "tip: to get output of a certain layer\n",
    "\n",
    "`from keras.models import Model`\n",
    "then give the layer a name then \n",
    "\n",
    "`sliced_model = Model(inputs = original_model.input , outputs = original_model.get_layer(layer_name).output )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lp1HH0VOqN0N",
    "outputId": "48988c31-2e40-4d4b-a69e-e593fa9e9849",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f64757848b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f64752320d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n",
      "Output Network without activation function\n",
      "[[[[ 0.3135101 ]\n",
      "   [ 0.43296742]\n",
      "   [ 0.2926268 ]]\n",
      "\n",
      "  [[ 0.1481274 ]\n",
      "   [ 0.2183942 ]\n",
      "   [ 0.11018097]]\n",
      "\n",
      "  [[-0.10441318]\n",
      "   [-0.14919026]\n",
      "   [-0.2297165 ]]]]\n",
      "\n",
      "Output Network after ReLU activation function\n",
      "[[[[0.3135101 ]\n",
      "   [0.43296742]\n",
      "   [0.2926268 ]]\n",
      "\n",
      "  [[0.1481274 ]\n",
      "   [0.2183942 ]\n",
      "   [0.11018097]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Activation\n",
    "\n",
    "# Generate dummy data\n",
    "input_feature = np.random.random((1, 3, 3, 1)) - 0.5\n",
    " \n",
    "# input: 3x3 image with 1 channel -> (3, 3, 1) tensor.\n",
    "# This applies a 1 convolution filter of size 3x3 each.\n",
    "# This applies a ReLU activation function\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3, 3), input_shape=(3, 3, 1), padding=\"same\", name=\"conv\"))\n",
    "model.add(Activation(\"relu\")) # we added activation as a separate layer, but we could have put it in the conv2D instead\n",
    "\n",
    "model_before_ReLU = Model(inputs=model.input, outputs=model.get_layer(\"conv\").output)\n",
    "\n",
    "output_feature = model_before_ReLU.predict(input_feature)\n",
    "output_ReLu_feature = model.predict(input_feature)\n",
    "\n",
    "print('Output Network without activation function')\n",
    "print(output_feature)\n",
    "\n",
    "print('')\n",
    "print('Output Network after ReLU activation function')\n",
    "print(output_ReLu_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eU3PPj9ehh7"
   },
   "source": [
    "- we notice that the model before applying the acivation has a +ve and -ve parts \n",
    "- but after applying the relu we have either +ve or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys6ZB1DOTLH6"
   },
   "source": [
    "# Pooling Layer\n",
    "\n",
    "It is a common practice to insert a pooling layer between convolutional layers in CNNs. In a standard CNN architecture, we set the feature sizes to become smaller progressively to reduce the number of parameters and the computation in the networks, and also to merge the information from different spatial locations. To reduce the feature map sizes, we can either use bigger strides in the convolutional layers or we can use pooling layers. Pooling layers perform a spatial sliding window and apply an operation to reduce the spatial size. Those operations vary depending on the architecture, being the max, mean and min pooling the most typical ones. Here, we will explain the max pooling, although all the others work similarly. Max pooling keeps only the max value in a neighborhood, where the neighborhood is defined by the size of the kernel. The next example shows the result of a Max Pooling layer with a 2x2 kernel and a stride of 2.\n",
    "\n",
    "![](https://i.ibb.co/Xp454S4/MaxPool.png)\n",
    "\n",
    "As in convolutional layers, the final size is conditioned to the stride size of the pooling layer. However, contrary to convolutional layers, pooling layers operate independently on each of the input channels, without modifying the depth of the feature maps. To add max pooling to our model we need to import `MaxPooling2d` from `keras.layers` and define the strides and the pooling size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S3GwrHIehh7"
   },
   "source": [
    "- `maxpooling2D(pool_size=(,) , strides = (,) )` notice that the pooling size on x does not have to be the same size on y\n",
    "    - example poolsize can be equal to (2,3) (this will divide the image to 2x3 pools\n",
    "    - if we wrote a single number 2 it will be casted internally to be 2x2\n",
    "    \n",
    "tip: the strides as well can be treated like that \n",
    "- strides 2 is internally casted to 2x2 (will skip 2 pixels to the right and 2 pixels down \n",
    "- strides (m,n) will skip m pixels to the right and b pixels down\n",
    "\n",
    "i guess the **max pooling output size** also can be estimated using the formula we used with the **conv2D layer output size** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzoTGOwDmQm4",
    "outputId": "bfdef3b2-9f74-4963-aa8a-9866d6cee9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n",
      "Input size: (100, 100, 3)\n",
      "Output size: (50, 50, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D\n",
    "\n",
    "# Generate dummy data\n",
    "input_feature = np.random.random((1, 100, 100, 3))\n",
    "\n",
    "# input: 100x100 image with 3 channels -> (100, 100, 3) tensor.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "# attribute padding='same' applies zero-padding to the input feature map\n",
    "# attribute strides=1 applies applies stride of 1\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(100, 100, 3), padding=\"same\", strides=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\"))\n",
    "\n",
    "\n",
    "\n",
    "output_feature = model.predict(input_feature)\n",
    "\n",
    "print('Input size: ({:}, {:}, {:})'.format(input_feature.shape[1], input_feature.shape[2], input_feature.shape[3]))\n",
    "print('Output size: ({:}, {:}, {:})'.format(output_feature.shape[1], output_feature.shape[2], output_feature.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKU4-GbZHw0R"
   },
   "source": [
    "# Classification on MNIST\n",
    "\n",
    "in this section will be seeing how to perform image classification when the input data is a 2D image instead of a flat 1D vector. \n",
    "\n",
    "As discussed above, Convolutional Neural Networks aim to extract and exploit the local relationships on 2D maps, thus, CNNs are much more convenient for images than Multi-layer Perceptron models. \n",
    "\n",
    "First of all, we load the MNIST dataset from Keras' framework. The definition of the data is almost identical than in the Keras tutorial, although this time we are not reshaping the input images to have a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3t0Wq9ArL4EZ",
    "outputId": "861636ba-7f40-4bdf-f0cf-627ab8409bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "xtrain before expansion: (60000, 28, 28)\n",
      "Image shape: (28, 28, 1)\n",
      "Total number of training samples: 60000\n",
      "Total number of test samples: 10000\n",
      "xtrain after expansion: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "#from tensorflow.keras.optimizers import rmsprop\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(\"xtrain before expansion: \" + str(x_train.shape))\n",
    "\n",
    "shape = x_train.shape\n",
    "\n",
    "# Normalize and reshape the input images, we expand the shape to contain the axis for the number of channels (so that we have series of 3D images)\n",
    "x_train = np.expand_dims(x_train.astype('float32'), -1) # -1 means that the new axis will be added to the last\n",
    "x_test = np.expand_dims(x_test.astype('float32'), -1)\n",
    "\n",
    "# normalize the values to be from 0 to 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert the labels to one hot vectors  \n",
    "y_train_class = np_utils.to_categorical(y_train, 10)\n",
    "y_test_class = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print('Image shape: {0}'.format(x_train.shape[1:])) #print the single image shape\n",
    "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
    "print('Total number of test samples: {0}'.format(x_test.shape[0]))\n",
    "\n",
    "print(\"xtrain after expansion: \" + str(x_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UKb51Keehh-",
    "outputId": "84a84dc2-8c3b-4a66-9a77-c31a4154b0da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.random(size=(28,28,1))\n",
    "dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjfXsdcZehh_",
    "outputId": "abb63777-1346-4a0b-cb96-4d6739b4059b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(dummy,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UY-un0ALehh_",
    "outputId": "8a9de8c8-f286-42bd-89f4-98dae0d0e776"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 1, 28, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(dummy,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNnmgHCsehiA",
    "outputId": "10dcac93-7c84-4739-f6de-41dd5dabdcd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(dummy,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4zHYU9IehiB",
    "outputId": "8da1fe00-4789-4edd-b6e0-9810eac0d64c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(x_train,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hDO0hgzehiC",
    "outputId": "82f8e70c-5625-4df3-f88c-5dc3a25683be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSgbf36behiC",
    "outputId": "25f3f1d1-d58d-4349-bc23-16671fb014bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_utils.to_categorical(y_train,num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Wp29GszMO-j"
   },
   "source": [
    "As illustrated in the cell, this time the input image before the model is 28x28x1. \n",
    "\n",
    "Now we can define a model composed of convolutional layers, activation functions, and maxpool operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B78NUAqwMeFR"
   },
   "outputs": [],
   "source": [
    "# Declare Convolutional Part\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\", input_shape=x_train.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "G-WQdkMHehiE"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16,(3,3),padding='same',input_shape=(x_train.shape[1:]),activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(16,(3,3),padding='same',activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    # after the CNN layers, we need to flatten the resulting output \n",
    "    tf.keras.layers.Flatten(),\n",
    "    # now we input to a dense layer (which will be the final output layer)\n",
    "    Dense(10,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX_KAFlBMnVZ"
   },
   "source": [
    "As explained in the previous labs, in a classification problem, the output of the model is a vector containing the probabilities of the input image of belonging to a specific class. \n",
    "\n",
    "Now the resulting feature map of the model is a map with the shape *Batch x Weight' x Height' x Channel*, and needs to be mapped to a vector with shape *Batch x Num Classes*. A common technique to process this mapping is to add a Flatten layer that will reshape the feature map to *Batch x (Weight' * Height' * Channel)*, and next to add a dense layer which brings this new feature map to the output desired size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3CAc1r2vOJQv"
   },
   "outputs": [],
   "source": [
    "# Declare FCN Part\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHInEJ-aON5v"
   },
   "source": [
    "Let's visualize the model's shape and outputs in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UD4NZtIxOTbY",
    "outputId": "deb5c2f0-0d45-4059-ab3e-aba2e1c8df7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 14, 14, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                7850      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,440\n",
      "Trainable params: 10,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_F66ds58OTBu"
   },
   "source": [
    "Now we can train our CNN and check its performance on MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "BmeolsG_Pi8A",
    "outputId": "ddef2a53-104e-49bc-89cc-bd8bca77374d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 [==============================] - 37s 585ms/step - loss: 2.2666 - accuracy: 0.2424\n",
      "Epoch 2/10\n",
      "22/59 [==========>...................] - ETA: 19s - loss: 2.1806 - accuracy: 0.3900"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d6b19f38cffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0003, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train_class, batch_size=1024, epochs=10,  verbose=1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test_class, verbose=2)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cT0wPZUtehiH"
   },
   "source": [
    "- why does a cnn layer contain biases? (the number of biases = number of filters, why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS5yYZdLehiI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
