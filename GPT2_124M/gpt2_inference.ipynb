{"cells":[{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T14:53:45.965494Z","iopub.status.busy":"2024-10-01T14:53:45.965126Z","iopub.status.idle":"2024-10-01T14:53:57.419000Z","shell.execute_reply":"2024-10-01T14:53:57.417651Z","shell.execute_reply.started":"2024-10-01T14:53:45.965459Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"]}],"source":["!pip install tiktoken\n","from dataclasses import dataclass \n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import tiktoken\n","enc = tiktoken.get_encoding('gpt2')\n","import inspect"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T15:09:39.371583Z","iopub.status.busy":"2024-10-01T15:09:39.371173Z","iopub.status.idle":"2024-10-01T15:09:42.694234Z","shell.execute_reply":"2024-10-01T15:09:42.693320Z","shell.execute_reply.started":"2024-10-01T15:09:39.371543Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on cuda\n","Loaded The model Successfuly!\n"]}],"source":["@dataclass \n","class GPTConfig:\n","    block_size: int = 1024 # max sequence length\n","    vocab_size: int = 50257 # vocab size of GPT-2 50,000 BPE merges + 256 single byte tokens + 1 special token <|endoftext|>\n","    n_layer: int = 12 # number of layers (how many times we repeat the block)\n","    n_head: int = 12 # number of heads in the multi-head attention\n","    n_embed: int = 768 # embedding dimension, so the head size is 768/12 = 64\n","\n","\n","        \n","class GPT(nn.Module):\n","    def __init__(self, config: GPTConfig):\n","        super().__init__()\n","        self.config = config\n","\n","        self.transformer = nn.ModuleDict(\n","        dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embed),\n","            wpe = nn.Embedding(config.block_size, config.n_embed),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = nn.LayerNorm(config.n_embed)\n","        )\n","        )\n","        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False) # projects the n_embd features to vocab_size\n","\n","        # parameter sharing between the embedding weights and the final linear layer\n","        self.transformer.wte.weight = self.lm_head.weight # we redirect the pointer of the embedding weights to the linear layer weights, the old embedding weights are orphaned, and python will garbage collect them\n","\n","        # initialize the parameters, we call the apply method on self -which is a method implemented in nn.Module, it will iterate over all the submodules and apply the function to them-\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            std = 0.02\n","            # if has the NANOGPT_SCALE_INIT attribute, scale the std\n","            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n","                std *= (2 * self.config.n_layer)**-0.5\n","            # initialize the weights of the linear layer with a normal distribution of mean 0 and std 0.02\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n","            # if the linear layer has a bias, initialize it to zeros (by default pytorch initializes the bias to uniform)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","    \n","    def forward(self,x,y=None):\n","        # input x is the token sequence a tensor of shape (B,T) where B is the batch size and T is the sequence length\n","        B,T = x.size()\n","        assert T <= self.config.block_size, \"Length of input tokens exceeds block size\"\n","        ## get the token embeddings\n","        token_embeddings = self.transformer.wte(x) # shape (B,T,n_embed)\n","        \n","        ## get the positional encodings\n","        pos = torch.arange(0, T, dtype=torch.long, device=x.device) # position indices shape (T)\n","        pos = self.transformer.wpe(pos) # convert them to embeddings, shape (T,n_embed)\n","        \n","        ## sum the token embeddings and positional embeddings\n","        x = token_embeddings + pos # shape (B,T,n_embed), the positional embeddings are broadcasted along the batch dimension\n","        \n","        ## forward through all the transformer blocks\n","        for block in self.transformer.h:\n","            x = block(x) # takes input of shape (B,T,n_embed) and returns the same shape\n","        # forward the final layer normalization and classifier\n","        x = self.transformer.ln_f(x) # shape (B,T,n_embed)\n","        logits = self.lm_head(x) # shape (B,T,vocab_size)\n","\n","        loss = None\n","        if y is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1)) # cross entropy loss\n","        \n","        return logits, loss\n","\n","    @classmethod\n","    def from_pretrained(cls, model_type):\n","        \"\"\" Loads pretrained GPT-2 model weights from HuggingFace \"\"\"\n","        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n","        from transformers import GPT2LMHeadModel\n","        print(f\"Loading {model_type} model weights\")\n","        \n","        ## Prepare the configuration\n","        # n_layer, n_head, and n_embed are determined from the model_type\n","        config_args = {\n","            'gpt2':        dict(n_layer=12, n_head=12, n_embed=768), # 124M params\n","            'gpt2-medium': dict(n_layer=24, n_head=16, n_embed=1024), # 350M params\n","            'gpt2-large':  dict(n_layer=36, n_head=20, n_embed=1280), # 774M params\n","            'gpt2-xl':     dict(n_layer=48, n_head=25, n_embed=1600) # 1558M params\n","        }[model_type]\n","\n","        config_args['vocab_size'] = 50257 # the same for all GPT-2 models\n","        config_args['block_size'] = 1024 # the same for all GPT-2 models\n","\n","        # initialize the model (our implementation)\n","        config = GPTConfig(**config_args)\n","        model = GPT(config)\n","        sd = model.state_dict()\n","        sd_keys = sd.keys()\n","        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard the masks/buffers, not a parameter so we don't need to copy it\n","\n","        # inita hugging face transformer model\n","        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n","        # get its state dict\n","        sd_hf = model_hf.state_dict()\n","        sd_keys_hf = sd_hf.keys()\n","        #mine: these buffers are not in hugging face state dict anyway\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n","        # some of the weights in the hugging face model are transposed, so we need to transpose them back before copying them\n","        # this comes from the tensorflow repo\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","\n","        # copy while ensuring all of the parameters are aligned and match in names and shapes\n","        assert len(sd_keys_hf) == len(sd_keys), \"Mismatched Keys {} != {}\".format(len(sd_keys_hf), len(sd_keys))\n","        for k in sd_keys_hf:\n","            if any(k.endswith(w) for w in transposed):\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t()) # copy_ is an inplace copy, t() is the transpose\n","            else:\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","\n","        return model \n","\n","    \n","    \n","    def generate(self, prompt,num_return_sequences = 4, max_length = 32, seed = 1337):\n","        # get the prefix tokens\n","        tokens = enc.encode(prompt)\n","        tokens = torch.tensor(tokens, dtype=torch.long) # shape (T)\n","        x = tokens.unsqueeze(0).repeat(num_return_sequences, 1).to(device)  # shape (num_return_sequences, T)\n","        # we created a genrator object in pytorch sepciically for the sampling\n","        # we don't want to affect the global random state that is used for training\n","        sample_rng = torch.Generator(device=device) # create a generator for the sampling\n","        # we seed it differently for every rank, and we will make them all print their generations\n","        sample_rng.manual_seed(seed)\n","        while x.size(1) < max_length:\n","            # forward the model to get the logits\n","            with torch.no_grad():\n","                with torch.autocast(device_type=device, dtype=torch.float16):\n","                    logits, loss = model(x)\n","                # take the logits at the last position, we only care about the last token's logits\n","                logits = logits[:, -1, :] # shape (num_return_sequences, vocab_size)\n","                # get the probabilities by applying softmax\n","                probs = F.softmax(logits, dim=-1)\n","                # do top-k sampling of k = 50 (in which we get the top k tokens and sample from them)\n","                # this is hugging face's pipeline default\n","                # topk_probs and topk_indices are of shape (num_return_sequences, 50)\n","                topk_props, topk_indices = torch.topk(probs, 50, dim=-1) # get the top 50 tokens and their probabilities\n","                # sample a token from the top 50 tokens\n","                ix = torch.multinomial(topk_props, num_samples=1, generator=sample_rng) # the indices of chosen tokens (in range 0-49)\n","                # use the indices to index to the actual indices (get the actual tokens)\n","                next_token = torch.gather(topk_indices, -1, ix) # use the indices to index to the actual indices \n","                # append the next token to the sequence\n","                x = torch.cat((x, next_token), dim=1)\n","\n","        # decode the tokens\n","        for i in range(num_return_sequences):\n","            tokens = x[i].tolist()\n","            decoded = enc.decode(tokens)\n","            print(f\"Response {i+1}: {decoded}\")\n","    \n","\n","class Block(nn.Module):\n","    def __init__(self, config: GPTConfig):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embed)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embed)\n","        self.mlp  = MLP(config)\n","\n","    def forward(self,x):\n","        x = x + self.attn(self.ln_1(x)) # communication\n","        x = x + self.mlp(self.ln_2(x)) # computation, to think on what they gathered\n","        return x\n","\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)\n","        self.gelu = nn.GELU(approximate='tanh')\n","        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","    def forward(self,x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        return x\n","\n","class CausalSelfAttention(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        assert config.n_embed % config.n_head == 0, 'n_embed should be divisible by n_head'\n","        # key, Query, and Value projections for all heads, but in a batch (mine: instead of separate matrices Key, Query, and Value)\n","        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed) # we concatenated all key, query, and value in a single matrix (each one is n_embed which is further concatenation of n_head*head_size -so each is the concatenation of all heads-)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n","        # the mask, but we call it bias to match the huggingFace state Dict \n","        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size)) # reshape it to 4D tensor (1,1,block_size,block_size), so it will be reshaped later for all examples and heads\n","        \n","        self.n_head  = config.n_head\n","        self.n_embed = config.n_embed\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","        \n","    def forward(self,x):\n","        B,T,C = x.size() # batch_size, sequence_length, n_embed\n","        qkv = self.c_attn(x) # batch_size, sequence_length, 3 * n_embed \n","        q,k,v = qkv.split(self.n_embed,dim=2) # batch_size, sequence_length, n_embed for each (that is for all heads, each head will have part of that n_embed, precisely n_head = n_embed/n_head)\n","        # further split the q,k,v into multiple heads\n","        head_size = C // self.n_head # head_size = n_embed // number of heads\n","        k = k.view(B,T, self.n_head, head_size).transpose(1,2) # (batch_size, n_head, sequence_length, head_size), notice that we first reshaped the n_embed to n_head*head_size, then transposed\n","        q = q.view(B,T, self.n_head, head_size).transpose(1,2) # (batch_size, n_head, sequence_length, head_size)\n","        v = v.view(B,T, self.n_head, head_size).transpose(1,2) # (batch_size, n_head, sequence_length, head_size)\n","\n","        # # compute the attention scores (affinities) for each example and each head\n","        # wei = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1))) # (batch_size, n_head, sequence_length, sequence_length), then divide by sqrt(head_size) to normalize\n","        # # discard the future tokens for each token\n","        # wei = wei.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # mask the future tokens\n","        # # apply the softmax to get the attention weights\n","        # wei = F.softmax(wei, dim=-1)\n","        # # use the attention weights to get the weighted sum of the values\n","        # y = wei @ v # (batch_size, n_head, sequence_length, sequence_length) @ (batch_size, n_head, sequence_length, head_size) = (batch_size, n_head, sequence_length, head_size)\n","        \n","        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n","        # concatenate the heads together \n","        y = y.transpose(1,2).contiguous().view(B,T,C) # first transpose to (batch_size, sequence_length, n_head, head_size) then contiguous to make sure the memory is contiguous, then view to (batch_size, sequence_length, n_embed = n_head * head_size)\n","\n","        # output projection\n","        y = self.c_proj(y)\n","        return y\n","\n","\n","# Vanilla, non-DDP training\n","ddp_rank = 0\n","ddp_local_rank = 0\n","ddp_world_size = 1\n","master_process = True\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Running on {device}\")\n","\n","\n","\n","model = GPT(GPTConfig(vocab_size=50304))\n","model.to(device)\n","\n","\n","checkpoint_path = '/kaggle/input/gpt2-124m-parameters-base-model/gpt2_checkpoint.pth'\n","checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n","model.load_state_dict(checkpoint['model'])\n","### Generation\n","model.eval()\n","print(f\"Loaded The model Successfuly!\")"]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T15:11:48.619517Z","iopub.status.busy":"2024-10-01T15:11:48.618909Z","iopub.status.idle":"2024-10-01T15:11:49.021701Z","shell.execute_reply":"2024-10-01T15:11:49.020770Z","shell.execute_reply.started":"2024-10-01T15:11:48.619466Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["sample 0: There are a lot of ways to take care of your health. one of them is to take the vitamins you need to properly look after your health. Many people also take their iron well enough. It’s a good idea to take a little\n","sample 1: There are a lot of ways to take care of your health. one of them is keeping your teeth and gums healthy. Make sure that you do not over or under-exhort every day. You should also be getting regular checkups.\n","sample 2: There are a lot of ways to take care of your health. one of them is to eat plenty of vegetables and fruits. Some people think their health is on the whole important thing. But others may think it’s all a lot more.\n","sample 3: There are a lot of ways to take care of your health. one of them is by giving yourself some time. try to be flexible with yourself and to work on some.\n","Another option here is the opportunity to volunteer in your organization instead of by\n"]}],"source":["num_return_sequences = 4 # the number of responses to generate\n","max_length = 50 # the length of the response\n","seed = 1337 # the seed for the random number generator\n","prompt = \"There are a lot of ways to take care of your health. one of them is\"\n","# remove trailing spaces from the end of the prompt\n","prompt = prompt.rstrip()\n","model.generate(prompt ,num_return_sequences ,max_length ,seed)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5799411,"sourceId":9524216,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
