{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181ad0d0",
   "metadata": {
    "id": "181ad0d0"
   },
   "source": [
    "# project 2\n",
    "\n",
    "# Name: yousef mohamed elsharkawy\n",
    "\n",
    "# Id: 18Q4486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8iXDsOu07XGW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iXDsOu07XGW",
    "outputId": "30b37758-9d9d-445b-8221-bcf06965ee52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rSQPC5JV7kiN",
   "metadata": {
    "id": "rSQPC5JV7kiN"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "GSHNZPbD8Pa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSHNZPbD8Pa1",
    "outputId": "791e8dd7-0066-4b08-8cf8-89fd87cabe3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/Datasets/fashionMnist2\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/Datasets/fashionMnist2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ZuUx46H79aL4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuUx46H79aL4",
    "outputId": "8f51f075-64a6-4eb8-9e78-8f3eafc7032d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion-mnist_test.csv   kaggle.json             Train_data_modified.csv\n",
      "fashion-mnist_train.csv  t10k-images-idx3-ubyte  train-images-idx3-ubyte\n",
      "fashionmnist.zip         t10k-labels-idx1-ubyte  train-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "y6uM_QsB9CAP",
   "metadata": {
    "id": "y6uM_QsB9CAP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Datasets/fashionMnist2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0DVRHx7S9MTI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DVRHx7S9MTI",
    "outputId": "5617ef73-d5d8-4176-b5b7-1861182f5822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashionmnist.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "XNfMiaRH9ch8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNfMiaRH9ch8",
    "outputId": "76439290-cddf-4035-e1ac-1ce10fedfdc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fashionmnist.zip\n",
      "replace fashion-mnist_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip fashionmnist.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2f5b3",
   "metadata": {
    "id": "1dc2f5b3"
   },
   "source": [
    "## read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "139f8aab",
   "metadata": {
    "id": "139f8aab"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C_043LNw5QWP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "C_043LNw5QWP",
    "outputId": "3d4fd468-a2cf-4330-e8e2-ce75f3a59c8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-fcc629dc-9399-4e90-9d84-b06a77675cc8\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-fcc629dc-9399-4e90-9d84-b06a77675cc8\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5c2e8a8d365b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m   \"\"\"\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    145\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    146\u001b[0m           input_id=input_id, output_id=output_id))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15c13262",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "15c13262",
    "outputId": "33045e3f-10d7-4eaf-e29f-c1d2a8001c53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5e7b947b-b16e-48bb-9a5a-919d2c825e67\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e7b947b-b16e-48bb-9a5a-919d2c825e67')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5e7b947b-b16e-48bb-9a5a-919d2c825e67 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5e7b947b-b16e-48bb-9a5a-919d2c825e67');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fashion-mnist_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7069a",
   "metadata": {
    "id": "7fa7069a"
   },
   "source": [
    "## Describe the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fe3b2a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fe3b2a6",
    "outputId": "64413a73-09b4-4113-98da-d940c86098af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abd10f02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abd10f02",
    "outputId": "6bfd8e9d-91de-4e90-a1b2-19fd3b1b8823"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label       int64\n",
       "pixel1      int64\n",
       "pixel2      int64\n",
       "pixel3      int64\n",
       "pixel4      int64\n",
       "            ...  \n",
       "pixel780    int64\n",
       "pixel781    int64\n",
       "pixel782    int64\n",
       "pixel783    int64\n",
       "pixel784    int64\n",
       "Length: 785, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07c4df35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07c4df35",
    "outputId": "557df941-552b-452b-e9e9-87c94a39c746"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label        10\n",
       "pixel1        8\n",
       "pixel2       17\n",
       "pixel3       27\n",
       "pixel4       65\n",
       "           ... \n",
       "pixel780    251\n",
       "pixel781    244\n",
       "pixel782    233\n",
       "pixel783    185\n",
       "pixel784     65\n",
       "Length: 785, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5781cf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "d5781cf2",
    "outputId": "9a43e183-d732-4a94-8870-67843a1e416f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d2a172d3-7de8-4dd5-9376-93f0e41d4881\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.035333</td>\n",
       "      <td>0.101933</td>\n",
       "      <td>0.247967</td>\n",
       "      <td>0.411467</td>\n",
       "      <td>0.805767</td>\n",
       "      <td>2.198283</td>\n",
       "      <td>5.682000</td>\n",
       "      <td>...</td>\n",
       "      <td>34.625400</td>\n",
       "      <td>23.300683</td>\n",
       "      <td>16.588267</td>\n",
       "      <td>17.869433</td>\n",
       "      <td>22.814817</td>\n",
       "      <td>17.911483</td>\n",
       "      <td>8.520633</td>\n",
       "      <td>2.753300</td>\n",
       "      <td>0.855517</td>\n",
       "      <td>0.07025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.872305</td>\n",
       "      <td>0.094689</td>\n",
       "      <td>0.271011</td>\n",
       "      <td>1.222324</td>\n",
       "      <td>2.452871</td>\n",
       "      <td>4.306912</td>\n",
       "      <td>5.836188</td>\n",
       "      <td>8.215169</td>\n",
       "      <td>14.093378</td>\n",
       "      <td>23.819481</td>\n",
       "      <td>...</td>\n",
       "      <td>57.545242</td>\n",
       "      <td>48.854427</td>\n",
       "      <td>41.979611</td>\n",
       "      <td>43.966032</td>\n",
       "      <td>51.830477</td>\n",
       "      <td>45.149388</td>\n",
       "      <td>29.614859</td>\n",
       "      <td>17.397652</td>\n",
       "      <td>9.356960</td>\n",
       "      <td>2.12587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>170.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2a172d3-7de8-4dd5-9376-93f0e41d4881')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d2a172d3-7de8-4dd5-9376-93f0e41d4881 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d2a172d3-7de8-4dd5-9376-93f0e41d4881');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              label        pixel1        pixel2        pixel3        pixel4  \\\n",
       "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000   \n",
       "mean       4.500000      0.000900      0.006150      0.035333      0.101933   \n",
       "std        2.872305      0.094689      0.271011      1.222324      2.452871   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        2.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        4.500000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        7.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        9.000000     16.000000     36.000000    226.000000    164.000000   \n",
       "\n",
       "             pixel5        pixel6        pixel7        pixel8        pixel9  \\\n",
       "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000   \n",
       "mean       0.247967      0.411467      0.805767      2.198283      5.682000   \n",
       "std        4.306912      5.836188      8.215169     14.093378     23.819481   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max      227.000000    230.000000    224.000000    255.000000    254.000000   \n",
       "\n",
       "       ...      pixel775      pixel776      pixel777      pixel778  \\\n",
       "count  ...  60000.000000  60000.000000  60000.000000  60000.000000   \n",
       "mean   ...     34.625400     23.300683     16.588267     17.869433   \n",
       "std    ...     57.545242     48.854427     41.979611     43.966032   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...     58.000000      9.000000      0.000000      0.000000   \n",
       "max    ...    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000   \n",
       "mean      22.814817     17.911483      8.520633      2.753300      0.855517   \n",
       "std       51.830477     45.149388     29.614859     17.397652      9.356960   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "          pixel784  \n",
       "count  60000.00000  \n",
       "mean       0.07025  \n",
       "std        2.12587  \n",
       "min        0.00000  \n",
       "25%        0.00000  \n",
       "50%        0.00000  \n",
       "75%        0.00000  \n",
       "max      170.00000  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d5f81",
   "metadata": {
    "id": "b48d5f81"
   },
   "source": [
    "## Clean the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a7c1c",
   "metadata": {
    "id": "c23a7c1c"
   },
   "source": [
    "### check for missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4417e7d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4417e7d7",
    "outputId": "f6530779-8c00-4c03-f853-be63f572f7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 785 columns):\n",
      " #    Column    Non-Null Count  Dtype\n",
      "---   ------    --------------  -----\n",
      " 0    label     60000 non-null  int64\n",
      " 1    pixel1    60000 non-null  int64\n",
      " 2    pixel2    60000 non-null  int64\n",
      " 3    pixel3    60000 non-null  int64\n",
      " 4    pixel4    60000 non-null  int64\n",
      " 5    pixel5    60000 non-null  int64\n",
      " 6    pixel6    60000 non-null  int64\n",
      " 7    pixel7    60000 non-null  int64\n",
      " 8    pixel8    60000 non-null  int64\n",
      " 9    pixel9    60000 non-null  int64\n",
      " 10   pixel10   60000 non-null  int64\n",
      " 11   pixel11   60000 non-null  int64\n",
      " 12   pixel12   60000 non-null  int64\n",
      " 13   pixel13   60000 non-null  int64\n",
      " 14   pixel14   60000 non-null  int64\n",
      " 15   pixel15   60000 non-null  int64\n",
      " 16   pixel16   60000 non-null  int64\n",
      " 17   pixel17   60000 non-null  int64\n",
      " 18   pixel18   60000 non-null  int64\n",
      " 19   pixel19   60000 non-null  int64\n",
      " 20   pixel20   60000 non-null  int64\n",
      " 21   pixel21   60000 non-null  int64\n",
      " 22   pixel22   60000 non-null  int64\n",
      " 23   pixel23   60000 non-null  int64\n",
      " 24   pixel24   60000 non-null  int64\n",
      " 25   pixel25   60000 non-null  int64\n",
      " 26   pixel26   60000 non-null  int64\n",
      " 27   pixel27   60000 non-null  int64\n",
      " 28   pixel28   60000 non-null  int64\n",
      " 29   pixel29   60000 non-null  int64\n",
      " 30   pixel30   60000 non-null  int64\n",
      " 31   pixel31   60000 non-null  int64\n",
      " 32   pixel32   60000 non-null  int64\n",
      " 33   pixel33   60000 non-null  int64\n",
      " 34   pixel34   60000 non-null  int64\n",
      " 35   pixel35   60000 non-null  int64\n",
      " 36   pixel36   60000 non-null  int64\n",
      " 37   pixel37   60000 non-null  int64\n",
      " 38   pixel38   60000 non-null  int64\n",
      " 39   pixel39   60000 non-null  int64\n",
      " 40   pixel40   60000 non-null  int64\n",
      " 41   pixel41   60000 non-null  int64\n",
      " 42   pixel42   60000 non-null  int64\n",
      " 43   pixel43   60000 non-null  int64\n",
      " 44   pixel44   60000 non-null  int64\n",
      " 45   pixel45   60000 non-null  int64\n",
      " 46   pixel46   60000 non-null  int64\n",
      " 47   pixel47   60000 non-null  int64\n",
      " 48   pixel48   60000 non-null  int64\n",
      " 49   pixel49   60000 non-null  int64\n",
      " 50   pixel50   60000 non-null  int64\n",
      " 51   pixel51   60000 non-null  int64\n",
      " 52   pixel52   60000 non-null  int64\n",
      " 53   pixel53   60000 non-null  int64\n",
      " 54   pixel54   60000 non-null  int64\n",
      " 55   pixel55   60000 non-null  int64\n",
      " 56   pixel56   60000 non-null  int64\n",
      " 57   pixel57   60000 non-null  int64\n",
      " 58   pixel58   60000 non-null  int64\n",
      " 59   pixel59   60000 non-null  int64\n",
      " 60   pixel60   60000 non-null  int64\n",
      " 61   pixel61   60000 non-null  int64\n",
      " 62   pixel62   60000 non-null  int64\n",
      " 63   pixel63   60000 non-null  int64\n",
      " 64   pixel64   60000 non-null  int64\n",
      " 65   pixel65   60000 non-null  int64\n",
      " 66   pixel66   60000 non-null  int64\n",
      " 67   pixel67   60000 non-null  int64\n",
      " 68   pixel68   60000 non-null  int64\n",
      " 69   pixel69   60000 non-null  int64\n",
      " 70   pixel70   60000 non-null  int64\n",
      " 71   pixel71   60000 non-null  int64\n",
      " 72   pixel72   60000 non-null  int64\n",
      " 73   pixel73   60000 non-null  int64\n",
      " 74   pixel74   60000 non-null  int64\n",
      " 75   pixel75   60000 non-null  int64\n",
      " 76   pixel76   60000 non-null  int64\n",
      " 77   pixel77   60000 non-null  int64\n",
      " 78   pixel78   60000 non-null  int64\n",
      " 79   pixel79   60000 non-null  int64\n",
      " 80   pixel80   60000 non-null  int64\n",
      " 81   pixel81   60000 non-null  int64\n",
      " 82   pixel82   60000 non-null  int64\n",
      " 83   pixel83   60000 non-null  int64\n",
      " 84   pixel84   60000 non-null  int64\n",
      " 85   pixel85   60000 non-null  int64\n",
      " 86   pixel86   60000 non-null  int64\n",
      " 87   pixel87   60000 non-null  int64\n",
      " 88   pixel88   60000 non-null  int64\n",
      " 89   pixel89   60000 non-null  int64\n",
      " 90   pixel90   60000 non-null  int64\n",
      " 91   pixel91   60000 non-null  int64\n",
      " 92   pixel92   60000 non-null  int64\n",
      " 93   pixel93   60000 non-null  int64\n",
      " 94   pixel94   60000 non-null  int64\n",
      " 95   pixel95   60000 non-null  int64\n",
      " 96   pixel96   60000 non-null  int64\n",
      " 97   pixel97   60000 non-null  int64\n",
      " 98   pixel98   60000 non-null  int64\n",
      " 99   pixel99   60000 non-null  int64\n",
      " 100  pixel100  60000 non-null  int64\n",
      " 101  pixel101  60000 non-null  int64\n",
      " 102  pixel102  60000 non-null  int64\n",
      " 103  pixel103  60000 non-null  int64\n",
      " 104  pixel104  60000 non-null  int64\n",
      " 105  pixel105  60000 non-null  int64\n",
      " 106  pixel106  60000 non-null  int64\n",
      " 107  pixel107  60000 non-null  int64\n",
      " 108  pixel108  60000 non-null  int64\n",
      " 109  pixel109  60000 non-null  int64\n",
      " 110  pixel110  60000 non-null  int64\n",
      " 111  pixel111  60000 non-null  int64\n",
      " 112  pixel112  60000 non-null  int64\n",
      " 113  pixel113  60000 non-null  int64\n",
      " 114  pixel114  60000 non-null  int64\n",
      " 115  pixel115  60000 non-null  int64\n",
      " 116  pixel116  60000 non-null  int64\n",
      " 117  pixel117  60000 non-null  int64\n",
      " 118  pixel118  60000 non-null  int64\n",
      " 119  pixel119  60000 non-null  int64\n",
      " 120  pixel120  60000 non-null  int64\n",
      " 121  pixel121  60000 non-null  int64\n",
      " 122  pixel122  60000 non-null  int64\n",
      " 123  pixel123  60000 non-null  int64\n",
      " 124  pixel124  60000 non-null  int64\n",
      " 125  pixel125  60000 non-null  int64\n",
      " 126  pixel126  60000 non-null  int64\n",
      " 127  pixel127  60000 non-null  int64\n",
      " 128  pixel128  60000 non-null  int64\n",
      " 129  pixel129  60000 non-null  int64\n",
      " 130  pixel130  60000 non-null  int64\n",
      " 131  pixel131  60000 non-null  int64\n",
      " 132  pixel132  60000 non-null  int64\n",
      " 133  pixel133  60000 non-null  int64\n",
      " 134  pixel134  60000 non-null  int64\n",
      " 135  pixel135  60000 non-null  int64\n",
      " 136  pixel136  60000 non-null  int64\n",
      " 137  pixel137  60000 non-null  int64\n",
      " 138  pixel138  60000 non-null  int64\n",
      " 139  pixel139  60000 non-null  int64\n",
      " 140  pixel140  60000 non-null  int64\n",
      " 141  pixel141  60000 non-null  int64\n",
      " 142  pixel142  60000 non-null  int64\n",
      " 143  pixel143  60000 non-null  int64\n",
      " 144  pixel144  60000 non-null  int64\n",
      " 145  pixel145  60000 non-null  int64\n",
      " 146  pixel146  60000 non-null  int64\n",
      " 147  pixel147  60000 non-null  int64\n",
      " 148  pixel148  60000 non-null  int64\n",
      " 149  pixel149  60000 non-null  int64\n",
      " 150  pixel150  60000 non-null  int64\n",
      " 151  pixel151  60000 non-null  int64\n",
      " 152  pixel152  60000 non-null  int64\n",
      " 153  pixel153  60000 non-null  int64\n",
      " 154  pixel154  60000 non-null  int64\n",
      " 155  pixel155  60000 non-null  int64\n",
      " 156  pixel156  60000 non-null  int64\n",
      " 157  pixel157  60000 non-null  int64\n",
      " 158  pixel158  60000 non-null  int64\n",
      " 159  pixel159  60000 non-null  int64\n",
      " 160  pixel160  60000 non-null  int64\n",
      " 161  pixel161  60000 non-null  int64\n",
      " 162  pixel162  60000 non-null  int64\n",
      " 163  pixel163  60000 non-null  int64\n",
      " 164  pixel164  60000 non-null  int64\n",
      " 165  pixel165  60000 non-null  int64\n",
      " 166  pixel166  60000 non-null  int64\n",
      " 167  pixel167  60000 non-null  int64\n",
      " 168  pixel168  60000 non-null  int64\n",
      " 169  pixel169  60000 non-null  int64\n",
      " 170  pixel170  60000 non-null  int64\n",
      " 171  pixel171  60000 non-null  int64\n",
      " 172  pixel172  60000 non-null  int64\n",
      " 173  pixel173  60000 non-null  int64\n",
      " 174  pixel174  60000 non-null  int64\n",
      " 175  pixel175  60000 non-null  int64\n",
      " 176  pixel176  60000 non-null  int64\n",
      " 177  pixel177  60000 non-null  int64\n",
      " 178  pixel178  60000 non-null  int64\n",
      " 179  pixel179  60000 non-null  int64\n",
      " 180  pixel180  60000 non-null  int64\n",
      " 181  pixel181  60000 non-null  int64\n",
      " 182  pixel182  60000 non-null  int64\n",
      " 183  pixel183  60000 non-null  int64\n",
      " 184  pixel184  60000 non-null  int64\n",
      " 185  pixel185  60000 non-null  int64\n",
      " 186  pixel186  60000 non-null  int64\n",
      " 187  pixel187  60000 non-null  int64\n",
      " 188  pixel188  60000 non-null  int64\n",
      " 189  pixel189  60000 non-null  int64\n",
      " 190  pixel190  60000 non-null  int64\n",
      " 191  pixel191  60000 non-null  int64\n",
      " 192  pixel192  60000 non-null  int64\n",
      " 193  pixel193  60000 non-null  int64\n",
      " 194  pixel194  60000 non-null  int64\n",
      " 195  pixel195  60000 non-null  int64\n",
      " 196  pixel196  60000 non-null  int64\n",
      " 197  pixel197  60000 non-null  int64\n",
      " 198  pixel198  60000 non-null  int64\n",
      " 199  pixel199  60000 non-null  int64\n",
      " 200  pixel200  60000 non-null  int64\n",
      " 201  pixel201  60000 non-null  int64\n",
      " 202  pixel202  60000 non-null  int64\n",
      " 203  pixel203  60000 non-null  int64\n",
      " 204  pixel204  60000 non-null  int64\n",
      " 205  pixel205  60000 non-null  int64\n",
      " 206  pixel206  60000 non-null  int64\n",
      " 207  pixel207  60000 non-null  int64\n",
      " 208  pixel208  60000 non-null  int64\n",
      " 209  pixel209  60000 non-null  int64\n",
      " 210  pixel210  60000 non-null  int64\n",
      " 211  pixel211  60000 non-null  int64\n",
      " 212  pixel212  60000 non-null  int64\n",
      " 213  pixel213  60000 non-null  int64\n",
      " 214  pixel214  60000 non-null  int64\n",
      " 215  pixel215  60000 non-null  int64\n",
      " 216  pixel216  60000 non-null  int64\n",
      " 217  pixel217  60000 non-null  int64\n",
      " 218  pixel218  60000 non-null  int64\n",
      " 219  pixel219  60000 non-null  int64\n",
      " 220  pixel220  60000 non-null  int64\n",
      " 221  pixel221  60000 non-null  int64\n",
      " 222  pixel222  60000 non-null  int64\n",
      " 223  pixel223  60000 non-null  int64\n",
      " 224  pixel224  60000 non-null  int64\n",
      " 225  pixel225  60000 non-null  int64\n",
      " 226  pixel226  60000 non-null  int64\n",
      " 227  pixel227  60000 non-null  int64\n",
      " 228  pixel228  60000 non-null  int64\n",
      " 229  pixel229  60000 non-null  int64\n",
      " 230  pixel230  60000 non-null  int64\n",
      " 231  pixel231  60000 non-null  int64\n",
      " 232  pixel232  60000 non-null  int64\n",
      " 233  pixel233  60000 non-null  int64\n",
      " 234  pixel234  60000 non-null  int64\n",
      " 235  pixel235  60000 non-null  int64\n",
      " 236  pixel236  60000 non-null  int64\n",
      " 237  pixel237  60000 non-null  int64\n",
      " 238  pixel238  60000 non-null  int64\n",
      " 239  pixel239  60000 non-null  int64\n",
      " 240  pixel240  60000 non-null  int64\n",
      " 241  pixel241  60000 non-null  int64\n",
      " 242  pixel242  60000 non-null  int64\n",
      " 243  pixel243  60000 non-null  int64\n",
      " 244  pixel244  60000 non-null  int64\n",
      " 245  pixel245  60000 non-null  int64\n",
      " 246  pixel246  60000 non-null  int64\n",
      " 247  pixel247  60000 non-null  int64\n",
      " 248  pixel248  60000 non-null  int64\n",
      " 249  pixel249  60000 non-null  int64\n",
      " 250  pixel250  60000 non-null  int64\n",
      " 251  pixel251  60000 non-null  int64\n",
      " 252  pixel252  60000 non-null  int64\n",
      " 253  pixel253  60000 non-null  int64\n",
      " 254  pixel254  60000 non-null  int64\n",
      " 255  pixel255  60000 non-null  int64\n",
      " 256  pixel256  60000 non-null  int64\n",
      " 257  pixel257  60000 non-null  int64\n",
      " 258  pixel258  60000 non-null  int64\n",
      " 259  pixel259  60000 non-null  int64\n",
      " 260  pixel260  60000 non-null  int64\n",
      " 261  pixel261  60000 non-null  int64\n",
      " 262  pixel262  60000 non-null  int64\n",
      " 263  pixel263  60000 non-null  int64\n",
      " 264  pixel264  60000 non-null  int64\n",
      " 265  pixel265  60000 non-null  int64\n",
      " 266  pixel266  60000 non-null  int64\n",
      " 267  pixel267  60000 non-null  int64\n",
      " 268  pixel268  60000 non-null  int64\n",
      " 269  pixel269  60000 non-null  int64\n",
      " 270  pixel270  60000 non-null  int64\n",
      " 271  pixel271  60000 non-null  int64\n",
      " 272  pixel272  60000 non-null  int64\n",
      " 273  pixel273  60000 non-null  int64\n",
      " 274  pixel274  60000 non-null  int64\n",
      " 275  pixel275  60000 non-null  int64\n",
      " 276  pixel276  60000 non-null  int64\n",
      " 277  pixel277  60000 non-null  int64\n",
      " 278  pixel278  60000 non-null  int64\n",
      " 279  pixel279  60000 non-null  int64\n",
      " 280  pixel280  60000 non-null  int64\n",
      " 281  pixel281  60000 non-null  int64\n",
      " 282  pixel282  60000 non-null  int64\n",
      " 283  pixel283  60000 non-null  int64\n",
      " 284  pixel284  60000 non-null  int64\n",
      " 285  pixel285  60000 non-null  int64\n",
      " 286  pixel286  60000 non-null  int64\n",
      " 287  pixel287  60000 non-null  int64\n",
      " 288  pixel288  60000 non-null  int64\n",
      " 289  pixel289  60000 non-null  int64\n",
      " 290  pixel290  60000 non-null  int64\n",
      " 291  pixel291  60000 non-null  int64\n",
      " 292  pixel292  60000 non-null  int64\n",
      " 293  pixel293  60000 non-null  int64\n",
      " 294  pixel294  60000 non-null  int64\n",
      " 295  pixel295  60000 non-null  int64\n",
      " 296  pixel296  60000 non-null  int64\n",
      " 297  pixel297  60000 non-null  int64\n",
      " 298  pixel298  60000 non-null  int64\n",
      " 299  pixel299  60000 non-null  int64\n",
      " 300  pixel300  60000 non-null  int64\n",
      " 301  pixel301  60000 non-null  int64\n",
      " 302  pixel302  60000 non-null  int64\n",
      " 303  pixel303  60000 non-null  int64\n",
      " 304  pixel304  60000 non-null  int64\n",
      " 305  pixel305  60000 non-null  int64\n",
      " 306  pixel306  60000 non-null  int64\n",
      " 307  pixel307  60000 non-null  int64\n",
      " 308  pixel308  60000 non-null  int64\n",
      " 309  pixel309  60000 non-null  int64\n",
      " 310  pixel310  60000 non-null  int64\n",
      " 311  pixel311  60000 non-null  int64\n",
      " 312  pixel312  60000 non-null  int64\n",
      " 313  pixel313  60000 non-null  int64\n",
      " 314  pixel314  60000 non-null  int64\n",
      " 315  pixel315  60000 non-null  int64\n",
      " 316  pixel316  60000 non-null  int64\n",
      " 317  pixel317  60000 non-null  int64\n",
      " 318  pixel318  60000 non-null  int64\n",
      " 319  pixel319  60000 non-null  int64\n",
      " 320  pixel320  60000 non-null  int64\n",
      " 321  pixel321  60000 non-null  int64\n",
      " 322  pixel322  60000 non-null  int64\n",
      " 323  pixel323  60000 non-null  int64\n",
      " 324  pixel324  60000 non-null  int64\n",
      " 325  pixel325  60000 non-null  int64\n",
      " 326  pixel326  60000 non-null  int64\n",
      " 327  pixel327  60000 non-null  int64\n",
      " 328  pixel328  60000 non-null  int64\n",
      " 329  pixel329  60000 non-null  int64\n",
      " 330  pixel330  60000 non-null  int64\n",
      " 331  pixel331  60000 non-null  int64\n",
      " 332  pixel332  60000 non-null  int64\n",
      " 333  pixel333  60000 non-null  int64\n",
      " 334  pixel334  60000 non-null  int64\n",
      " 335  pixel335  60000 non-null  int64\n",
      " 336  pixel336  60000 non-null  int64\n",
      " 337  pixel337  60000 non-null  int64\n",
      " 338  pixel338  60000 non-null  int64\n",
      " 339  pixel339  60000 non-null  int64\n",
      " 340  pixel340  60000 non-null  int64\n",
      " 341  pixel341  60000 non-null  int64\n",
      " 342  pixel342  60000 non-null  int64\n",
      " 343  pixel343  60000 non-null  int64\n",
      " 344  pixel344  60000 non-null  int64\n",
      " 345  pixel345  60000 non-null  int64\n",
      " 346  pixel346  60000 non-null  int64\n",
      " 347  pixel347  60000 non-null  int64\n",
      " 348  pixel348  60000 non-null  int64\n",
      " 349  pixel349  60000 non-null  int64\n",
      " 350  pixel350  60000 non-null  int64\n",
      " 351  pixel351  60000 non-null  int64\n",
      " 352  pixel352  60000 non-null  int64\n",
      " 353  pixel353  60000 non-null  int64\n",
      " 354  pixel354  60000 non-null  int64\n",
      " 355  pixel355  60000 non-null  int64\n",
      " 356  pixel356  60000 non-null  int64\n",
      " 357  pixel357  60000 non-null  int64\n",
      " 358  pixel358  60000 non-null  int64\n",
      " 359  pixel359  60000 non-null  int64\n",
      " 360  pixel360  60000 non-null  int64\n",
      " 361  pixel361  60000 non-null  int64\n",
      " 362  pixel362  60000 non-null  int64\n",
      " 363  pixel363  60000 non-null  int64\n",
      " 364  pixel364  60000 non-null  int64\n",
      " 365  pixel365  60000 non-null  int64\n",
      " 366  pixel366  60000 non-null  int64\n",
      " 367  pixel367  60000 non-null  int64\n",
      " 368  pixel368  60000 non-null  int64\n",
      " 369  pixel369  60000 non-null  int64\n",
      " 370  pixel370  60000 non-null  int64\n",
      " 371  pixel371  60000 non-null  int64\n",
      " 372  pixel372  60000 non-null  int64\n",
      " 373  pixel373  60000 non-null  int64\n",
      " 374  pixel374  60000 non-null  int64\n",
      " 375  pixel375  60000 non-null  int64\n",
      " 376  pixel376  60000 non-null  int64\n",
      " 377  pixel377  60000 non-null  int64\n",
      " 378  pixel378  60000 non-null  int64\n",
      " 379  pixel379  60000 non-null  int64\n",
      " 380  pixel380  60000 non-null  int64\n",
      " 381  pixel381  60000 non-null  int64\n",
      " 382  pixel382  60000 non-null  int64\n",
      " 383  pixel383  60000 non-null  int64\n",
      " 384  pixel384  60000 non-null  int64\n",
      " 385  pixel385  60000 non-null  int64\n",
      " 386  pixel386  60000 non-null  int64\n",
      " 387  pixel387  60000 non-null  int64\n",
      " 388  pixel388  60000 non-null  int64\n",
      " 389  pixel389  60000 non-null  int64\n",
      " 390  pixel390  60000 non-null  int64\n",
      " 391  pixel391  60000 non-null  int64\n",
      " 392  pixel392  60000 non-null  int64\n",
      " 393  pixel393  60000 non-null  int64\n",
      " 394  pixel394  60000 non-null  int64\n",
      " 395  pixel395  60000 non-null  int64\n",
      " 396  pixel396  60000 non-null  int64\n",
      " 397  pixel397  60000 non-null  int64\n",
      " 398  pixel398  60000 non-null  int64\n",
      " 399  pixel399  60000 non-null  int64\n",
      " 400  pixel400  60000 non-null  int64\n",
      " 401  pixel401  60000 non-null  int64\n",
      " 402  pixel402  60000 non-null  int64\n",
      " 403  pixel403  60000 non-null  int64\n",
      " 404  pixel404  60000 non-null  int64\n",
      " 405  pixel405  60000 non-null  int64\n",
      " 406  pixel406  60000 non-null  int64\n",
      " 407  pixel407  60000 non-null  int64\n",
      " 408  pixel408  60000 non-null  int64\n",
      " 409  pixel409  60000 non-null  int64\n",
      " 410  pixel410  60000 non-null  int64\n",
      " 411  pixel411  60000 non-null  int64\n",
      " 412  pixel412  60000 non-null  int64\n",
      " 413  pixel413  60000 non-null  int64\n",
      " 414  pixel414  60000 non-null  int64\n",
      " 415  pixel415  60000 non-null  int64\n",
      " 416  pixel416  60000 non-null  int64\n",
      " 417  pixel417  60000 non-null  int64\n",
      " 418  pixel418  60000 non-null  int64\n",
      " 419  pixel419  60000 non-null  int64\n",
      " 420  pixel420  60000 non-null  int64\n",
      " 421  pixel421  60000 non-null  int64\n",
      " 422  pixel422  60000 non-null  int64\n",
      " 423  pixel423  60000 non-null  int64\n",
      " 424  pixel424  60000 non-null  int64\n",
      " 425  pixel425  60000 non-null  int64\n",
      " 426  pixel426  60000 non-null  int64\n",
      " 427  pixel427  60000 non-null  int64\n",
      " 428  pixel428  60000 non-null  int64\n",
      " 429  pixel429  60000 non-null  int64\n",
      " 430  pixel430  60000 non-null  int64\n",
      " 431  pixel431  60000 non-null  int64\n",
      " 432  pixel432  60000 non-null  int64\n",
      " 433  pixel433  60000 non-null  int64\n",
      " 434  pixel434  60000 non-null  int64\n",
      " 435  pixel435  60000 non-null  int64\n",
      " 436  pixel436  60000 non-null  int64\n",
      " 437  pixel437  60000 non-null  int64\n",
      " 438  pixel438  60000 non-null  int64\n",
      " 439  pixel439  60000 non-null  int64\n",
      " 440  pixel440  60000 non-null  int64\n",
      " 441  pixel441  60000 non-null  int64\n",
      " 442  pixel442  60000 non-null  int64\n",
      " 443  pixel443  60000 non-null  int64\n",
      " 444  pixel444  60000 non-null  int64\n",
      " 445  pixel445  60000 non-null  int64\n",
      " 446  pixel446  60000 non-null  int64\n",
      " 447  pixel447  60000 non-null  int64\n",
      " 448  pixel448  60000 non-null  int64\n",
      " 449  pixel449  60000 non-null  int64\n",
      " 450  pixel450  60000 non-null  int64\n",
      " 451  pixel451  60000 non-null  int64\n",
      " 452  pixel452  60000 non-null  int64\n",
      " 453  pixel453  60000 non-null  int64\n",
      " 454  pixel454  60000 non-null  int64\n",
      " 455  pixel455  60000 non-null  int64\n",
      " 456  pixel456  60000 non-null  int64\n",
      " 457  pixel457  60000 non-null  int64\n",
      " 458  pixel458  60000 non-null  int64\n",
      " 459  pixel459  60000 non-null  int64\n",
      " 460  pixel460  60000 non-null  int64\n",
      " 461  pixel461  60000 non-null  int64\n",
      " 462  pixel462  60000 non-null  int64\n",
      " 463  pixel463  60000 non-null  int64\n",
      " 464  pixel464  60000 non-null  int64\n",
      " 465  pixel465  60000 non-null  int64\n",
      " 466  pixel466  60000 non-null  int64\n",
      " 467  pixel467  60000 non-null  int64\n",
      " 468  pixel468  60000 non-null  int64\n",
      " 469  pixel469  60000 non-null  int64\n",
      " 470  pixel470  60000 non-null  int64\n",
      " 471  pixel471  60000 non-null  int64\n",
      " 472  pixel472  60000 non-null  int64\n",
      " 473  pixel473  60000 non-null  int64\n",
      " 474  pixel474  60000 non-null  int64\n",
      " 475  pixel475  60000 non-null  int64\n",
      " 476  pixel476  60000 non-null  int64\n",
      " 477  pixel477  60000 non-null  int64\n",
      " 478  pixel478  60000 non-null  int64\n",
      " 479  pixel479  60000 non-null  int64\n",
      " 480  pixel480  60000 non-null  int64\n",
      " 481  pixel481  60000 non-null  int64\n",
      " 482  pixel482  60000 non-null  int64\n",
      " 483  pixel483  60000 non-null  int64\n",
      " 484  pixel484  60000 non-null  int64\n",
      " 485  pixel485  60000 non-null  int64\n",
      " 486  pixel486  60000 non-null  int64\n",
      " 487  pixel487  60000 non-null  int64\n",
      " 488  pixel488  60000 non-null  int64\n",
      " 489  pixel489  60000 non-null  int64\n",
      " 490  pixel490  60000 non-null  int64\n",
      " 491  pixel491  60000 non-null  int64\n",
      " 492  pixel492  60000 non-null  int64\n",
      " 493  pixel493  60000 non-null  int64\n",
      " 494  pixel494  60000 non-null  int64\n",
      " 495  pixel495  60000 non-null  int64\n",
      " 496  pixel496  60000 non-null  int64\n",
      " 497  pixel497  60000 non-null  int64\n",
      " 498  pixel498  60000 non-null  int64\n",
      " 499  pixel499  60000 non-null  int64\n",
      " 500  pixel500  60000 non-null  int64\n",
      " 501  pixel501  60000 non-null  int64\n",
      " 502  pixel502  60000 non-null  int64\n",
      " 503  pixel503  60000 non-null  int64\n",
      " 504  pixel504  60000 non-null  int64\n",
      " 505  pixel505  60000 non-null  int64\n",
      " 506  pixel506  60000 non-null  int64\n",
      " 507  pixel507  60000 non-null  int64\n",
      " 508  pixel508  60000 non-null  int64\n",
      " 509  pixel509  60000 non-null  int64\n",
      " 510  pixel510  60000 non-null  int64\n",
      " 511  pixel511  60000 non-null  int64\n",
      " 512  pixel512  60000 non-null  int64\n",
      " 513  pixel513  60000 non-null  int64\n",
      " 514  pixel514  60000 non-null  int64\n",
      " 515  pixel515  60000 non-null  int64\n",
      " 516  pixel516  60000 non-null  int64\n",
      " 517  pixel517  60000 non-null  int64\n",
      " 518  pixel518  60000 non-null  int64\n",
      " 519  pixel519  60000 non-null  int64\n",
      " 520  pixel520  60000 non-null  int64\n",
      " 521  pixel521  60000 non-null  int64\n",
      " 522  pixel522  60000 non-null  int64\n",
      " 523  pixel523  60000 non-null  int64\n",
      " 524  pixel524  60000 non-null  int64\n",
      " 525  pixel525  60000 non-null  int64\n",
      " 526  pixel526  60000 non-null  int64\n",
      " 527  pixel527  60000 non-null  int64\n",
      " 528  pixel528  60000 non-null  int64\n",
      " 529  pixel529  60000 non-null  int64\n",
      " 530  pixel530  60000 non-null  int64\n",
      " 531  pixel531  60000 non-null  int64\n",
      " 532  pixel532  60000 non-null  int64\n",
      " 533  pixel533  60000 non-null  int64\n",
      " 534  pixel534  60000 non-null  int64\n",
      " 535  pixel535  60000 non-null  int64\n",
      " 536  pixel536  60000 non-null  int64\n",
      " 537  pixel537  60000 non-null  int64\n",
      " 538  pixel538  60000 non-null  int64\n",
      " 539  pixel539  60000 non-null  int64\n",
      " 540  pixel540  60000 non-null  int64\n",
      " 541  pixel541  60000 non-null  int64\n",
      " 542  pixel542  60000 non-null  int64\n",
      " 543  pixel543  60000 non-null  int64\n",
      " 544  pixel544  60000 non-null  int64\n",
      " 545  pixel545  60000 non-null  int64\n",
      " 546  pixel546  60000 non-null  int64\n",
      " 547  pixel547  60000 non-null  int64\n",
      " 548  pixel548  60000 non-null  int64\n",
      " 549  pixel549  60000 non-null  int64\n",
      " 550  pixel550  60000 non-null  int64\n",
      " 551  pixel551  60000 non-null  int64\n",
      " 552  pixel552  60000 non-null  int64\n",
      " 553  pixel553  60000 non-null  int64\n",
      " 554  pixel554  60000 non-null  int64\n",
      " 555  pixel555  60000 non-null  int64\n",
      " 556  pixel556  60000 non-null  int64\n",
      " 557  pixel557  60000 non-null  int64\n",
      " 558  pixel558  60000 non-null  int64\n",
      " 559  pixel559  60000 non-null  int64\n",
      " 560  pixel560  60000 non-null  int64\n",
      " 561  pixel561  60000 non-null  int64\n",
      " 562  pixel562  60000 non-null  int64\n",
      " 563  pixel563  60000 non-null  int64\n",
      " 564  pixel564  60000 non-null  int64\n",
      " 565  pixel565  60000 non-null  int64\n",
      " 566  pixel566  60000 non-null  int64\n",
      " 567  pixel567  60000 non-null  int64\n",
      " 568  pixel568  60000 non-null  int64\n",
      " 569  pixel569  60000 non-null  int64\n",
      " 570  pixel570  60000 non-null  int64\n",
      " 571  pixel571  60000 non-null  int64\n",
      " 572  pixel572  60000 non-null  int64\n",
      " 573  pixel573  60000 non-null  int64\n",
      " 574  pixel574  60000 non-null  int64\n",
      " 575  pixel575  60000 non-null  int64\n",
      " 576  pixel576  60000 non-null  int64\n",
      " 577  pixel577  60000 non-null  int64\n",
      " 578  pixel578  60000 non-null  int64\n",
      " 579  pixel579  60000 non-null  int64\n",
      " 580  pixel580  60000 non-null  int64\n",
      " 581  pixel581  60000 non-null  int64\n",
      " 582  pixel582  60000 non-null  int64\n",
      " 583  pixel583  60000 non-null  int64\n",
      " 584  pixel584  60000 non-null  int64\n",
      " 585  pixel585  60000 non-null  int64\n",
      " 586  pixel586  60000 non-null  int64\n",
      " 587  pixel587  60000 non-null  int64\n",
      " 588  pixel588  60000 non-null  int64\n",
      " 589  pixel589  60000 non-null  int64\n",
      " 590  pixel590  60000 non-null  int64\n",
      " 591  pixel591  60000 non-null  int64\n",
      " 592  pixel592  60000 non-null  int64\n",
      " 593  pixel593  60000 non-null  int64\n",
      " 594  pixel594  60000 non-null  int64\n",
      " 595  pixel595  60000 non-null  int64\n",
      " 596  pixel596  60000 non-null  int64\n",
      " 597  pixel597  60000 non-null  int64\n",
      " 598  pixel598  60000 non-null  int64\n",
      " 599  pixel599  60000 non-null  int64\n",
      " 600  pixel600  60000 non-null  int64\n",
      " 601  pixel601  60000 non-null  int64\n",
      " 602  pixel602  60000 non-null  int64\n",
      " 603  pixel603  60000 non-null  int64\n",
      " 604  pixel604  60000 non-null  int64\n",
      " 605  pixel605  60000 non-null  int64\n",
      " 606  pixel606  60000 non-null  int64\n",
      " 607  pixel607  60000 non-null  int64\n",
      " 608  pixel608  60000 non-null  int64\n",
      " 609  pixel609  60000 non-null  int64\n",
      " 610  pixel610  60000 non-null  int64\n",
      " 611  pixel611  60000 non-null  int64\n",
      " 612  pixel612  60000 non-null  int64\n",
      " 613  pixel613  60000 non-null  int64\n",
      " 614  pixel614  60000 non-null  int64\n",
      " 615  pixel615  60000 non-null  int64\n",
      " 616  pixel616  60000 non-null  int64\n",
      " 617  pixel617  60000 non-null  int64\n",
      " 618  pixel618  60000 non-null  int64\n",
      " 619  pixel619  60000 non-null  int64\n",
      " 620  pixel620  60000 non-null  int64\n",
      " 621  pixel621  60000 non-null  int64\n",
      " 622  pixel622  60000 non-null  int64\n",
      " 623  pixel623  60000 non-null  int64\n",
      " 624  pixel624  60000 non-null  int64\n",
      " 625  pixel625  60000 non-null  int64\n",
      " 626  pixel626  60000 non-null  int64\n",
      " 627  pixel627  60000 non-null  int64\n",
      " 628  pixel628  60000 non-null  int64\n",
      " 629  pixel629  60000 non-null  int64\n",
      " 630  pixel630  60000 non-null  int64\n",
      " 631  pixel631  60000 non-null  int64\n",
      " 632  pixel632  60000 non-null  int64\n",
      " 633  pixel633  60000 non-null  int64\n",
      " 634  pixel634  60000 non-null  int64\n",
      " 635  pixel635  60000 non-null  int64\n",
      " 636  pixel636  60000 non-null  int64\n",
      " 637  pixel637  60000 non-null  int64\n",
      " 638  pixel638  60000 non-null  int64\n",
      " 639  pixel639  60000 non-null  int64\n",
      " 640  pixel640  60000 non-null  int64\n",
      " 641  pixel641  60000 non-null  int64\n",
      " 642  pixel642  60000 non-null  int64\n",
      " 643  pixel643  60000 non-null  int64\n",
      " 644  pixel644  60000 non-null  int64\n",
      " 645  pixel645  60000 non-null  int64\n",
      " 646  pixel646  60000 non-null  int64\n",
      " 647  pixel647  60000 non-null  int64\n",
      " 648  pixel648  60000 non-null  int64\n",
      " 649  pixel649  60000 non-null  int64\n",
      " 650  pixel650  60000 non-null  int64\n",
      " 651  pixel651  60000 non-null  int64\n",
      " 652  pixel652  60000 non-null  int64\n",
      " 653  pixel653  60000 non-null  int64\n",
      " 654  pixel654  60000 non-null  int64\n",
      " 655  pixel655  60000 non-null  int64\n",
      " 656  pixel656  60000 non-null  int64\n",
      " 657  pixel657  60000 non-null  int64\n",
      " 658  pixel658  60000 non-null  int64\n",
      " 659  pixel659  60000 non-null  int64\n",
      " 660  pixel660  60000 non-null  int64\n",
      " 661  pixel661  60000 non-null  int64\n",
      " 662  pixel662  60000 non-null  int64\n",
      " 663  pixel663  60000 non-null  int64\n",
      " 664  pixel664  60000 non-null  int64\n",
      " 665  pixel665  60000 non-null  int64\n",
      " 666  pixel666  60000 non-null  int64\n",
      " 667  pixel667  60000 non-null  int64\n",
      " 668  pixel668  60000 non-null  int64\n",
      " 669  pixel669  60000 non-null  int64\n",
      " 670  pixel670  60000 non-null  int64\n",
      " 671  pixel671  60000 non-null  int64\n",
      " 672  pixel672  60000 non-null  int64\n",
      " 673  pixel673  60000 non-null  int64\n",
      " 674  pixel674  60000 non-null  int64\n",
      " 675  pixel675  60000 non-null  int64\n",
      " 676  pixel676  60000 non-null  int64\n",
      " 677  pixel677  60000 non-null  int64\n",
      " 678  pixel678  60000 non-null  int64\n",
      " 679  pixel679  60000 non-null  int64\n",
      " 680  pixel680  60000 non-null  int64\n",
      " 681  pixel681  60000 non-null  int64\n",
      " 682  pixel682  60000 non-null  int64\n",
      " 683  pixel683  60000 non-null  int64\n",
      " 684  pixel684  60000 non-null  int64\n",
      " 685  pixel685  60000 non-null  int64\n",
      " 686  pixel686  60000 non-null  int64\n",
      " 687  pixel687  60000 non-null  int64\n",
      " 688  pixel688  60000 non-null  int64\n",
      " 689  pixel689  60000 non-null  int64\n",
      " 690  pixel690  60000 non-null  int64\n",
      " 691  pixel691  60000 non-null  int64\n",
      " 692  pixel692  60000 non-null  int64\n",
      " 693  pixel693  60000 non-null  int64\n",
      " 694  pixel694  60000 non-null  int64\n",
      " 695  pixel695  60000 non-null  int64\n",
      " 696  pixel696  60000 non-null  int64\n",
      " 697  pixel697  60000 non-null  int64\n",
      " 698  pixel698  60000 non-null  int64\n",
      " 699  pixel699  60000 non-null  int64\n",
      " 700  pixel700  60000 non-null  int64\n",
      " 701  pixel701  60000 non-null  int64\n",
      " 702  pixel702  60000 non-null  int64\n",
      " 703  pixel703  60000 non-null  int64\n",
      " 704  pixel704  60000 non-null  int64\n",
      " 705  pixel705  60000 non-null  int64\n",
      " 706  pixel706  60000 non-null  int64\n",
      " 707  pixel707  60000 non-null  int64\n",
      " 708  pixel708  60000 non-null  int64\n",
      " 709  pixel709  60000 non-null  int64\n",
      " 710  pixel710  60000 non-null  int64\n",
      " 711  pixel711  60000 non-null  int64\n",
      " 712  pixel712  60000 non-null  int64\n",
      " 713  pixel713  60000 non-null  int64\n",
      " 714  pixel714  60000 non-null  int64\n",
      " 715  pixel715  60000 non-null  int64\n",
      " 716  pixel716  60000 non-null  int64\n",
      " 717  pixel717  60000 non-null  int64\n",
      " 718  pixel718  60000 non-null  int64\n",
      " 719  pixel719  60000 non-null  int64\n",
      " 720  pixel720  60000 non-null  int64\n",
      " 721  pixel721  60000 non-null  int64\n",
      " 722  pixel722  60000 non-null  int64\n",
      " 723  pixel723  60000 non-null  int64\n",
      " 724  pixel724  60000 non-null  int64\n",
      " 725  pixel725  60000 non-null  int64\n",
      " 726  pixel726  60000 non-null  int64\n",
      " 727  pixel727  60000 non-null  int64\n",
      " 728  pixel728  60000 non-null  int64\n",
      " 729  pixel729  60000 non-null  int64\n",
      " 730  pixel730  60000 non-null  int64\n",
      " 731  pixel731  60000 non-null  int64\n",
      " 732  pixel732  60000 non-null  int64\n",
      " 733  pixel733  60000 non-null  int64\n",
      " 734  pixel734  60000 non-null  int64\n",
      " 735  pixel735  60000 non-null  int64\n",
      " 736  pixel736  60000 non-null  int64\n",
      " 737  pixel737  60000 non-null  int64\n",
      " 738  pixel738  60000 non-null  int64\n",
      " 739  pixel739  60000 non-null  int64\n",
      " 740  pixel740  60000 non-null  int64\n",
      " 741  pixel741  60000 non-null  int64\n",
      " 742  pixel742  60000 non-null  int64\n",
      " 743  pixel743  60000 non-null  int64\n",
      " 744  pixel744  60000 non-null  int64\n",
      " 745  pixel745  60000 non-null  int64\n",
      " 746  pixel746  60000 non-null  int64\n",
      " 747  pixel747  60000 non-null  int64\n",
      " 748  pixel748  60000 non-null  int64\n",
      " 749  pixel749  60000 non-null  int64\n",
      " 750  pixel750  60000 non-null  int64\n",
      " 751  pixel751  60000 non-null  int64\n",
      " 752  pixel752  60000 non-null  int64\n",
      " 753  pixel753  60000 non-null  int64\n",
      " 754  pixel754  60000 non-null  int64\n",
      " 755  pixel755  60000 non-null  int64\n",
      " 756  pixel756  60000 non-null  int64\n",
      " 757  pixel757  60000 non-null  int64\n",
      " 758  pixel758  60000 non-null  int64\n",
      " 759  pixel759  60000 non-null  int64\n",
      " 760  pixel760  60000 non-null  int64\n",
      " 761  pixel761  60000 non-null  int64\n",
      " 762  pixel762  60000 non-null  int64\n",
      " 763  pixel763  60000 non-null  int64\n",
      " 764  pixel764  60000 non-null  int64\n",
      " 765  pixel765  60000 non-null  int64\n",
      " 766  pixel766  60000 non-null  int64\n",
      " 767  pixel767  60000 non-null  int64\n",
      " 768  pixel768  60000 non-null  int64\n",
      " 769  pixel769  60000 non-null  int64\n",
      " 770  pixel770  60000 non-null  int64\n",
      " 771  pixel771  60000 non-null  int64\n",
      " 772  pixel772  60000 non-null  int64\n",
      " 773  pixel773  60000 non-null  int64\n",
      " 774  pixel774  60000 non-null  int64\n",
      " 775  pixel775  60000 non-null  int64\n",
      " 776  pixel776  60000 non-null  int64\n",
      " 777  pixel777  60000 non-null  int64\n",
      " 778  pixel778  60000 non-null  int64\n",
      " 779  pixel779  60000 non-null  int64\n",
      " 780  pixel780  60000 non-null  int64\n",
      " 781  pixel781  60000 non-null  int64\n",
      " 782  pixel782  60000 non-null  int64\n",
      " 783  pixel783  60000 non-null  int64\n",
      " 784  pixel784  60000 non-null  int64\n",
      "dtypes: int64(785)\n",
      "memory usage: 359.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True,show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "111d29ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "111d29ae",
    "outputId": "28f443c3-d233-4b3b-e9d3-fdebda24f1fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3172578",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3172578",
    "outputId": "03989b96-50c0-4cd3-8c0b-a9a79641529f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f5c8a",
   "metadata": {
    "id": "521f5c8a"
   },
   "source": [
    "### Detect duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c7ea41b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c7ea41b",
    "outputId": "cd664ce5-e18a-4153-f1d5-7ba64726226e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39e81d",
   "metadata": {
    "id": "bb39e81d"
   },
   "source": [
    "43 duplicates were found !, let's drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bdf56c1",
   "metadata": {
    "id": "8bdf56c1"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc34f166",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc34f166",
    "outputId": "87161d13-d7de-4404-8363-feead59bf50a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdfe4d46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdfe4d46",
    "outputId": "0f110e0d-bab6-4c68-e1b8-60f2438af9f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59957, 785)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce69572a",
   "metadata": {
    "id": "ce69572a"
   },
   "outputs": [],
   "source": [
    "# save the new data frame then load it again \n",
    "df.to_csv('Train_data_modified.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b6f236e",
   "metadata": {
    "id": "4b6f236e"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Train_data_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "931e8a89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "931e8a89",
    "outputId": "75828370-601c-4753-9e7c-547efade3cb4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8a7ad32c-50d6-4ca5-9f23-d021d66145f3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a7ad32c-50d6-4ca5-9f23-d021d66145f3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8a7ad32c-50d6-4ca5-9f23-d021d66145f3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8a7ad32c-50d6-4ca5-9f23-d021d66145f3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ceae3a61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceae3a61",
    "outputId": "69b9c851-3080-4316-b2a9-886ce6845e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59957, 785)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7bd9fbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7bd9fbd",
    "outputId": "4f2442c6-5a5d-4598-9814-dfc56cd5d9ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=59957, step=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01d607b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01d607b8",
    "outputId": "7b7bfafa-c7a2-4943-f0e6-92f14ec930fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 59956)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.min(),df.index.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e672a2",
   "metadata": {
    "id": "63e672a2"
   },
   "source": [
    "### visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43e3ce6f",
   "metadata": {
    "id": "43e3ce6f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb6898fa",
   "metadata": {
    "id": "eb6898fa"
   },
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i,column in enumerate(df.columns):\n",
    "    columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "182e8963",
   "metadata": {
    "id": "182e8963"
   },
   "outputs": [],
   "source": [
    "label = columns[0]\n",
    "pixels = columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "275065e9",
   "metadata": {
    "id": "275065e9"
   },
   "outputs": [],
   "source": [
    "def show_pixels_distributions(index = 0):\n",
    "    image = np.array(df.loc[index][1:])\n",
    "    label = str(df.loc[index][0])\n",
    "    image = image.reshape(28,28)\n",
    "    pixel_dist = image.sum(axis=0)\n",
    "    x = range(28)\n",
    "    plt.bar(x,pixel_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c3d8321",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "5c3d8321",
    "outputId": "54c0cc0a-4ec3-44be-a5a4-f93edecdab03",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pixels_distributions(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8998d074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "8998d074",
    "outputId": "c0409009-a3c7-4621-db7f-88578d9ea1c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f367ea4a580>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAADnCAYAAADGrxD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXxU5b3/P9+ZyUrCACFAIISwzwQSwr6EBEi0WkFAa73WqqC2inuV9kr7q3WsV43a9motbm1vF6+t3Grb2x+0t+2vVAhLWGRJgBAwAtkXtuyZzMz5/v6YgRtDktnONifP+/WaF0nmnOf5kJzPnOc8z/f5fomZIRAIjINJawECgUBehKkFAoMhTC0QGAxhaoHAYAhTCwQGQ5haIDAYwtQCgcEQphYIDIYwtUBgMISpBQKDIUwtEBgMYWqBwGAIUwsEBkOYWiAwGMLUgxAieoKIjhHRcSL6htZ6BPIiTD3IIKKZAL4OYAGAWQBWEdEUbVUJ5ESYevBhB7CPmTuY2Q1gB4BbNdYkkBFh6sHHMQC5RJRERPEAbgIwXmNNAhmxaC1AoC7MXEZELwP4G4B2AEcAeLRVJZATEjnKBjdE9CKAamZ+U2stAnkQd+pBCBGNYuZGIkqD93l6kdaaBPIhTD04+YiIkgC4ADzCzJe1FiSQDzH8FggMhpj9FggMhjC1QGAwhKkFAoMhTC0QGAwx+21AMn+VaQIwAcB0ACkARgJI9v175evhAKLgvQbMABjeIBQPgC4AFwGc7+NVCeBk6brSOvX+R4JgELPfEU7mrzKnAsiGN6b7ymsagDiFu24GUA6gDMBJ37/7hdm1R5g6gsj8VSYByASQ53vlAhijqahrqQCwE0ARgKLSdaWfaqxn0CFMrXMyf5WZAmA1vBsvcuEdNkcS9QA+BvB/Afy5dF2pCHRRGGFqHVJms6cxcPs93zSvdkbRUgCktSaZcGU4nb/eUttwAMBHcDSf11qQERETZTqhzGZPBHA3gLsALCKAco7z/u3ZZBRDA0DUyraODAD3A/gJHNbtAH4J4EM4ml1qCCCiJwF8Dd6JwVIA9zJzlxp9q4VY0uoDIrqRiMqJ6FMi2qRkX2U2+8wym/0tALUANgNYDN+d+aaDkioXumowO29tbZvp+84C4AsAfgOgCg7rC3BY05TsnojGAXgcwDxmngnvrP8dSvapBeJO3QsiMsNrrusBVAM4QER/YuYTcvVRZrNHAfgSgIfhfU7uk/FNmGnxcLfbTNFy9a0lYzyeownMC/p4azSA7wB4Gg7rNgBvAfgrHM1KPBtaAMQRkQtAPLwfpoZC3KmvZQGAT5n5M2buBvABgDVyNFxms8eU2eyPATgL4LcYwNAAQIB1cRmXyNG3Hritta3bzyFmeCcF/wLgOBzW2+Gwyvb4wcw1AH4A71p7HYBmZv6bXO3rBWHqaxkHoKrH99W+n4VMmc0eVWazPwjgNIAfAxgb6Lkr90vGeN5jdt3RcnXoHQh2AFsAHIXDeqsc5iai4fB+QE+E928whIjuCrddvSFMrSBlNrulzGa/D8ApAG8jhFxg6Q3IMEnsll2cyoz0SCVWSRoWwqmZAD4C8Akc1pvDlHEdgDPM3MTMLgC/B7AkzDZ1hzD1tdTg8+ZL9f0sKMps9i8AOA7g5wDSQxVjAkYsOMWloZ6vF9a2tXWE2cRsAH+Cw7oLDmtWiG1UAlhERPFERAAK4I2EMxTC1NdyAMBUIppIRNHwzo7+KdCTy2z2sWU2+xYAf4U3XDNsVu2TWuVoRzOYpa+2tNpkai0H3rv2j+CwJgYng/cB+BDAIXiXs0wA3pVJl24QwSd9QEQ3AXgN3omb/2DmF/ydU2azmwE8AuB5AEPl1CMRmr7ytDmJiSLyQ9jq8RzZVVmTrUDTNQCehKP5dwq0HbFE5EWiNMz8Z2aexsyTAzT0PAD7AbwOmQ0NACZG8uxPI3cIvqqtvVmhpscB+C84rP8Dh3WiQn1EHMLUYVBms5vLbPbvAdgLYI6Sfa3eJyllDGVh5nXNrbI8hgzADfDOkq9XuJ+IQJg6RMps9nR4S9Y8BxWCeKZXYyoi8Fkpgfl4iseTokJXiQB+AYf1d3BYI23Ti6wIU4dAmc1+C4DD8E7aqIKZkZJ5lo+r1Z9c3NDWcUHlLm8DcBgO60KV+9UNwtRB4AsieR3e9c1Q1lzDYnUxq22QsFnf3KLFs+4EAEVwWJ/SoG/NEaYOkDKbfTi89ace10rDjEqerFXfoRArSeXpbreimzQGIArAD+Gw/gIOa5RGGjRBmDoAymz2yfBOhi3XUodFQqq9kiMmWCK/o1MPqY3WA/jbYHrOFqb2Q5nNngOgGN4kfpqzulhq0FpDoNzb3KKXErnLAeyFwxpRI51QEaYegDKb/SsA/gFvBk5dkHWG07XWEAjRElfYul16MtF0APvgsC7VWojSCFP3Q5nN/iSA9wHEaK2lJ1ES0ifX8imtdfhjaWdnlf+jVCcJwP+Dw7pKayFKIkzdBz5D/wg6zQ22tljSw7PqgNzX3KLG2nQoxAD4yMjGFqbuRQ9D65bZn3Kq1hoGwsJcOcvZrYs5iH6IhtfY4W7l1CXC1D0os9m/AZ0bGgCiPZg8oYErtNbRHws7uz7TWkMARAP40IjGFqb24TP0v2utI1DWFEt6fGYFANzb3JKstYYAuWLs1VoLkRNhagBlNvt6RJChAWDeaQ44JZKamJjrFnQ5M7TWEQTR8O70Msys+KA3dZnNvhwRuFE+1oVpYy/wOa119GZOl/MU6XSCcQBiAPzBKOvYg9rUZTb7NHjjuCMyjHBNsXRWaw29WdfcqnpMvEyMBLDNCJFng9bUZTZ7EoBtiLzaVFdZdJJ19exKzE15nZ2ZWusIg+kAfh/pseKD0tRlNns0gD8AmKK1lnCI60bGqMscdFJEpZjR3X3SFPnX1HJE4ONYTyL9DxAqb8JPIv1I4eZiSTelYu9pbo3XWoNMrIfD+qTWIkJl0Jm6zGa/C94CbYZg6QkeobUGACDmy9e1d4SaulePvBypiRYGlanLbPbp8NZpMgzxTsxMauF6rXVM7XYdi4rQCcd+iAKwpeXZMVathQTLoDH15g3bo3cs/eHrF4fbzmitRU4IoJX7Jc03eNzV0mqIIn5XYIb7956lZ7OdP31Hay3BMmhMDeB5jyX2hiNZj9qP29d/zCBJa0FykXeMZU9LHBTMrSvb2mdpqkFGXGyuvsv1nZNPuR5eJsH0L+mbtq3TWlMwDApTb96wfTmAbwIAiCwNo+cv37WksKQrZrjudzsFQmInMq1t3KRV/+kud2m0zraohspnUsqeOc63E3dLM3sW83sjfdO2iMkrbnhTb96wPQbAT9Hr/+qKTsjes+j52JqUnH3aKJMPAsw3HZROatX/V1paIy2C7BqY0fGa+9ai/O4fLmnFkN7P0YnwrphEBIY3NYBN6G89mmh4+fQ7Fx6cvXGnx2SJ6JKxK0p4iCYdM3eubWuP6FnvDo4p/2J3Yf1r7tsGWua8MX3TtttUExUGhjb15g3bJ8Nr6gFpsU7KK8p5taolYfxpFWQpgrUdWQkdfEntfse5PUfjWaMPFBkoluw7s53vpp/ktEkBHP5a+qZtQRXl0wJDmxrATwDEBnKgZI6eenDu0+NPT75lp8KaFIEAy42H1E/2f3trq0ftPuVAYrrwTdeD++/ofiavG1GBzgeMg7cii64xbNXLzRu23wYgpGqIsV0X9s0/+PK0KHd7RMWFX0jEgYcetcxXrUPm7t2V1Z1DJY6otdwLnHh4lfPFlDokjQnhdA+AuWcLVx6VW5dcGPJOvXnD9iEIY390V2zSwqKcwq6mpKwjMspSnBGtmBXXxS1q9Zfs8ZREkqF9a8875jnfmhWioQFveWNdT5oZ0tQAHgUQXh4vMqWUznwgq2TmAx9LZHLLI0tZCIi+/oh6JW9vbW3vVKuvcOm59swwhXvdL0nftE23iQsNZ+rNG7YnAviWLI0Rmc6PnLV8V87LZR1xI6tlaVNhvnBIUrwCJwCA2XNnS2tEZDjpZ+05XJ5L37RNl0t5hjM1vLWukuRs0G2Jzyxe4EisTM3fI2e7SpDcjKyYbm5Xup/hklQyQpJk/T3LjZ+153CZA2CtzG3KgqFMvXnD9qEANirSOJH10ylfWrJ/7qZdHlO04qYJFQLi8o9yidL9rG5rb1W6j3AIcO05XHR5tzaUqQE8CYUzmbQljl+6c+mrjZeHTtQsgssfN3yicFg7M9/d3KrbvN5Brj2HQyaALyvcR9AYZklr84btVgDnAKgzG8vcPbZ2197ppz/I01uiPQba7v6m2dwdRXFKtJ/okUr2VFbrLopMYrrwr+4HKj70LFugYrcnAMw8W7hSN0Yy0p36PqhlaAAgiq4dl7ts9+IXPnFGJWq2maIvCEjIPabcEPym9nbVI9f8cYETD+c4f+xS2dAAkAHgepX7HBBDmHrzhu0EYIMWfXfHDJu3e8mLaBg196AW/ffHTQclxSK91jW36iaVrkxrz+HysEb99okuTE1EsUS0n4iOEtFxIgo2FK8AwDQltAUEmZKP2++dezjr0R0Smbs109GD1POYYfGw7FriJenEeLdbF7W8ZF57DodV6Zu2BV2Lm4jOElEpER0hItluCrowNQAngHxmngUgG8CNRLQoiPO1/6Qkoksj7MuKcl6paI8fc1ZzOYB1yQmWPZTxuvaORrnbDAWF1p5DxQzgwRDPXcHM2cw8Ty4xujA1e2nzfRvlewU08bB5w/ZxAHRTC8ljibXvm//dkWcm3Lhbay0rD0hOudu8r7klXe42g0Hhtedw+Fr6pm26SOmkC1MDABGZiegIgEYAf2fmQJMXPADvJ6V+IEo4M/HmnOL5z+xxm2NVi8XuTXoDZpgkli3ENUaSTk92udPlai9YVFp7DpXRAL4U5DkM4G9E9AkRPSCXEN2Ympk9zJwNb8z2AiLyO6zyTZCtV1pbqHQMGbOkKOeV5ovDph/Ton8Chi8sl28IntfZpVnhABXXnsMh2FxmS5l5DoAvAniEiPLkEKEbU1+BmS8D+CeAGwM4fBGANGUVhQebzOOPzHrMdtx2jybJDlftkzrkauu+yy3j5GorUELc96wVBembto0M9GBmb3UVZm6Et2KMLMtxujA1ESUT0TDf13HwrvsFErH1L4oKkwsiS8OYhct3LXnpqNrJDifXw0bMYX+YRDGfndndPVUOTYGi4dpzqFgA3BrIgUQ0hIgSr3wN4AsAZBnR6cLUAFIA/JOISgAcgPeZeutAJ/iG3sE+w2iKKzpx9p5Fz8fUjlm8X60+TYzkuafD3465uLPrrAxyAkIna8+hEmges9EAdhHRUQD7AWxj5v+RQ0DEholu3rB9HrwfABGJtbliZ/bRHy8wS+6A0i2FQ1kqdjx7t2VZOG38oq7hxDwVism72Fy93vX0ZZ0sVYWCC0Dy2cKVzVoJ0MudOhTWaC0gHJqtk/OKcl6tak0Yr3iBu2k1mI4wPr3NzNVqGFpna8+hEgXgJi0FRLKpb9ZaQLhI5uipB+Y+Pe7TSWsVTXZoZozJOsMhP6/N63JWyKmnNzpeew4VTa/NiDT15g3bRwDQ3S6hkCCKq0y7Pm/PwueKXZYhim2UWFPMF0M9d31zi2LbWXW+9hwqYT3qhEtEmhrAUuhsu2O4dMWNXFSU81LX+aSZimSpzKjikDZhmJgbcjq7MuXWA0TM2nMojE3ftE2zTS+Ramojfar/L2ROKZm5IbN0xtc/ZpCsu6zMElIzKvlEsOdlObvL5d4vHmFrz6EiSyBJKAhT6w0iU1Ny9vKinFdOdMbKm+xwdbEU9GaMdc0tslakiMC151DR7BqNOFNv3rA9Ht6kb4bGHRWfuXehI7Fq3PK9crWZdYaDqtxIzBdXdHTKMnfBDPdHntyPI3TtORTEnToIFsG7bGB8iKynp3558f65T+/ymKLDDve0SJgwpYbLAz3e1u06bpZhs4yLzVVfdX2nfKProeUa73tWk8npm7alaNFxJP6CF2otQG3aEtOW7lz6SkPz0IkBG7I/1hRL9YEee1dLS9iBMRVSyp7Zznese6SZM8JtKwJRrwRSDyLR1HatBWgBm6ImfjJ7Y3r51Nt3cIB7zftidgUHlrWEufnGto5ZofbDjPbX3LfuKuj+4ZI2xA8NtZ0IR5NrNRJNbdNagGYQxdSMW7Zsz+IXDnZHJZ4PpYloDyan17PfYJLJLvexaCCkTf8dHFN+Y3dh42vu25aGcr6B0ORajURT6zbftFo4Y4bN37XkRakhec4noZy/ep9U5e+YO1taQ3qW3uvJ2JntfDe9nNOCmpQzKOJO7Y/NG7aPBTBYh3Kfh0yjjmfcN+dI1iM7JDK5gjl1/ikeO+ABzO2r29qDGnpLTBc2dm848BXXd4289hws4k4dAIN36N0XRHRxRMayopxXT7fHjz4X6Gkxbkwbd577PX68210SyxxwIYALnHh4ifMN90dSniYTQzrGqsUMeKSZetAPvfvCY4nN2Df/maSzaTfsCvScNcXS2f7eu6OlLaCJON/a8455zrdm1WPE6ED7HmSofiOKNFNP0FqAbiFK+GzS6qXF87+7222O8Vu8bmE5J/f5BrPz1tY2v7HePdaetc65rXdUz5E+YC1jIhowNQsz/15eOX4JOP/TYKVjSEpOUc4rlbNKNleOuHyq37XhuG5kjL7E1Q3D6XMX3RiP52gC84AhnBVSyp413c/PbEN80AnsByGqX7P+CpQPtC+UAahtal3XQ9YLbLKkHZn1uHtMw74d9pP/mUvgPu+kN++TKn52o/lzpr6tta3fqh7MaH/dc+thsVQVFPoyNTPfq5aQABF36kAhstSPWbTswogZh+YfLBwX2335mmfenBOc9LOeOVuZXXe0tPWZdaSDY8pv6X4uupzThKGDQ/VrNqBnISIaTUQ/J6K/+L7PIKL7lZXWJ8LUQeKKTpyzZ/Hzlroxi65JdhjvxIykZr6a3TTJI5VaJWlY7+PE2nNY9D13oSCBTnD8EsBfAVxZ3zwF4BtKCPKDMHUokCmpzHb3gk+yn9wpkeVqKR4CaNV+6fSV79e0tbX1PE2sPcuCPu/UAEYy838BkACAmd0AFCuV2he+lMCKpdUZDDQPm5K3c+mr51qHjLsaJpp3nL3BPMzSXS2tVyOgxNqzbKg+DxSoqduJKAm+jQS+ipRqp0CNgt5qZkUgkjl62oF53x5bMXF1EQAkdCLL2sZNVkkqTfZIyWLtWXZUH+H4m/2+wlMA/gRgMhHthvc5IdCk5XIhDC0XRHHnJtyQ2zhqTvG8T16xrTzQWYY5Hexic9U619Mte6SZmibOMxiqX7cBmZqZDxHRMngjughAOTMHFW8sA8LUMtMZl7yoKKewdmLdL9oTmk9aZjtftYq1Z9nRp6mJKBbewu5L4R2CFxHR28zcpaS4njibf8kstfZMnEe9vubP/4yp13E93+8jkR73bq83A7zPPX/mR1efX/v+5f40DvR/Gahvv21bKFqKm1Lg/OXCBThvmeIEEHIqYUEfMFQvZRzo8PvXAFoBvOH7/k4A7wH4shKi+oKliwxA8SoRg4kRMSnl+Sl3Rv09unT43Z3vmxYn7qr7Ib6dJJFZ9eqWhoXgN2RXbgI19Uxm7mmofxJR0Olmw0S24ukCYPaIgp1Th85dSEQxtaZL5u4ae1O2bXfW27i3+Rl+eW8DpSzWWqNBUP26DXT2+5BvxhsAQEQLARxURlK/qP0Mb0hiTPEXbh7/0P5p1nl5RBTTSM3lEvGEpqYJWcxoHoJ264/w6OLV/FERmGWrbT2IUf26HdDURFTqKy87F8AeIjpLRGcA7AUwTw2BV9i4ZStD/WU0QzE2fsqR1WmPuOItQ69u2CixnKsDAGZzdEeH9Wq9rX/Bb3JfxDfrotl5SgutBkL1OQp/w+9VqqgInCYARiigpioE8iwZtbZoXPzUPCL63Ad5lenC1dnumhp71LRpxVffm4Czk9/FPc6X+Nmd5ZShWR7rCCekXHLhMOCdmpnP9XwB6IR35vXKS21U/wVFOkMsw6rXpj1+InXItOW9DX2R2s54SLpa86mxcWIWMz4XKhoFd8z38Eze13nzfmLpglq6DYS+TH0FIlpNRKcBnAGwA8BZAH9RUFd/NGnQZ8QyJXHO3pWpDyRGm2P7THpQYjn3uZRGLFliOzuHlvZ17HJsX/A6NrgSufmwEloNjD5NDeB5eCtjnGLmiQAKABQPfIoiiDt1AJjJ0nnd2LuL5o68fjER9fu4ctbUdE0YaG1t/xmjknBhzJu4f9ZS/ngHvPH/Av/o1tQuZr4AwEREJmb+J1SeKPMhTO2H4dGjT9+S9kRtUszYAQu0NVNHtZs816SwbaifnMWMfme9TWDTQ3hj2bfx3Ekzu/2mGhaoP7oM1NSXiSgBwE4A7xPR6wDalZPVLwGXjBmMzBq+fOf1Y9eNN5ssfmsjl5or+0zoL0lRQ7q6Ekr8nT8TpTPfxr3WFK7ZE4rWQUSd/0PkJVBTr4F3kuxJAP8DoAIDpzpSirBrSRmRaFPs5ZWpDxbbhi3M84X0+uUzc8OI/t6rq50uBdJGPDqG/gCPL/kSf7ALzFp8yEcCJ9XuMCBTM3M7M3uY2c3Mv2LmH/uG42pTpkGfuiYlbtLRNWmPdSREDVvk/2gvbeiq74a7z7RFAFBfPzWTGc7+3u/Nrfjd0kI82RjDneLv83k6AFSq3am/4JNWImrp49VKRKoHqsM7667aJhI9QyDP4uTVH+eOvm2miUwDV9zoxTFL5SlQn5tWAAAeT1Si0xl/NJg2x6Nq4rtYP3kGH90BZi2WO/VIef2KbNV/F/7WqROZeWgfr0RmVr38zcYtWyWIITjiLUPr1qQ9diwtwb6ciILe2vepuT7R3zH19VODDm+0wB39HXx/2cN4/RNiSSw/ajD0BiIvmT8wyIfgkxKy9q1K3RAbY44LqcxsJ7ovdMGV5e+4utrpM5lDi1vOQdG8N/B1WPlSSAX8DIQm16owdYRgInNXfspXd85P/uJCIgo5V9sJS3UZyP/Gfbc7xtrdHRfUELwnw3E5eTO+NmcF/30H1E+ooRfEnTpAjmgtQG2GRY+quCXticrk2NSw46/LzbUB58xqqJ8S1vwFAfQ1vL3su/jeaTO7Ai7gZyA0GalEoql3QZu4c03IHJ5b9IWx61Mspqhp4bblhKu5A86Ah+21tTY7c/hZY+04kfEO1ieN57O7w20rgqipX5H9mRYdR5ypN27ZehHAca11KE2UKab5ptSv780YtiSXiOLlaPOkufYYCNGBHu9yxSa5XDF+A1ECIQ5dCYXYmHMHv7cbzKpnA9GAIq06jjhT+9iptQAlGR2bXro27bHWxKgRsmYfOWmuDjTTzVUaGya3+T8qcG7GH3N+gMcvxXGH0T+YhamDxKimlhYmr9qxbMztdhOZZS2B6oKno5W6/Jao7U11jd3GjIAizAIlBbVpb2P9tGz+xMhr2sLUQWI4U8eZE+vXpD12ND1hxjIiCvqO6o/T5roSEIIexru645Pd7uhj/o8MDgs8Ud/Ci8uewA8OE0sNcrevMRcByP47C5SINPXGLVvrAJz2e2CEkJ4w88DN4x+KijXHz1aqj+PmqpDvto2NEy/JqaUnC1A8503cbxnB5w8o1YcG7NIikuwKEWlqH1okaZAVE0zdK8bcsWNh8sr5vrJGiuCB5GymjqCH3leoqc6YxqzcisNQtCS9gQfnf4G37QBzwDHnOmablp1Hsqn/qLWAcBgaNfLMLROe+GxU3ATFS9xUmBpKQPAbGtofTmdCiscTpfjE1jr8x7Ln8O2zFu7WZClIJhjeElUDQkTjieifRHSCiI4T0RNyCdCFqYloOhEd6fFqISJ/pXJ3IkKrSWQMW7LrxnH3jbKYom1q9HfcUhn23a+pKV2VXXlTcHr6u1g/ZiJ/qtlEU5jsq1+RHci+fzeAjb58+osAPEJEshSr0IWpmbmcmbOZORvedMQdAP4w0Dkbt2z1APhvNfTJRRRFt9w47v7dmcNzlxLREDX6lCC5L1DbjHDbqa7K8Jt4QS5i4Iz/Nzydew//fC+YIy0t9IeBHMTMdcx8yPd1K7zhz7JURtGFqXtRAKDCl73UH1uUFiMXybHjT6yd8Phla/TIHDX7PWdqKgWFX9e7q2toqsdjUTXu/gb8efGP8EhrPLf3mQxRhzCA3wV7EhGlA5gNYJ8cIvRo6jsA/DbAY/8B/ect4/kjb9yxYsxXpprInKZ256WWKtmit86fT2uUq61AGY2G1HewPmM+F38MZlnXyxWguH5FdlBJEXxpwj4C8A1mliVHga5MTUTRAFYjwE+7jVu2ugF8oKioMIg1D2laPf6RQ5MSZy0joii1+2ew1ETNsj23V1fNmCBXW8FggmT+Bl5d/hQKS0zsUT3nVxC8F8zBvmviIwDvM/Pv5RKhK1MD+CKAQ8wcTDDCW0qJCYe0IfaDq8c/jDhLwlytNFSbLhxnwii52uvoGJbu8Zg1K8MzFwez38T9sSO5UZZhqsy0IghTExEB+DmAMmb+kZxC9GbqryDwoTcAYOOWrScAfKyImhAgmFx5o2/fsSj55rlEpmQttZSaK2VfHbh4IVXTO2UiWoe/jocWruQ/7oSK9dED4L36FdnBxMnnALgbQH6PVZ+b5BAiezhiqPhmg68H8GAIp78JYLmsgkIgMWrEuevHrmuPMkUrvvYcCHWmy7LPWFdVzUxNHqX91ug78V7eIuw+/X1+gVwUPUVrPfBegwHDzLuA/vPEhYNu7tS+jKVJHNoSxh8A1MqtKRhs1oW7vzjuayOiTNGyrDWGSx1dOsHEsm4KAYD29hGTJcnUZ85wtZmEz6a+g3XjpnK51nsBdtavyNbNrjPdmDocfBNm72rRt4Wi2m4Yd+/uWSOW5xBRyFFbclNiOadY4r+LF8dVK9V2sMSgO86B7+Tdz2/vA7NiMep+COourTSGMLWPdwF0q9nhyJhxZWsnPHF+WPQoVdeeA6HGdFGx5bPqqhljlGo7VPLx94Wv4aGuBG5RO91VDQDZZq7lwDCm9u3c+plK3fHcpOt35Kd8dbKZzOkq9RkwTdRyWiKeqFT7ra3J0yXJpP2DdVawddEAAAttSURBVC+S0ZTyFu7LWsxFO8AcdhqmAHmxfkW2rhIrGsbUPl6Awsn+Y0zx528e//DBKUPnLPOtq+uOEsu5GqX7uHwp5YzSfYSCCWx6FK8texrPnzCxR+nHhEqodyMJGEOZeuOWrbUA3laq/dT4aYfWpD3iibckzleqDzmoNJ2XJYZ4IKqrM2Rb/1aCLBzNfBv3Jo7h2r0KdvNC/YpsVR/5AsFQpvZRCPRfijUUCOTOHf2lHUtGrc0mMl1T01lPXKL2cx6SpirdT3PzmAxJIt1MmPXFELRbf4jHFt/C/1UEZlmvCQBnAPxC5jZlwXCm3rhlawOAzXK1l2AZVr12wuMnx8ZPWUZEuv99lZjPqTYsbm4erYulLX/chi25L2FjfQx3yVmy6Xm9PUtfQfcXaYi8AiDs4PhpQ+ftuSn1gcRoU2y/FSL1xllzo2rD4urqGYpla5GbNJyb9A7Wpdv52A4ZmisH8GsZ2lEEQ5p645at5wF8L9TzzWTpuH7suqLZSQVLiMgqozRFaaXOWhc8drX6u3wpZQYz6XmDxeeIgjvmu3h22YP8xgFiKZzdfY/Ur8hWa3Y9aAxpah8/AXA42JNGxKSU3zLhifoRMWNyFdCkKKXmytMDlaiVH6KWluSISwCZh4/n/xgPeoby5UMhnP5B/Yrsf8guSkYMa2pfZpQNQOA5q2ePKNh5Xcrd6WayTFJOmXJUmOuHqd1ndVVGxIxkejICF0e/iftn5/H2YAr4tQB4SkldcmBYUwPAxi1b9wN4x99x0aa4i6vGb9g/zTovj4gCLiCnJzrgbHLCHXLG0FC5eDE1kxkRWYuaAHoQm5f9Hzx7yszuQJIbPFO/Ilv3jxuGNrWP7wDod3/22PgpR9akPeocYrEuUFGT7ByzVJWBtPh7kqm1daQmJVvlIgPHZ7yD9cPHcdVABfwOQ8ZVFSUxvKk3btl6GcA1mUkJ5MkZtfbjpaNuzTKRKUUDabJy2lynSiLDvqipztCsb7mIQ2fiK/hGzu38/m4w994X7QbwgJ4nx3pieFMDwMYtWz9Aj+QLQyzWmrVpjx9PHTJ9eSSsPfujC92XOtEdcIlauTl/fnwWc2Sma+7NGvw+5xU8cSGWO0/0+PFz9SuyD2omKkgi/oIOgocAVE5OzC5emfrgkGhzbJbWguSizFxzAqRlwguTpb1txAn/x0UG41Az4R2sm5rJh3fAm8zgJa01BQMZt+jgtex8ZPOiSYlZRdBRxhc5+G3Mrv3t5NR0TiB51GcHbbbd87TUoACXq5E6a13+jqAyhGrNYLpTI2/zI8UAntVah5x0w93aDqdmQ+8rnG+akMWMSEu874/7Is3QwCAztY+XAPxVaxFyUW6uLQVB82U4ZnN0R8cwzcq3KsAbBfkVA1aJ0SuDztSphbkM4E4YpBRumblaN3/Dmhqb6rnNFWI7gI1aiwgV3VwQapJamHsRwEoAqhR9Uwo3PJ0t1Kl6wEl/NDZOzGKGbBVBNOIkgC8V5FfocgdWIAxKUwNAamHuaQC3QOW8ZnLyqbm+BATdrBGzZInt7BwaKXWv+uI8gJUF+RWXtRYSDoPW1ACQWphbBOBrWusIlePmKrfWGnpTWzs9Uq8pJ4C1BfkVkVwbG8AgNzUApBbmvgfg+1rrCBYJkusStetun3dD/ZQsZnkzz6jEfQX5FQOFiUYMg97UAJBamPssgJ9qrSMYPjM1HgVBdzukJMkS39WVUKK1jiB5siC/4jdai5ALYer/5UFoVBAgFI5ZKju11tAfdbXT9V5ytidPFuRXvKa1CDkRpvbhW+ragAC2amqNBPacp1bVMpwES3391ExmOLXWEQCGMzQgTP05fMZ+CAqmGZaDKtP5UhBGaq2jPzyeqESnM/6o1jr88JQRDQ0IU1+Dz9gPQ8fGLrGc0304Zn3dND0vFT5VkF/x71qLUAph6j7oYex/01pLbxjMjdQ8XWsd/qirm5bJDL0FcHQDuMfIhgaEqfsltTCXUwtznwFwD3QUoFJrunicCborUNcbtzvG2t0dp6ch+EUA1xfkV7yntRClEab2g28d+zroJKS0xFypCx2B0FA/RS8z9J8CWFyQX6F1HWtVEKYOAF/k2SJ4k7hrSq3pUsRkOq2psc1ghtZRb0UAFhXkV5zSWIdqCFMHSGph7qcAFkPDbZsNdPkkE4/Xqv9gcbtjR7hcsVoGovwMwHUF+RURM7qRA2HqIEgtzL0E4IvwZihVPQldieVcv1lR9Upjw6R2DbptA3BXQX7F1wvyK3QzH6IWwtRB4ptAewnAcnjrE6tGteliqpr9yUF1jd3GHHhBBRk4BGBeQX7F+yr2qSuEqUMktTB3F4BZ6JGlVEkuUGuFh6TJavQlJ67u+GS3O1qN7ZgSgJfhfX7WfO5DS4SpwyC1MPdyamHunfBmUlG0SkWJ5Zyua0EPRGPjJKWDZcoB5BfkV2yK5OQGciFM3QdE9B9E1EhEAeXcSi3M/S2A6fBGoSky1DxnatJ1sfuBqKnOmMoMJdLWdgF4BkBWQX5FQCVqiWgYEX1IRCeJqIyIFiugS1MGVYrgQCGiPHgnW37NzEHtWa7eVLQAwFsA5silp5k6qn4XszdiZr37YvGSD45ZLC4593//BcCjwSY1IKJfAShi5p8RUTSAeGaO6EwnvRF36j5g5p1AaBUnUgtz9wOYD+AxQJ6UuSXmcxGfjaOpKV2uZaUaAF8uyK+4KQRDWwHkAfg5ADBzt9EMDQhTK0JqYa6UWpj7EwBTAfwACC8TyGfmxiRZhGlIdVVGuJN8TQC+BWBqQX7FhyG2MdHXzi+I6DAR/YyIdJPjTS6EqRUktTC3KbUw91sAJgF4Dd5nwKBoQ1edC+4ZsotTma6uoalut6UshFMvwhsXMKkgv+IHBfkV4YSeWuB9LHqLmWcDaAewKYz2dIkwtQqkFuY2pBbmPglgMoCfAIEnEDhmqTwFAikmTkUunE8LJnimGYADwMSC/IqXCvIreleiDIVqANXMvM/3/YeQce5DLwhTq0hqYW5tamHuY/Ca+3kAfguYnzbX6y4PWahUV89ID+CwkwCeADChIL/iuYL8iha5+mfmegBVRHRl62oBAMMU9ruCmP3uAyL6LbwRYyPhLVj/LDP/XO5+qjcVWQCshXfv9ore73ei+/z7MUXDQTDL3bdWLMn5zSmz2TOt1489AP4bwJsF+RX/ULJ/IsqGNyY8GsBnAO5l5ktK9qk2wtQ6oXpTkQ3eHGlfhffDBActFUVHLGdzNRUmM9NtRR+PGnV2ue/bswDeA/BuQX5FxAbX6A1hap3hu3vnA7j9tzG7prSTc5nWmuQkIeHCntlz/rwfwAcF+RX7/J4gCBphah3jcDjMAJbCO0RfBWCKtopCQgJQAmArgD86HI5PNNZjeISpIwiHwzEWQG6P10zob7LTDeATADt9r10Oh8NwAR56Rpg6gnE4HMMALIE3gs0GwA5gGoA4lSS0ACjr8ToEYK/D4dBiD7XAhzC1wXA4HARgArwmtwEYByAJ3sm3kT2+Hg70u/4twRv0cd73aurxby28y05lDoejVrH/iCBkhKkHKQ6HwwRvhJUJgBleg7sBuB0Oh9Z5xQRhIEwtEBgMvU2yCASCMBGmFggMhjC1QGAwhKkF10BEZt9+461aaxEEjzC1oC+egHfdWRCBCFMLPgcRpQJYCe9OJkEEIkwt6M1rAP4VCmVFFSiPMLXgKkS0CkAjM4tNFxGMMLWgJzkAVhPRWQAfAMgnov/UVpIgWEREmaBPiGg5gG8y8yqttQiCQ9ypBQKDIe7UAoHBEHdqgcBgCFMLBAZDmFogMBjC1AKBwRCmFggMhjC1QGAwhKkFAoMhTC0QGAxhaoHAYAhTCwQGQ5haIDAYwtQCgcEQphYIDIYwtUBgMISpBQKD8f8BQIaBY8BJGiUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[label].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08b36255",
   "metadata": {
    "id": "08b36255"
   },
   "outputs": [],
   "source": [
    "def draw_pixel_hist(pixel_number = 1):\n",
    "    pixel = \"pixel\" + str(pixel_number)\n",
    "    df[pixel].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d0a1546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "5d0a1546",
    "outputId": "8ae1d825-2c8b-4930-bd19-26775fa8b11c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_pixel_hist(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "488de8e7",
   "metadata": {
    "id": "488de8e7"
   },
   "outputs": [],
   "source": [
    "def box_plot(pixel_number = 1):\n",
    "    pixel = 'pixel'  + str(pixel_number)\n",
    "    df[pixel].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af4d35dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "af4d35dd",
    "outputId": "6f8a290a-3850-459d-8976-0b9197f3dedd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANuUlEQVR4nO3da4xc5X3H8e8v2EVJIATqjUWNydLUVDiNYqINpSJpqZAol1YmakVNI7AQjUkLbaPSFxuklrQSkqM2iRqpoTKBxLQQgnIRVm0VCL2kVAphTSkxEIRDbGHLwAYiQAqiYP59sccwmF3vZXZ24eH7kUZ75jmXefbN10ePZ3ZSVUiS2vK2xZ6AJGn+GXdJapBxl6QGGXdJapBxl6QGLVnsCQAsW7ashoeHF3sakvSmsn379p9U1dBk+94QcR8eHmZsbGyxpyFJbypJdk+1z2UZSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBr0hPsQkLZTh0a0L8jq7Np67IK8jTWXauCdZCdwALAcK2FRVf5/kM8AngPHu0Curalt3zqeBS4D9wJ9W1W0DmLs0a3OJ7vDoVmOtN52Z3Lm/BFxRVfcmORLYnuSObt8Xqurveg9OshpYB7wf+AXgO0lOrKr98zlxSdLUpl1zr6p9VXVvt/0c8BCw4hCnrAVurqoXqurHwE7glPmYrCRpZmb1H6pJhoGTgbu7ocuT3J/k+iRHd2MrgMd6TtvDJP8YJNmQZCzJ2Pj4+MG7JUl9mHHckxwBfBP4VFU9C1wDvA9YA+wDPjebF66qTVU1UlUjQ0OT/sVKSdIczSjuSZYyEfYbq+pbAFX1RFXtr6qXgWt5dellL7Cy5/TjujFJ0gKZNu5JAlwHPFRVn+8ZP7bnsI8BO7rtLcC6JIcnOQFYBXx//qYsSZrOTN4tcxpwIfCDJPd1Y1cCFyRZw8TbI3cBlwJU1QNJbgEeZOKdNpf5ThlJWljTxr2q7gIyya5thzjnauDqPuYlSeqDf35Akhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQUsWewLSXH3wr2/nmedfXJDXGh7dOtDrH/X2pfzvVWcO9DX01mLc9ab1zPMvsmvjuYs9jXkx6H889NbjsowkNci4S1KDjLskNci4S1KDpo17kpVJ/j3Jg0keSPJn3fgxSe5I8kj38+huPEm+mGRnkvuTfGjQv4Qk6bVmcuf+EnBFVa0GTgUuS7IaGAXurKpVwJ3dc4CzgVXdYwNwzbzPWpJ0SNPGvar2VdW93fZzwEPACmAtsLk7bDNwXre9FrihJnwPeHeSY+d95pKkKc1qzT3JMHAycDewvKr2dbseB5Z32yuAx3pO29ONHXytDUnGkoyNj4/PctqSpEOZcdyTHAF8E/hUVT3bu6+qCqjZvHBVbaqqkaoaGRoams2pkqRpzCjuSZYyEfYbq+pb3fATB5Zbup9PduN7gZU9px/XjUmSFshM3i0T4Drgoar6fM+uLcD6bns9cGvP+EXdu2ZOBZ7pWb6RJC2AmfxtmdOAC4EfJLmvG7sS2AjckuQSYDdwfrdvG3AOsBP4GXDxvM5YkjStaeNeVXcBmWL3GZMcX8Blfc5LktQHP6EqSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ2ayZd1SG9IR540ygc2jy72NObFkScBnLvY01BDjLvetJ57aCO7NrYRxOHRrYs9BTXGZRlJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGTRv3JNcneTLJjp6xzyTZm+S+7nFOz75PJ9mZ5OEkvzWoiUuSpjaTO/evAmdNMv6FqlrTPbYBJFkNrAPe353zpSSHzddkJUkzM23cq+q7wNMzvN5a4OaqeqGqfgzsBE7pY36SpDnoZ8398iT3d8s2R3djK4DHeo7Z041JkhbQXON+DfA+YA2wD/jcbC+QZEOSsSRj4+Pjc5yGJGkyc4p7VT1RVfur6mXgWl5detkLrOw59LhubLJrbKqqkaoaGRoamss0JElTmFPckxzb8/RjwIF30mwB1iU5PMkJwCrg+/1NUZI0W9N+h2qSrwGnA8uS7AGuAk5PsgYoYBdwKUBVPZDkFuBB4CXgsqraP5ipS5KmMm3cq+qCSYavO8TxVwNX9zMpSVJ//ISqJDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg6aNe5LrkzyZZEfP2DFJ7kjySPfz6G48Sb6YZGeS+5N8aJCTlyRNbiZ37l8FzjpobBS4s6pWAXd2zwHOBlZ1jw3ANfMzTUnSbEwb96r6LvD0QcNrgc3d9mbgvJ7xG2rC94B3Jzl2viYrSZqZua65L6+qfd3248DybnsF8FjPcXu6sddJsiHJWJKx8fHxOU5DkjSZvv9DtaoKqDmct6mqRqpqZGhoqN9pSJJ6zDXuTxxYbul+PtmN7wVW9hx3XDcmSVpAc437FmB9t70euLVn/KLuXTOnAs/0LN9IkhbIkukOSPI14HRgWZI9wFXARuCWJJcAu4Hzu8O3AecAO4GfARcPYM7SK4ZHty72FObFUW9futhTUGMysWS+uEZGRmpsbGyxpyFNanh0K7s2nrvY05BeJ8n2qhqZbJ+fUJWkBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWrQkn5OTrILeA7YD7xUVSNJjgG+DgwDu4Dzq+qn/U1TkjQb83Hn/ptVtaaqRrrno8CdVbUKuLN7LklaQINYllkLbO62NwPnDeA1JEmH0G/cC7g9yfYkG7qx5VW1r9t+HFg+2YlJNiQZSzI2Pj7e5zQkSb36WnMHPlJVe5O8B7gjyQ97d1ZVJanJTqyqTcAmgJGRkUmPkSTNTV937lW1t/v5JPBt4BTgiSTHAnQ/n+x3kpKk2Zlz3JO8M8mRB7aBM4EdwBZgfXfYeuDWficpSZqdfpZllgPfTnLgOjdV1b8muQe4JcklwG7g/P6nKUmajTnHvaoeBT44yfhTwBn9TEqS1B8/oSpJDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktSggcU9yVlJHk6yM8nooF5HkvR6A4l7ksOAfwDOBlYDFyRZPYjXkiS93qDu3E8BdlbVo1X1f8DNwNoBvZYk6SBLBnTdFcBjPc/3AL/ae0CSDcAGgOOPP35A05Bea3h064Kct2vjuXN6HWm+DCru06qqTcAmgJGRkVqseeitxejqrWJQyzJ7gZU9z4/rxiRJC2BQcb8HWJXkhCQ/B6wDtgzotSRJBxnIskxVvZTkcuA24DDg+qp6YBCvJUl6vYGtuVfVNmDboK4vSZqan1CVpAYZd0lqkHGXpAYZd0lqUKoW//NDScaB3Ys9D2kKy4CfLPYkpEm8t6qGJtvxhoi79EaWZKyqRhZ7HtJsuCwjSQ0y7pLUIOMuTW/TYk9Ami3X3CWpQd65S1KDjLskNci46y0hyZfn8j2+SYaT7OjZfj7Jfd3jH3uO+4/uC+EP7HtPN354kq93XxR/d5Lh+fqdpENZtG9ikhZSVf3hPF3qR1W1Zop9H6+qsYPGLgF+WlW/lGQd8Fng9+dpLtKUvHNXU7q76x8muTHJQ0m+keQd3Z31SJL3JnkkybIkb0vyX0nOTHJYkr9Nck+S+5NcOk9TWgts7ra/AZyRJPN0bWlKxl0t+mXgS1V1EvAs8McHdlTVbibunq8BrgAerKrbmbjDfqaqPgx8GPhEkhMmufYJSf4nyX8m+ehB+77SLcn8ZU/AX/my+Kp6CXgG+Pl5+02lKRh3teixqvrvbvufgY/07qyqLwPvAj4J/EU3fCZwUZL7gLuZCPCqg667Dzi+qk4G/hy4Kcm7un0fr6oPAB/tHhfO768kzY5xV4sO/vDGa54neQcTX9oOcMSBYeBPqmpN9zihu6N/9SJVL1TVU932duBHwInd873dz+eAm4BTutNe+bL4JEuAo4Cn+v4NpWkYd7Xo+CS/1m3/AXDXQfs/C9wI/BVwbTd2G/BHSZYCJDkxyTt7T0oylOSwbvsXmbizfzTJkiTLuvGlwG8DO7rTtgDru+3fA/6t/OSgFoDvllGLHgYuS3I98CAT6+u/A5DkN5hYUz+tqvYn+d0kFwNfBoaBe7v18nHgvIOu++vA3yR5EXgZ+GRVPd39I3BbF/bDgO/w6j8a1wH/lGQn8DSwblC/tNTLPz+gpnTvI/+XqvqVRZ6KtKhclpGkBnnnLkkN8s5dkhpk3CWpQcZdkhpk3CWpQcZdkhr0/7FiOVlhzaBFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_plot(550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05087006",
   "metadata": {
    "id": "05087006"
   },
   "source": [
    "# Draw the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0963e9b8",
   "metadata": {
    "id": "0963e9b8"
   },
   "outputs": [],
   "source": [
    "classes = {'0':'T-shirt/top',\n",
    "           '1':'Trouser',\n",
    "           '2':'Pullover',\n",
    "           '3':'Dress',\n",
    "           '4':'Coat',\n",
    "           '5':'Sandal',\n",
    "           '6':'Shirt',\n",
    "           '7':'Sneaker',\n",
    "           '8':'Bag',\n",
    "           '9':'Ankle boot'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a6fdbf6",
   "metadata": {
    "id": "5a6fdbf6"
   },
   "outputs": [],
   "source": [
    "def draw(index = 0):\n",
    "    image = np.array(df.loc[index][1:])\n",
    "    label = str(df.loc[index][0])\n",
    "    image = image.reshape(28,28)\n",
    "    fig, (ax0) = plt.subplots(nrows=1,ncols=1,figsize=(2,2))\n",
    "    ax0.imshow(image,cmap='gray',aspect='auto')\n",
    "    plt.title(f\"The image belong to a class: \" + classes[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76acd377",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "76acd377",
    "outputId": "d82c55bf-36dc-4475-d155-e0372e14533d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b12da",
   "metadata": {
    "id": "e11b12da"
   },
   "source": [
    "### carry out required correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70ab42fd",
   "metadata": {
    "id": "70ab42fd"
   },
   "outputs": [],
   "source": [
    "def show_label_to_pixel_correlation(pixel_number = 1):\n",
    "    pixel = 'pixel' + str(pixel_number)\n",
    "    df.plot(x=pixel,y=label,kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e7cd961",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "7e7cd961",
    "outputId": "3031dfb8-f6e6-440d-e8ef-033561b47a6c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_label_to_pixel_correlation(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a0c5e",
   "metadata": {
    "id": "889a0c5e"
   },
   "source": [
    "## convert data to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b0e7088",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b0e7088",
    "outputId": "1ba04ad7-d795-4349-958f-c3c2a68e6404"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 6, ..., 8, 8, 7])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['label']\n",
    "Y = Y.to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a27b08ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a27b08ef",
    "outputId": "8c5338c8-51f9-4357-c131-3ae84a50df9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[pixels]\n",
    "X = X.to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0f5b8d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0f5b8d1",
    "outputId": "c1b003f2-ebf4-4166-fa77-d9d3d973b3fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59957, 784), (59957,))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e637f7e",
   "metadata": {
    "id": "2e637f7e"
   },
   "source": [
    "## splitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8acc4561",
   "metadata": {
    "id": "8acc4561"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea535491",
   "metadata": {
    "id": "ea535491"
   },
   "outputs": [],
   "source": [
    "x_train , x_cv , y_train , y_cv = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41882b53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41882b53",
    "outputId": "b426b5c9-1616-4b3c-b0cb-2a62c5031cf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47965, 784), (47965,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "227c2d99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "227c2d99",
    "outputId": "f1afe67e-c194-4df8-8be8-ccce330e3ed0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11992, 784), (11992,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cv.shape,y_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "192f362a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "192f362a",
    "outputId": "f1904e45-24d4-4a9b-b4fd-d9e148f4d7e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28585c9",
   "metadata": {
    "id": "c28585c9"
   },
   "source": [
    "## preprocessing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd18f2",
   "metadata": {
    "id": "f8cd18f2"
   },
   "source": [
    "### wrape the input to be an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7078d81",
   "metadata": {
    "id": "b7078d81"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda,Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb61c827",
   "metadata": {
    "id": "cb61c827"
   },
   "outputs": [],
   "source": [
    "m_train = x_train.shape[0]\n",
    "m_cv = x_cv.shape[0]\n",
    "x_train = x_train.reshape(m_train,28,28)\n",
    "x_cv = x_cv.reshape(m_cv,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "90f5875f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90f5875f",
    "outputId": "53e61b0c-8f0d-4c0e-ff1e-64fe3cfcf185",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47965, 28, 28), (11992, 28, 28))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9834620f",
   "metadata": {
    "id": "9834620f"
   },
   "outputs": [],
   "source": [
    "# Normalize and reshape the input images, we expand the shape to contain the axis for the number of channels (so that we have series of 3D images)\n",
    "x_train = np.expand_dims(x_train.astype('float32'), -1) # -1 means that the new axis will be added to the last\n",
    "x_cv = np.expand_dims(x_cv.astype('float32'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d98e82df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d98e82df",
    "outputId": "2df355bc-d169-434e-b6c3-439be3d29927"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47965, 28, 28, 1), (11992, 28, 28, 1))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76936033",
   "metadata": {
    "id": "76936033"
   },
   "source": [
    "### normalize by dividing by the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2963cf8d",
   "metadata": {
    "id": "2963cf8d"
   },
   "outputs": [],
   "source": [
    "# normalize the values to be from 0 to 1\n",
    "x_train /= 255\n",
    "x_cv /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8b3f9",
   "metadata": {
    "id": "cce8b3f9"
   },
   "source": [
    "## encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca114af2",
   "metadata": {
    "id": "ca114af2"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# convert the labels to one hot vectors  \n",
    "y_train_encoded = np_utils.to_categorical(y_train, 10)\n",
    "y_cv_encoded = np_utils.to_categorical(y_cv, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9b96967",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9b96967",
    "outputId": "1d0c9616-a2ff-48b2-8d24-bbde024ba2cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47965, 10), (11992, 10))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_encoded.shape,y_cv_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82511d",
   "metadata": {
    "id": "3b82511d"
   },
   "source": [
    "## training the NN (LeNet-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bca6c0",
   "metadata": {
    "id": "77bca6c0"
   },
   "outputs": [],
   "source": [
    "#define the hyper parameters\n",
    "optimizers = ['sgd','adam','rmsprop']\n",
    "dropouts = [0,0.5,0.8]\n",
    "learning_rates = [1e-3,3e-3]\n",
    "kernel_sizes = [3,5]\n",
    "activations = ['relu','tanh']\n",
    "batch_sizes = [32,2048,8192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z36uWsyQwC5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z36uWsyQwC5a",
    "outputId": "25386e7f-b380-497f-8063-f88fdfb7ac09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13e951",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed13e951",
    "outputId": "0d6c457c-5687-4875-bfc0-a9c4396926d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*3*2*2*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48c707",
   "metadata": {
    "id": "5b48c707"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320e150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0320e150",
    "outputId": "1101b016-d051-4aa8-fe82-c0f5405a1b69",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1672671213"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0552c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22b0552c",
    "outputId": "e8641ddc-891d-4072-bd17-3539b1013798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.5140 - accuracy: 0.8120 - val_loss: 0.4397 - val_accuracy: 0.8273 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 5s - loss: 0.3888 - accuracy: 0.8585 - val_loss: 0.3885 - val_accuracy: 0.8571 - 5s/epoch - 3ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3585 - accuracy: 0.8704 - val_loss: 0.4206 - val_accuracy: 0.8468 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3402 - accuracy: 0.8777 - val_loss: 0.3609 - val_accuracy: 0.8680 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3235 - accuracy: 0.8823 - val_loss: 0.4260 - val_accuracy: 0.8443 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3107 - accuracy: 0.8854 - val_loss: 0.3617 - val_accuracy: 0.8680 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 5s - loss: 0.3006 - accuracy: 0.8890 - val_loss: 0.3705 - val_accuracy: 0.8694 - 5s/epoch - 3ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 5s - loss: 0.2933 - accuracy: 0.8934 - val_loss: 0.3466 - val_accuracy: 0.8754 - 5s/epoch - 3ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 5s - loss: 0.2861 - accuracy: 0.8947 - val_loss: 0.3453 - val_accuracy: 0.8776 - 5s/epoch - 3ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 5s - loss: 0.2772 - accuracy: 0.8990 - val_loss: 0.3532 - val_accuracy: 0.8771 - 5s/epoch - 3ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 5s - loss: 0.2724 - accuracy: 0.9003 - val_loss: 0.3577 - val_accuracy: 0.8740 - 5s/epoch - 3ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 5s - loss: 0.2684 - accuracy: 0.9021 - val_loss: 0.3482 - val_accuracy: 0.8760 - 5s/epoch - 3ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.2609 - accuracy: 0.9036 - val_loss: 0.3801 - val_accuracy: 0.8677 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 5s - loss: 0.2590 - accuracy: 0.9046 - val_loss: 0.3491 - val_accuracy: 0.8763 - 5s/epoch - 3ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2523 - accuracy: 0.9074 - val_loss: 0.3515 - val_accuracy: 0.8748 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2476 - accuracy: 0.9091 - val_loss: 0.3454 - val_accuracy: 0.8763 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.2466 - accuracy: 0.9086 - val_loss: 0.3679 - val_accuracy: 0.8702 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2463 - accuracy: 0.9104 - val_loss: 0.3586 - val_accuracy: 0.8772 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 5s - loss: 0.2376 - accuracy: 0.9130 - val_loss: 0.3640 - val_accuracy: 0.8758 - 5s/epoch - 3ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 5s - loss: 0.2357 - accuracy: 0.9112 - val_loss: 0.3461 - val_accuracy: 0.8823 - 5s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.3185 - accuracy: 0.5299 - val_loss: 0.8467 - val_accuracy: 0.6824 - 1s/epoch - 47ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7315 - accuracy: 0.7272 - val_loss: 0.6887 - val_accuracy: 0.7393 - 435ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6202 - accuracy: 0.7625 - val_loss: 0.5929 - val_accuracy: 0.7754 - 443ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5647 - accuracy: 0.7849 - val_loss: 0.5462 - val_accuracy: 0.7935 - 465ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5236 - accuracy: 0.8033 - val_loss: 0.5098 - val_accuracy: 0.8110 - 448ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4850 - accuracy: 0.8232 - val_loss: 0.4807 - val_accuracy: 0.8237 - 442ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4569 - accuracy: 0.8343 - val_loss: 0.4767 - val_accuracy: 0.8190 - 448ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4440 - accuracy: 0.8400 - val_loss: 0.4415 - val_accuracy: 0.8416 - 448ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4205 - accuracy: 0.8483 - val_loss: 0.4285 - val_accuracy: 0.8459 - 442ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4106 - accuracy: 0.8518 - val_loss: 0.4075 - val_accuracy: 0.8552 - 464ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3964 - accuracy: 0.8564 - val_loss: 0.3964 - val_accuracy: 0.8543 - 459ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3831 - accuracy: 0.8610 - val_loss: 0.3874 - val_accuracy: 0.8579 - 453ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3714 - accuracy: 0.8665 - val_loss: 0.3824 - val_accuracy: 0.8644 - 442ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3587 - accuracy: 0.8708 - val_loss: 0.3728 - val_accuracy: 0.8636 - 460ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3610 - accuracy: 0.8695 - val_loss: 0.3976 - val_accuracy: 0.8534 - 439ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3540 - accuracy: 0.8720 - val_loss: 0.3644 - val_accuracy: 0.8727 - 442ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3364 - accuracy: 0.8797 - val_loss: 0.3558 - val_accuracy: 0.8744 - 443ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3337 - accuracy: 0.8801 - val_loss: 0.3665 - val_accuracy: 0.8708 - 439ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3315 - accuracy: 0.8809 - val_loss: 0.3527 - val_accuracy: 0.8752 - 447ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3233 - accuracy: 0.8842 - val_loss: 0.3626 - val_accuracy: 0.8681 - 452ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.0838 - accuracy: 0.6149 - val_loss: 0.6634 - val_accuracy: 0.7432 - 1s/epoch - 44ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.5899 - accuracy: 0.7798 - val_loss: 0.5660 - val_accuracy: 0.7935 - 445ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.5115 - accuracy: 0.8148 - val_loss: 0.5060 - val_accuracy: 0.8100 - 436ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.4652 - accuracy: 0.8319 - val_loss: 0.4619 - val_accuracy: 0.8262 - 441ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.4336 - accuracy: 0.8447 - val_loss: 0.4410 - val_accuracy: 0.8358 - 442ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4118 - accuracy: 0.8517 - val_loss: 0.4097 - val_accuracy: 0.8507 - 453ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.3861 - accuracy: 0.8612 - val_loss: 0.3903 - val_accuracy: 0.8607 - 443ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.3751 - accuracy: 0.8668 - val_loss: 0.3854 - val_accuracy: 0.8568 - 448ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.3621 - accuracy: 0.8704 - val_loss: 0.3774 - val_accuracy: 0.8627 - 458ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.3524 - accuracy: 0.8727 - val_loss: 0.3636 - val_accuracy: 0.8662 - 462ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3473 - accuracy: 0.8746 - val_loss: 0.3633 - val_accuracy: 0.8668 - 443ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3348 - accuracy: 0.8792 - val_loss: 0.3525 - val_accuracy: 0.8731 - 450ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3229 - accuracy: 0.8825 - val_loss: 0.3407 - val_accuracy: 0.8773 - 470ms/epoch - 20ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3153 - accuracy: 0.8865 - val_loss: 0.3375 - val_accuracy: 0.8739 - 455ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3110 - accuracy: 0.8863 - val_loss: 0.3385 - val_accuracy: 0.8766 - 443ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3014 - accuracy: 0.8911 - val_loss: 0.3270 - val_accuracy: 0.8802 - 442ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.2921 - accuracy: 0.8938 - val_loss: 0.3254 - val_accuracy: 0.8798 - 453ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.2860 - accuracy: 0.8980 - val_loss: 0.3202 - val_accuracy: 0.8833 - 465ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.2819 - accuracy: 0.8977 - val_loss: 0.3231 - val_accuracy: 0.8805 - 463ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.2787 - accuracy: 0.8985 - val_loss: 0.3191 - val_accuracy: 0.8818 - 451ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1637 - accuracy: 0.3021 - val_loss: 1.7070 - val_accuracy: 0.5109 - 1s/epoch - 174ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.3265 - accuracy: 0.5693 - val_loss: 1.1387 - val_accuracy: 0.5989 - 438ms/epoch - 73ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1060 - accuracy: 0.6261 - val_loss: 0.8768 - val_accuracy: 0.6861 - 436ms/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8986 - accuracy: 0.6649 - val_loss: 0.8798 - val_accuracy: 0.6937 - 417ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8237 - accuracy: 0.6958 - val_loss: 0.7704 - val_accuracy: 0.7086 - 439ms/epoch - 73ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7472 - accuracy: 0.7217 - val_loss: 0.7211 - val_accuracy: 0.7160 - 422ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7018 - accuracy: 0.7323 - val_loss: 0.6843 - val_accuracy: 0.7399 - 421ms/epoch - 70ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6649 - accuracy: 0.7489 - val_loss: 0.6552 - val_accuracy: 0.7513 - 437ms/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6408 - accuracy: 0.7560 - val_loss: 0.6387 - val_accuracy: 0.7583 - 436ms/epoch - 73ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6192 - accuracy: 0.7659 - val_loss: 0.6144 - val_accuracy: 0.7623 - 437ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.5974 - accuracy: 0.7752 - val_loss: 0.5991 - val_accuracy: 0.7715 - 411ms/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5789 - accuracy: 0.7843 - val_loss: 0.5807 - val_accuracy: 0.7853 - 440ms/epoch - 73ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5627 - accuracy: 0.7911 - val_loss: 0.5684 - val_accuracy: 0.7884 - 440ms/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5471 - accuracy: 0.7971 - val_loss: 0.5526 - val_accuracy: 0.7983 - 426ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5354 - accuracy: 0.8039 - val_loss: 0.5412 - val_accuracy: 0.8007 - 432ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5226 - accuracy: 0.8095 - val_loss: 0.5282 - val_accuracy: 0.8045 - 418ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5108 - accuracy: 0.8155 - val_loss: 0.5212 - val_accuracy: 0.8076 - 437ms/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5016 - accuracy: 0.8174 - val_loss: 0.5129 - val_accuracy: 0.8135 - 443ms/epoch - 74ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4954 - accuracy: 0.8198 - val_loss: 0.5088 - val_accuracy: 0.8130 - 420ms/epoch - 70ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4908 - accuracy: 0.8205 - val_loss: 0.5091 - val_accuracy: 0.8130 - 418ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.7733 - accuracy: 0.4342 - val_loss: 1.1354 - val_accuracy: 0.5806 - 1s/epoch - 180ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 0.9947 - accuracy: 0.6377 - val_loss: 0.8561 - val_accuracy: 0.6785 - 447ms/epoch - 74ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.7921 - accuracy: 0.7025 - val_loss: 0.7158 - val_accuracy: 0.7270 - 443ms/epoch - 74ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.6797 - accuracy: 0.7400 - val_loss: 0.6599 - val_accuracy: 0.7504 - 422ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.6238 - accuracy: 0.7645 - val_loss: 0.6209 - val_accuracy: 0.7690 - 428ms/epoch - 71ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.5839 - accuracy: 0.7818 - val_loss: 0.5871 - val_accuracy: 0.7794 - 423ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.5531 - accuracy: 0.7959 - val_loss: 0.5588 - val_accuracy: 0.7969 - 444ms/epoch - 74ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.5257 - accuracy: 0.8099 - val_loss: 0.5323 - val_accuracy: 0.8070 - 443ms/epoch - 74ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.5042 - accuracy: 0.8186 - val_loss: 0.5158 - val_accuracy: 0.8115 - 433ms/epoch - 72ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.4883 - accuracy: 0.8210 - val_loss: 0.5071 - val_accuracy: 0.8095 - 434ms/epoch - 72ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.4737 - accuracy: 0.8286 - val_loss: 0.4814 - val_accuracy: 0.8261 - 444ms/epoch - 74ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.4575 - accuracy: 0.8352 - val_loss: 0.4705 - val_accuracy: 0.8262 - 420ms/epoch - 70ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.4437 - accuracy: 0.8415 - val_loss: 0.4531 - val_accuracy: 0.8366 - 425ms/epoch - 71ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.4309 - accuracy: 0.8465 - val_loss: 0.4457 - val_accuracy: 0.8391 - 418ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.4191 - accuracy: 0.8509 - val_loss: 0.4351 - val_accuracy: 0.8426 - 419ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.4113 - accuracy: 0.8539 - val_loss: 0.4259 - val_accuracy: 0.8471 - 443ms/epoch - 74ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.4021 - accuracy: 0.8586 - val_loss: 0.4180 - val_accuracy: 0.8464 - 434ms/epoch - 72ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.3979 - accuracy: 0.8588 - val_loss: 0.4124 - val_accuracy: 0.8491 - 425ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.3900 - accuracy: 0.8620 - val_loss: 0.4126 - val_accuracy: 0.8528 - 422ms/epoch - 70ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.3834 - accuracy: 0.8637 - val_loss: 0.4015 - val_accuracy: 0.8509 - 418ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6177 - accuracy: 0.7726 - val_loss: 0.4325 - val_accuracy: 0.8428 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4290 - accuracy: 0.8433 - val_loss: 0.3654 - val_accuracy: 0.8659 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3770 - accuracy: 0.8622 - val_loss: 0.3467 - val_accuracy: 0.8731 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3451 - accuracy: 0.8729 - val_loss: 0.3195 - val_accuracy: 0.8862 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3224 - accuracy: 0.8827 - val_loss: 0.3140 - val_accuracy: 0.8849 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 7s - loss: 0.3059 - accuracy: 0.8889 - val_loss: 0.3017 - val_accuracy: 0.8892 - 7s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.2889 - accuracy: 0.8923 - val_loss: 0.2823 - val_accuracy: 0.8969 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.2774 - accuracy: 0.8973 - val_loss: 0.2771 - val_accuracy: 0.8957 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.2650 - accuracy: 0.9019 - val_loss: 0.2706 - val_accuracy: 0.9003 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.2568 - accuracy: 0.9048 - val_loss: 0.2690 - val_accuracy: 0.8988 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2479 - accuracy: 0.9069 - val_loss: 0.2703 - val_accuracy: 0.8993 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.2380 - accuracy: 0.9100 - val_loss: 0.2572 - val_accuracy: 0.9038 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.2320 - accuracy: 0.9132 - val_loss: 0.2670 - val_accuracy: 0.9023 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.2267 - accuracy: 0.9148 - val_loss: 0.2560 - val_accuracy: 0.9049 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2164 - accuracy: 0.9180 - val_loss: 0.2546 - val_accuracy: 0.9050 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2124 - accuracy: 0.9201 - val_loss: 0.2589 - val_accuracy: 0.9030 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.2059 - accuracy: 0.9223 - val_loss: 0.2480 - val_accuracy: 0.9092 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2038 - accuracy: 0.9235 - val_loss: 0.2531 - val_accuracy: 0.9079 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.1983 - accuracy: 0.9240 - val_loss: 0.2438 - val_accuracy: 0.9104 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.1948 - accuracy: 0.9274 - val_loss: 0.2556 - val_accuracy: 0.9043 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5681 - accuracy: 0.7893 - val_loss: 0.4585 - val_accuracy: 0.8280 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4297 - accuracy: 0.8447 - val_loss: 0.3922 - val_accuracy: 0.8562 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3914 - accuracy: 0.8573 - val_loss: 0.3876 - val_accuracy: 0.8573 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3706 - accuracy: 0.8649 - val_loss: 0.3512 - val_accuracy: 0.8702 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3549 - accuracy: 0.8714 - val_loss: 0.3567 - val_accuracy: 0.8692 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3457 - accuracy: 0.8741 - val_loss: 0.3500 - val_accuracy: 0.8716 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3372 - accuracy: 0.8776 - val_loss: 0.3498 - val_accuracy: 0.8707 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3322 - accuracy: 0.8794 - val_loss: 0.3418 - val_accuracy: 0.8740 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3222 - accuracy: 0.8814 - val_loss: 0.3412 - val_accuracy: 0.8712 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3206 - accuracy: 0.8829 - val_loss: 0.3389 - val_accuracy: 0.8712 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3171 - accuracy: 0.8844 - val_loss: 0.3417 - val_accuracy: 0.8742 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3139 - accuracy: 0.8839 - val_loss: 0.3475 - val_accuracy: 0.8711 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3107 - accuracy: 0.8859 - val_loss: 0.3289 - val_accuracy: 0.8799 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.3093 - accuracy: 0.8854 - val_loss: 0.3367 - val_accuracy: 0.8763 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3087 - accuracy: 0.8860 - val_loss: 0.3398 - val_accuracy: 0.8774 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3064 - accuracy: 0.8867 - val_loss: 0.3292 - val_accuracy: 0.8786 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.3041 - accuracy: 0.8864 - val_loss: 0.3365 - val_accuracy: 0.8753 - 7s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3044 - accuracy: 0.8871 - val_loss: 0.3269 - val_accuracy: 0.8822 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2996 - accuracy: 0.8910 - val_loss: 0.3283 - val_accuracy: 0.8788 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2984 - accuracy: 0.8904 - val_loss: 0.3385 - val_accuracy: 0.8743 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 2s - loss: 1.7854 - accuracy: 0.4390 - val_loss: 0.9546 - val_accuracy: 0.6639 - 2s/epoch - 64ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8492 - accuracy: 0.6839 - val_loss: 0.7228 - val_accuracy: 0.7291 - 444ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7054 - accuracy: 0.7347 - val_loss: 0.6690 - val_accuracy: 0.7448 - 451ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6448 - accuracy: 0.7564 - val_loss: 0.6094 - val_accuracy: 0.7707 - 444ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6001 - accuracy: 0.7763 - val_loss: 0.5807 - val_accuracy: 0.7796 - 462ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5603 - accuracy: 0.7924 - val_loss: 0.5292 - val_accuracy: 0.8030 - 443ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5317 - accuracy: 0.8028 - val_loss: 0.5058 - val_accuracy: 0.8114 - 452ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5061 - accuracy: 0.8157 - val_loss: 0.4780 - val_accuracy: 0.8286 - 437ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4871 - accuracy: 0.8224 - val_loss: 0.4571 - val_accuracy: 0.8351 - 448ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4694 - accuracy: 0.8295 - val_loss: 0.4456 - val_accuracy: 0.8381 - 448ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4551 - accuracy: 0.8340 - val_loss: 0.4397 - val_accuracy: 0.8435 - 455ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4412 - accuracy: 0.8401 - val_loss: 0.4202 - val_accuracy: 0.8470 - 466ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4315 - accuracy: 0.8442 - val_loss: 0.4062 - val_accuracy: 0.8557 - 442ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4186 - accuracy: 0.8491 - val_loss: 0.4023 - val_accuracy: 0.8543 - 443ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4098 - accuracy: 0.8516 - val_loss: 0.3926 - val_accuracy: 0.8527 - 458ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4000 - accuracy: 0.8557 - val_loss: 0.3788 - val_accuracy: 0.8639 - 459ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3912 - accuracy: 0.8588 - val_loss: 0.3719 - val_accuracy: 0.8652 - 449ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3848 - accuracy: 0.8622 - val_loss: 0.3646 - val_accuracy: 0.8702 - 461ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3789 - accuracy: 0.8620 - val_loss: 0.3604 - val_accuracy: 0.8725 - 448ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3767 - accuracy: 0.8638 - val_loss: 0.3606 - val_accuracy: 0.8692 - 444ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.2621 - accuracy: 0.5705 - val_loss: 0.8005 - val_accuracy: 0.6978 - 1s/epoch - 44ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7264 - accuracy: 0.7284 - val_loss: 0.6417 - val_accuracy: 0.7597 - 446ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6182 - accuracy: 0.7711 - val_loss: 0.5838 - val_accuracy: 0.7839 - 453ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5668 - accuracy: 0.7913 - val_loss: 0.5454 - val_accuracy: 0.8030 - 443ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5362 - accuracy: 0.8026 - val_loss: 0.5134 - val_accuracy: 0.8100 - 444ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5086 - accuracy: 0.8163 - val_loss: 0.4911 - val_accuracy: 0.8212 - 448ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4903 - accuracy: 0.8223 - val_loss: 0.4744 - val_accuracy: 0.8256 - 457ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4738 - accuracy: 0.8295 - val_loss: 0.4572 - val_accuracy: 0.8340 - 461ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4581 - accuracy: 0.8342 - val_loss: 0.4474 - val_accuracy: 0.8341 - 453ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4459 - accuracy: 0.8383 - val_loss: 0.4319 - val_accuracy: 0.8433 - 445ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4342 - accuracy: 0.8432 - val_loss: 0.4240 - val_accuracy: 0.8413 - 462ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4227 - accuracy: 0.8488 - val_loss: 0.4141 - val_accuracy: 0.8512 - 456ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4168 - accuracy: 0.8492 - val_loss: 0.4099 - val_accuracy: 0.8492 - 463ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4075 - accuracy: 0.8546 - val_loss: 0.3985 - val_accuracy: 0.8529 - 454ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3996 - accuracy: 0.8552 - val_loss: 0.3958 - val_accuracy: 0.8566 - 439ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3913 - accuracy: 0.8596 - val_loss: 0.3877 - val_accuracy: 0.8585 - 439ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3866 - accuracy: 0.8593 - val_loss: 0.3848 - val_accuracy: 0.8596 - 466ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3830 - accuracy: 0.8619 - val_loss: 0.3811 - val_accuracy: 0.8584 - 447ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3779 - accuracy: 0.8626 - val_loss: 0.3797 - val_accuracy: 0.8589 - 461ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3719 - accuracy: 0.8658 - val_loss: 0.3727 - val_accuracy: 0.8595 - 441ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1985 - accuracy: 0.2161 - val_loss: 1.9640 - val_accuracy: 0.4530 - 1s/epoch - 175ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.7425 - accuracy: 0.4991 - val_loss: 1.3278 - val_accuracy: 0.6382 - 427ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1772 - accuracy: 0.6095 - val_loss: 0.9204 - val_accuracy: 0.6896 - 423ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.9459 - accuracy: 0.6556 - val_loss: 0.8286 - val_accuracy: 0.7083 - 424ms/epoch - 71ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8601 - accuracy: 0.6900 - val_loss: 0.7633 - val_accuracy: 0.7239 - 424ms/epoch - 71ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7932 - accuracy: 0.7097 - val_loss: 0.7276 - val_accuracy: 0.7347 - 420ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7471 - accuracy: 0.7253 - val_loss: 0.7015 - val_accuracy: 0.7466 - 431ms/epoch - 72ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7179 - accuracy: 0.7377 - val_loss: 0.6764 - val_accuracy: 0.7528 - 422ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6900 - accuracy: 0.7485 - val_loss: 0.6527 - val_accuracy: 0.7617 - 427ms/epoch - 71ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6629 - accuracy: 0.7567 - val_loss: 0.6320 - val_accuracy: 0.7725 - 418ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6490 - accuracy: 0.7641 - val_loss: 0.6175 - val_accuracy: 0.7763 - 420ms/epoch - 70ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6302 - accuracy: 0.7691 - val_loss: 0.6024 - val_accuracy: 0.7824 - 412ms/epoch - 69ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6139 - accuracy: 0.7766 - val_loss: 0.5869 - val_accuracy: 0.7863 - 418ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5971 - accuracy: 0.7820 - val_loss: 0.5719 - val_accuracy: 0.7936 - 418ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5857 - accuracy: 0.7866 - val_loss: 0.5621 - val_accuracy: 0.7942 - 447ms/epoch - 74ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5745 - accuracy: 0.7916 - val_loss: 0.5516 - val_accuracy: 0.8011 - 420ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5627 - accuracy: 0.7955 - val_loss: 0.5405 - val_accuracy: 0.8013 - 417ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5500 - accuracy: 0.7977 - val_loss: 0.5315 - val_accuracy: 0.8080 - 436ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5405 - accuracy: 0.8019 - val_loss: 0.5218 - val_accuracy: 0.8060 - 450ms/epoch - 75ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5304 - accuracy: 0.8068 - val_loss: 0.5078 - val_accuracy: 0.8159 - 419ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0290 - accuracy: 0.3814 - val_loss: 1.6012 - val_accuracy: 0.6157 - 1s/epoch - 187ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.3654 - accuracy: 0.6046 - val_loss: 1.0844 - val_accuracy: 0.6418 - 417ms/epoch - 69ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.0211 - accuracy: 0.6336 - val_loss: 0.9061 - val_accuracy: 0.6601 - 416ms/epoch - 69ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8861 - accuracy: 0.6681 - val_loss: 0.8045 - val_accuracy: 0.7126 - 419ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8001 - accuracy: 0.7035 - val_loss: 0.7415 - val_accuracy: 0.7211 - 416ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7388 - accuracy: 0.7244 - val_loss: 0.6941 - val_accuracy: 0.7420 - 434ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6944 - accuracy: 0.7399 - val_loss: 0.6676 - val_accuracy: 0.7487 - 417ms/epoch - 70ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6640 - accuracy: 0.7513 - val_loss: 0.6414 - val_accuracy: 0.7627 - 440ms/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6400 - accuracy: 0.7636 - val_loss: 0.6231 - val_accuracy: 0.7689 - 425ms/epoch - 71ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6199 - accuracy: 0.7704 - val_loss: 0.6048 - val_accuracy: 0.7775 - 439ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6029 - accuracy: 0.7779 - val_loss: 0.5897 - val_accuracy: 0.7839 - 429ms/epoch - 72ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5878 - accuracy: 0.7843 - val_loss: 0.5746 - val_accuracy: 0.7924 - 422ms/epoch - 70ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5742 - accuracy: 0.7905 - val_loss: 0.5626 - val_accuracy: 0.7957 - 433ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5611 - accuracy: 0.7954 - val_loss: 0.5502 - val_accuracy: 0.8007 - 429ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5495 - accuracy: 0.8006 - val_loss: 0.5396 - val_accuracy: 0.8048 - 432ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5413 - accuracy: 0.8039 - val_loss: 0.5310 - val_accuracy: 0.8077 - 419ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5300 - accuracy: 0.8074 - val_loss: 0.5212 - val_accuracy: 0.8137 - 421ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5202 - accuracy: 0.8129 - val_loss: 0.5120 - val_accuracy: 0.8168 - 429ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5139 - accuracy: 0.8126 - val_loss: 0.5028 - val_accuracy: 0.8217 - 429ms/epoch - 71ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5035 - accuracy: 0.8189 - val_loss: 0.4955 - val_accuracy: 0.8215 - 431ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.7376 - accuracy: 0.7246 - val_loss: 0.5133 - val_accuracy: 0.8000 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.5129 - accuracy: 0.8121 - val_loss: 0.4116 - val_accuracy: 0.8466 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4506 - accuracy: 0.8391 - val_loss: 0.3758 - val_accuracy: 0.8592 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4080 - accuracy: 0.8545 - val_loss: 0.3525 - val_accuracy: 0.8664 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3787 - accuracy: 0.8640 - val_loss: 0.3271 - val_accuracy: 0.8771 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 5s - loss: 0.3556 - accuracy: 0.8716 - val_loss: 0.3197 - val_accuracy: 0.8812 - 5s/epoch - 3ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 5s - loss: 0.3431 - accuracy: 0.8770 - val_loss: 0.3024 - val_accuracy: 0.8879 - 5s/epoch - 3ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3254 - accuracy: 0.8817 - val_loss: 0.3004 - val_accuracy: 0.8888 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 5s - loss: 0.3104 - accuracy: 0.8885 - val_loss: 0.2933 - val_accuracy: 0.8900 - 5s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3037 - accuracy: 0.8912 - val_loss: 0.2935 - val_accuracy: 0.8895 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2942 - accuracy: 0.8932 - val_loss: 0.2743 - val_accuracy: 0.8972 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 5s - loss: 0.2865 - accuracy: 0.8954 - val_loss: 0.2730 - val_accuracy: 0.8978 - 5s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.2825 - accuracy: 0.8979 - val_loss: 0.2806 - val_accuracy: 0.8933 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.2744 - accuracy: 0.9006 - val_loss: 0.2780 - val_accuracy: 0.8960 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 5s - loss: 0.2685 - accuracy: 0.9012 - val_loss: 0.2737 - val_accuracy: 0.8981 - 5s/epoch - 3ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 5s - loss: 0.2644 - accuracy: 0.9029 - val_loss: 0.2667 - val_accuracy: 0.9014 - 5s/epoch - 3ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 5s - loss: 0.2618 - accuracy: 0.9049 - val_loss: 0.2703 - val_accuracy: 0.8990 - 5s/epoch - 3ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2576 - accuracy: 0.9058 - val_loss: 0.2633 - val_accuracy: 0.9020 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 5s - loss: 0.2519 - accuracy: 0.9075 - val_loss: 0.2642 - val_accuracy: 0.9035 - 5s/epoch - 3ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2470 - accuracy: 0.9096 - val_loss: 0.2727 - val_accuracy: 0.9003 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.6232 - accuracy: 0.7703 - val_loss: 0.4852 - val_accuracy: 0.8234 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4779 - accuracy: 0.8279 - val_loss: 0.4203 - val_accuracy: 0.8431 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 5s - loss: 0.4393 - accuracy: 0.8413 - val_loss: 0.3975 - val_accuracy: 0.8549 - 5s/epoch - 3ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4113 - accuracy: 0.8511 - val_loss: 0.3719 - val_accuracy: 0.8622 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 5s - loss: 0.3911 - accuracy: 0.8597 - val_loss: 0.3623 - val_accuracy: 0.8646 - 5s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 5s - loss: 0.3751 - accuracy: 0.8638 - val_loss: 0.3443 - val_accuracy: 0.8728 - 5s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3611 - accuracy: 0.8697 - val_loss: 0.3452 - val_accuracy: 0.8717 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3458 - accuracy: 0.8755 - val_loss: 0.3263 - val_accuracy: 0.8790 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 5s - loss: 0.3382 - accuracy: 0.8780 - val_loss: 0.3237 - val_accuracy: 0.8804 - 5s/epoch - 3ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 5s - loss: 0.3291 - accuracy: 0.8807 - val_loss: 0.3112 - val_accuracy: 0.8828 - 5s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 5s - loss: 0.3202 - accuracy: 0.8853 - val_loss: 0.3106 - val_accuracy: 0.8848 - 5s/epoch - 3ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 5s - loss: 0.3102 - accuracy: 0.8879 - val_loss: 0.3060 - val_accuracy: 0.8898 - 5s/epoch - 3ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3052 - accuracy: 0.8893 - val_loss: 0.3138 - val_accuracy: 0.8825 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 5s - loss: 0.3015 - accuracy: 0.8914 - val_loss: 0.3013 - val_accuracy: 0.8891 - 5s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 5s - loss: 0.2921 - accuracy: 0.8949 - val_loss: 0.3063 - val_accuracy: 0.8889 - 5s/epoch - 3ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2893 - accuracy: 0.8965 - val_loss: 0.3116 - val_accuracy: 0.8852 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 5s - loss: 0.2858 - accuracy: 0.8970 - val_loss: 0.2971 - val_accuracy: 0.8870 - 5s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2821 - accuracy: 0.8978 - val_loss: 0.2884 - val_accuracy: 0.8925 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 5s - loss: 0.2760 - accuracy: 0.8997 - val_loss: 0.2884 - val_accuracy: 0.8923 - 5s/epoch - 3ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 5s - loss: 0.2720 - accuracy: 0.9019 - val_loss: 0.2873 - val_accuracy: 0.8951 - 5s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.9326 - accuracy: 0.3389 - val_loss: 1.0831 - val_accuracy: 0.6586 - 1s/epoch - 49ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.9888 - accuracy: 0.6245 - val_loss: 0.7272 - val_accuracy: 0.7277 - 455ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7553 - accuracy: 0.7136 - val_loss: 0.6289 - val_accuracy: 0.7544 - 474ms/epoch - 20ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6754 - accuracy: 0.7430 - val_loss: 0.5900 - val_accuracy: 0.7655 - 444ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6308 - accuracy: 0.7607 - val_loss: 0.5599 - val_accuracy: 0.7822 - 439ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5978 - accuracy: 0.7741 - val_loss: 0.5359 - val_accuracy: 0.7919 - 446ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5714 - accuracy: 0.7860 - val_loss: 0.5211 - val_accuracy: 0.8007 - 462ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5575 - accuracy: 0.7929 - val_loss: 0.4985 - val_accuracy: 0.8127 - 474ms/epoch - 20ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5351 - accuracy: 0.8030 - val_loss: 0.4860 - val_accuracy: 0.8160 - 435ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5245 - accuracy: 0.8079 - val_loss: 0.4822 - val_accuracy: 0.8214 - 460ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5081 - accuracy: 0.8146 - val_loss: 0.4651 - val_accuracy: 0.8309 - 459ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5012 - accuracy: 0.8176 - val_loss: 0.4556 - val_accuracy: 0.8325 - 452ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4897 - accuracy: 0.8206 - val_loss: 0.4443 - val_accuracy: 0.8383 - 451ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4777 - accuracy: 0.8253 - val_loss: 0.4406 - val_accuracy: 0.8356 - 437ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4718 - accuracy: 0.8303 - val_loss: 0.4289 - val_accuracy: 0.8419 - 457ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4642 - accuracy: 0.8303 - val_loss: 0.4208 - val_accuracy: 0.8424 - 443ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4578 - accuracy: 0.8337 - val_loss: 0.4138 - val_accuracy: 0.8472 - 449ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4505 - accuracy: 0.8372 - val_loss: 0.4087 - val_accuracy: 0.8442 - 453ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4442 - accuracy: 0.8382 - val_loss: 0.4049 - val_accuracy: 0.8488 - 468ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4378 - accuracy: 0.8419 - val_loss: 0.3945 - val_accuracy: 0.8536 - 488ms/epoch - 20ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.4753 - accuracy: 0.4939 - val_loss: 0.9267 - val_accuracy: 0.6639 - 1s/epoch - 45ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8645 - accuracy: 0.6754 - val_loss: 0.7136 - val_accuracy: 0.7320 - 449ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7091 - accuracy: 0.7365 - val_loss: 0.6346 - val_accuracy: 0.7627 - 451ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6363 - accuracy: 0.7648 - val_loss: 0.5855 - val_accuracy: 0.7849 - 435ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5926 - accuracy: 0.7820 - val_loss: 0.5582 - val_accuracy: 0.7972 - 466ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5617 - accuracy: 0.7951 - val_loss: 0.5289 - val_accuracy: 0.8096 - 443ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5422 - accuracy: 0.8025 - val_loss: 0.5154 - val_accuracy: 0.8160 - 455ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5227 - accuracy: 0.8119 - val_loss: 0.4977 - val_accuracy: 0.8174 - 435ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5089 - accuracy: 0.8161 - val_loss: 0.4851 - val_accuracy: 0.8263 - 461ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4973 - accuracy: 0.8218 - val_loss: 0.4754 - val_accuracy: 0.8255 - 443ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4836 - accuracy: 0.8264 - val_loss: 0.4595 - val_accuracy: 0.8346 - 445ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4736 - accuracy: 0.8295 - val_loss: 0.4535 - val_accuracy: 0.8351 - 458ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4654 - accuracy: 0.8322 - val_loss: 0.4419 - val_accuracy: 0.8399 - 460ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4559 - accuracy: 0.8365 - val_loss: 0.4412 - val_accuracy: 0.8417 - 459ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4509 - accuracy: 0.8381 - val_loss: 0.4296 - val_accuracy: 0.8441 - 455ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4468 - accuracy: 0.8412 - val_loss: 0.4360 - val_accuracy: 0.8391 - 457ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4385 - accuracy: 0.8434 - val_loss: 0.4239 - val_accuracy: 0.8463 - 442ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4294 - accuracy: 0.8453 - val_loss: 0.4133 - val_accuracy: 0.8476 - 433ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4256 - accuracy: 0.8477 - val_loss: 0.4065 - val_accuracy: 0.8499 - 454ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4212 - accuracy: 0.8489 - val_loss: 0.4070 - val_accuracy: 0.8507 - 440ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.2767 - accuracy: 0.1550 - val_loss: 2.2066 - val_accuracy: 0.2462 - 1s/epoch - 174ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 2.1272 - accuracy: 0.2415 - val_loss: 1.8936 - val_accuracy: 0.4330 - 430ms/epoch - 72ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.7815 - accuracy: 0.4029 - val_loss: 1.3660 - val_accuracy: 0.5650 - 435ms/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.3819 - accuracy: 0.5055 - val_loss: 1.0187 - val_accuracy: 0.6381 - 418ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.1278 - accuracy: 0.5731 - val_loss: 0.8993 - val_accuracy: 0.6681 - 415ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.9836 - accuracy: 0.6276 - val_loss: 0.8217 - val_accuracy: 0.6932 - 438ms/epoch - 73ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8921 - accuracy: 0.6636 - val_loss: 0.7622 - val_accuracy: 0.7139 - 429ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.8271 - accuracy: 0.6884 - val_loss: 0.7213 - val_accuracy: 0.7288 - 434ms/epoch - 72ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7838 - accuracy: 0.7051 - val_loss: 0.6905 - val_accuracy: 0.7362 - 432ms/epoch - 72ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7561 - accuracy: 0.7168 - val_loss: 0.6671 - val_accuracy: 0.7500 - 412ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.7292 - accuracy: 0.7257 - val_loss: 0.6542 - val_accuracy: 0.7523 - 423ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7105 - accuracy: 0.7337 - val_loss: 0.6400 - val_accuracy: 0.7574 - 441ms/epoch - 74ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6900 - accuracy: 0.7418 - val_loss: 0.6254 - val_accuracy: 0.7657 - 440ms/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6735 - accuracy: 0.7484 - val_loss: 0.6141 - val_accuracy: 0.7691 - 428ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6623 - accuracy: 0.7523 - val_loss: 0.6017 - val_accuracy: 0.7696 - 420ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6508 - accuracy: 0.7565 - val_loss: 0.5910 - val_accuracy: 0.7764 - 414ms/epoch - 69ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.6408 - accuracy: 0.7607 - val_loss: 0.5816 - val_accuracy: 0.7826 - 439ms/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6268 - accuracy: 0.7657 - val_loss: 0.5706 - val_accuracy: 0.7859 - 435ms/epoch - 72ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6212 - accuracy: 0.7685 - val_loss: 0.5650 - val_accuracy: 0.7839 - 410ms/epoch - 68ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.6107 - accuracy: 0.7725 - val_loss: 0.5577 - val_accuracy: 0.7885 - 426ms/epoch - 71ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0985 - accuracy: 0.3155 - val_loss: 1.7474 - val_accuracy: 0.6067 - 1s/epoch - 176ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.5354 - accuracy: 0.5230 - val_loss: 1.2177 - val_accuracy: 0.6254 - 417ms/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1657 - accuracy: 0.5857 - val_loss: 0.9893 - val_accuracy: 0.6502 - 425ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.0064 - accuracy: 0.6238 - val_loss: 0.8831 - val_accuracy: 0.6670 - 417ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9119 - accuracy: 0.6557 - val_loss: 0.8083 - val_accuracy: 0.6990 - 419ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8399 - accuracy: 0.6822 - val_loss: 0.7487 - val_accuracy: 0.7176 - 436ms/epoch - 73ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7801 - accuracy: 0.7067 - val_loss: 0.7055 - val_accuracy: 0.7268 - 420ms/epoch - 70ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7323 - accuracy: 0.7231 - val_loss: 0.6746 - val_accuracy: 0.7399 - 424ms/epoch - 71ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6980 - accuracy: 0.7377 - val_loss: 0.6509 - val_accuracy: 0.7513 - 416ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6703 - accuracy: 0.7478 - val_loss: 0.6323 - val_accuracy: 0.7580 - 427ms/epoch - 71ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6514 - accuracy: 0.7535 - val_loss: 0.6183 - val_accuracy: 0.7633 - 434ms/epoch - 72ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6369 - accuracy: 0.7597 - val_loss: 0.6063 - val_accuracy: 0.7680 - 431ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6190 - accuracy: 0.7674 - val_loss: 0.5941 - val_accuracy: 0.7743 - 431ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6079 - accuracy: 0.7717 - val_loss: 0.5867 - val_accuracy: 0.7808 - 422ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5996 - accuracy: 0.7776 - val_loss: 0.5835 - val_accuracy: 0.7820 - 422ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5903 - accuracy: 0.7813 - val_loss: 0.5678 - val_accuracy: 0.7917 - 441ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5822 - accuracy: 0.7838 - val_loss: 0.5609 - val_accuracy: 0.7933 - 418ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5713 - accuracy: 0.7908 - val_loss: 0.5523 - val_accuracy: 0.7958 - 428ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5641 - accuracy: 0.7927 - val_loss: 0.5472 - val_accuracy: 0.7979 - 438ms/epoch - 73ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5559 - accuracy: 0.7967 - val_loss: 0.5406 - val_accuracy: 0.8005 - 415ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5285 - accuracy: 0.8040 - val_loss: 0.4223 - val_accuracy: 0.8371 - 7s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.3783 - accuracy: 0.8605 - val_loss: 0.3360 - val_accuracy: 0.8744 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3414 - accuracy: 0.8734 - val_loss: 0.3127 - val_accuracy: 0.8837 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3177 - accuracy: 0.8821 - val_loss: 0.2941 - val_accuracy: 0.8887 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3017 - accuracy: 0.8875 - val_loss: 0.2826 - val_accuracy: 0.8939 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.2905 - accuracy: 0.8908 - val_loss: 0.2936 - val_accuracy: 0.8907 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.2835 - accuracy: 0.8928 - val_loss: 0.2939 - val_accuracy: 0.8888 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.2754 - accuracy: 0.8972 - val_loss: 0.2889 - val_accuracy: 0.8928 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.2671 - accuracy: 0.8998 - val_loss: 0.2780 - val_accuracy: 0.8958 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.2592 - accuracy: 0.9035 - val_loss: 0.2745 - val_accuracy: 0.8998 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2543 - accuracy: 0.9044 - val_loss: 0.2714 - val_accuracy: 0.9021 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.2505 - accuracy: 0.9060 - val_loss: 0.2662 - val_accuracy: 0.9039 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.2432 - accuracy: 0.9084 - val_loss: 0.2832 - val_accuracy: 0.8998 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.2378 - accuracy: 0.9117 - val_loss: 0.2783 - val_accuracy: 0.9018 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2349 - accuracy: 0.9107 - val_loss: 0.2600 - val_accuracy: 0.9078 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2333 - accuracy: 0.9121 - val_loss: 0.2841 - val_accuracy: 0.8994 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.2290 - accuracy: 0.9127 - val_loss: 0.2639 - val_accuracy: 0.9051 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2264 - accuracy: 0.9156 - val_loss: 0.2607 - val_accuracy: 0.9060 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2219 - accuracy: 0.9169 - val_loss: 0.2758 - val_accuracy: 0.9024 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2192 - accuracy: 0.9181 - val_loss: 0.2642 - val_accuracy: 0.9041 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.5654 - accuracy: 0.7927 - val_loss: 0.4764 - val_accuracy: 0.8256 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4938 - accuracy: 0.8190 - val_loss: 0.4682 - val_accuracy: 0.8264 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4882 - accuracy: 0.8222 - val_loss: 0.4799 - val_accuracy: 0.8238 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4785 - accuracy: 0.8258 - val_loss: 0.4752 - val_accuracy: 0.8300 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.4712 - accuracy: 0.8306 - val_loss: 0.4499 - val_accuracy: 0.8340 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.4596 - accuracy: 0.8319 - val_loss: 0.4393 - val_accuracy: 0.8387 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.4593 - accuracy: 0.8316 - val_loss: 0.4495 - val_accuracy: 0.8315 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.4467 - accuracy: 0.8374 - val_loss: 0.4386 - val_accuracy: 0.8372 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.4368 - accuracy: 0.8418 - val_loss: 0.4282 - val_accuracy: 0.8411 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4399 - accuracy: 0.8403 - val_loss: 0.4297 - val_accuracy: 0.8395 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4298 - accuracy: 0.8430 - val_loss: 0.4300 - val_accuracy: 0.8400 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.4330 - accuracy: 0.8430 - val_loss: 0.3999 - val_accuracy: 0.8511 - 7s/epoch - 5ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.4285 - accuracy: 0.8434 - val_loss: 0.4162 - val_accuracy: 0.8478 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.4318 - accuracy: 0.8437 - val_loss: 0.4223 - val_accuracy: 0.8456 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.4275 - accuracy: 0.8449 - val_loss: 0.4071 - val_accuracy: 0.8521 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.4279 - accuracy: 0.8424 - val_loss: 0.4289 - val_accuracy: 0.8397 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.4187 - accuracy: 0.8477 - val_loss: 0.4136 - val_accuracy: 0.8457 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.4166 - accuracy: 0.8496 - val_loss: 0.4102 - val_accuracy: 0.8507 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.4190 - accuracy: 0.8471 - val_loss: 0.4124 - val_accuracy: 0.8496 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.4196 - accuracy: 0.8480 - val_loss: 0.3994 - val_accuracy: 0.8545 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 2s - loss: 1.2109 - accuracy: 0.5756 - val_loss: 0.7377 - val_accuracy: 0.7215 - 2s/epoch - 64ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.6793 - accuracy: 0.7396 - val_loss: 0.5959 - val_accuracy: 0.7773 - 448ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.5783 - accuracy: 0.7819 - val_loss: 0.5360 - val_accuracy: 0.8108 - 462ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5228 - accuracy: 0.8076 - val_loss: 0.4822 - val_accuracy: 0.8247 - 448ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.4827 - accuracy: 0.8234 - val_loss: 0.4587 - val_accuracy: 0.8326 - 445ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4627 - accuracy: 0.8322 - val_loss: 0.4217 - val_accuracy: 0.8476 - 452ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4421 - accuracy: 0.8391 - val_loss: 0.4130 - val_accuracy: 0.8475 - 444ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4217 - accuracy: 0.8461 - val_loss: 0.3983 - val_accuracy: 0.8601 - 445ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4056 - accuracy: 0.8534 - val_loss: 0.3794 - val_accuracy: 0.8630 - 454ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.3909 - accuracy: 0.8599 - val_loss: 0.3619 - val_accuracy: 0.8693 - 458ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3782 - accuracy: 0.8638 - val_loss: 0.3584 - val_accuracy: 0.8698 - 455ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3670 - accuracy: 0.8671 - val_loss: 0.3526 - val_accuracy: 0.8735 - 441ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3575 - accuracy: 0.8699 - val_loss: 0.3396 - val_accuracy: 0.8766 - 458ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3515 - accuracy: 0.8725 - val_loss: 0.3296 - val_accuracy: 0.8819 - 447ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3383 - accuracy: 0.8782 - val_loss: 0.3290 - val_accuracy: 0.8792 - 459ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3342 - accuracy: 0.8772 - val_loss: 0.3243 - val_accuracy: 0.8798 - 451ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3256 - accuracy: 0.8827 - val_loss: 0.3161 - val_accuracy: 0.8858 - 458ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3249 - accuracy: 0.8826 - val_loss: 0.3169 - val_accuracy: 0.8829 - 455ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3169 - accuracy: 0.8840 - val_loss: 0.3163 - val_accuracy: 0.8823 - 456ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3120 - accuracy: 0.8863 - val_loss: 0.3119 - val_accuracy: 0.8857 - 452ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.0750 - accuracy: 0.6229 - val_loss: 0.6465 - val_accuracy: 0.7570 - 1s/epoch - 46ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.5896 - accuracy: 0.7815 - val_loss: 0.5381 - val_accuracy: 0.8077 - 461ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.5122 - accuracy: 0.8142 - val_loss: 0.4889 - val_accuracy: 0.8185 - 441ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.4714 - accuracy: 0.8293 - val_loss: 0.4554 - val_accuracy: 0.8361 - 454ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.4480 - accuracy: 0.8389 - val_loss: 0.4282 - val_accuracy: 0.8447 - 452ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4226 - accuracy: 0.8485 - val_loss: 0.4123 - val_accuracy: 0.8481 - 455ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4074 - accuracy: 0.8534 - val_loss: 0.4067 - val_accuracy: 0.8509 - 462ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.3956 - accuracy: 0.8585 - val_loss: 0.3934 - val_accuracy: 0.8572 - 459ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.3873 - accuracy: 0.8601 - val_loss: 0.3870 - val_accuracy: 0.8607 - 443ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.3754 - accuracy: 0.8649 - val_loss: 0.3801 - val_accuracy: 0.8619 - 467ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3708 - accuracy: 0.8666 - val_loss: 0.3753 - val_accuracy: 0.8627 - 445ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3650 - accuracy: 0.8672 - val_loss: 0.3688 - val_accuracy: 0.8647 - 447ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3539 - accuracy: 0.8722 - val_loss: 0.3633 - val_accuracy: 0.8683 - 479ms/epoch - 20ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3480 - accuracy: 0.8749 - val_loss: 0.3607 - val_accuracy: 0.8684 - 451ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3431 - accuracy: 0.8761 - val_loss: 0.3557 - val_accuracy: 0.8690 - 470ms/epoch - 20ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3403 - accuracy: 0.8765 - val_loss: 0.3552 - val_accuracy: 0.8706 - 471ms/epoch - 20ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3303 - accuracy: 0.8793 - val_loss: 0.3553 - val_accuracy: 0.8697 - 467ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3300 - accuracy: 0.8808 - val_loss: 0.3501 - val_accuracy: 0.8704 - 444ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3241 - accuracy: 0.8824 - val_loss: 0.3493 - val_accuracy: 0.8702 - 455ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3225 - accuracy: 0.8815 - val_loss: 0.3472 - val_accuracy: 0.8732 - 454ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.9514 - accuracy: 0.3476 - val_loss: 1.1918 - val_accuracy: 0.6029 - 1s/epoch - 176ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.0969 - accuracy: 0.6277 - val_loss: 0.9595 - val_accuracy: 0.6669 - 424ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9080 - accuracy: 0.6718 - val_loss: 0.7989 - val_accuracy: 0.7092 - 408ms/epoch - 68ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.7989 - accuracy: 0.7015 - val_loss: 0.7455 - val_accuracy: 0.7172 - 416ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7255 - accuracy: 0.7246 - val_loss: 0.6770 - val_accuracy: 0.7447 - 419ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.6763 - accuracy: 0.7420 - val_loss: 0.6429 - val_accuracy: 0.7519 - 420ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6379 - accuracy: 0.7563 - val_loss: 0.6041 - val_accuracy: 0.7715 - 430ms/epoch - 72ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6057 - accuracy: 0.7707 - val_loss: 0.5767 - val_accuracy: 0.7849 - 412ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.5767 - accuracy: 0.7815 - val_loss: 0.5495 - val_accuracy: 0.7955 - 428ms/epoch - 71ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.5527 - accuracy: 0.7924 - val_loss: 0.5273 - val_accuracy: 0.8061 - 429ms/epoch - 72ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.5350 - accuracy: 0.8005 - val_loss: 0.5085 - val_accuracy: 0.8129 - 424ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5181 - accuracy: 0.8078 - val_loss: 0.4954 - val_accuracy: 0.8194 - 413ms/epoch - 69ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5024 - accuracy: 0.8154 - val_loss: 0.4794 - val_accuracy: 0.8257 - 430ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.4913 - accuracy: 0.8181 - val_loss: 0.4655 - val_accuracy: 0.8296 - 424ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.4768 - accuracy: 0.8239 - val_loss: 0.4553 - val_accuracy: 0.8333 - 433ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.4664 - accuracy: 0.8284 - val_loss: 0.4462 - val_accuracy: 0.8367 - 437ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.4540 - accuracy: 0.8316 - val_loss: 0.4344 - val_accuracy: 0.8426 - 421ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.4428 - accuracy: 0.8390 - val_loss: 0.4238 - val_accuracy: 0.8449 - 425ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4344 - accuracy: 0.8409 - val_loss: 0.4159 - val_accuracy: 0.8497 - 431ms/epoch - 72ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4254 - accuracy: 0.8453 - val_loss: 0.4077 - val_accuracy: 0.8494 - 410ms/epoch - 68ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.6611 - accuracy: 0.4404 - val_loss: 0.9898 - val_accuracy: 0.6473 - 1s/epoch - 192ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 0.8888 - accuracy: 0.6704 - val_loss: 0.7632 - val_accuracy: 0.7112 - 432ms/epoch - 72ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.7295 - accuracy: 0.7258 - val_loss: 0.6741 - val_accuracy: 0.7497 - 431ms/epoch - 72ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.6572 - accuracy: 0.7530 - val_loss: 0.6260 - val_accuracy: 0.7601 - 432ms/epoch - 72ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.6098 - accuracy: 0.7705 - val_loss: 0.5879 - val_accuracy: 0.7839 - 415ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.5760 - accuracy: 0.7856 - val_loss: 0.5572 - val_accuracy: 0.7980 - 419ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.5473 - accuracy: 0.7987 - val_loss: 0.5323 - val_accuracy: 0.8042 - 425ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.5271 - accuracy: 0.8095 - val_loss: 0.5111 - val_accuracy: 0.8133 - 413ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.5075 - accuracy: 0.8164 - val_loss: 0.4941 - val_accuracy: 0.8204 - 435ms/epoch - 73ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.4908 - accuracy: 0.8230 - val_loss: 0.4805 - val_accuracy: 0.8260 - 429ms/epoch - 71ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.4787 - accuracy: 0.8271 - val_loss: 0.4685 - val_accuracy: 0.8306 - 425ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.4660 - accuracy: 0.8326 - val_loss: 0.4568 - val_accuracy: 0.8369 - 430ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.4547 - accuracy: 0.8367 - val_loss: 0.4504 - val_accuracy: 0.8355 - 419ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.4472 - accuracy: 0.8403 - val_loss: 0.4405 - val_accuracy: 0.8413 - 422ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.4375 - accuracy: 0.8431 - val_loss: 0.4301 - val_accuracy: 0.8445 - 416ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.4277 - accuracy: 0.8475 - val_loss: 0.4239 - val_accuracy: 0.8443 - 429ms/epoch - 72ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.4199 - accuracy: 0.8514 - val_loss: 0.4194 - val_accuracy: 0.8461 - 427ms/epoch - 71ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.4160 - accuracy: 0.8516 - val_loss: 0.4094 - val_accuracy: 0.8507 - 425ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4089 - accuracy: 0.8536 - val_loss: 0.4075 - val_accuracy: 0.8521 - 420ms/epoch - 70ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4020 - accuracy: 0.8570 - val_loss: 0.4010 - val_accuracy: 0.8569 - 432ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6379 - accuracy: 0.7610 - val_loss: 0.4319 - val_accuracy: 0.8383 - 7s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4336 - accuracy: 0.8413 - val_loss: 0.3705 - val_accuracy: 0.8641 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 5s - loss: 0.3881 - accuracy: 0.8618 - val_loss: 0.3596 - val_accuracy: 0.8667 - 5s/epoch - 3ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 5s - loss: 0.3648 - accuracy: 0.8675 - val_loss: 0.3377 - val_accuracy: 0.8750 - 5s/epoch - 3ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 5s - loss: 0.3459 - accuracy: 0.8735 - val_loss: 0.3140 - val_accuracy: 0.8838 - 5s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3306 - accuracy: 0.8810 - val_loss: 0.3171 - val_accuracy: 0.8837 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 5s - loss: 0.3209 - accuracy: 0.8830 - val_loss: 0.3073 - val_accuracy: 0.8868 - 5s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3168 - accuracy: 0.8842 - val_loss: 0.2983 - val_accuracy: 0.8907 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 5s - loss: 0.3045 - accuracy: 0.8896 - val_loss: 0.3176 - val_accuracy: 0.8828 - 5s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 5s - loss: 0.2966 - accuracy: 0.8928 - val_loss: 0.2900 - val_accuracy: 0.8954 - 5s/epoch - 3ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2949 - accuracy: 0.8938 - val_loss: 0.3003 - val_accuracy: 0.8888 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.2892 - accuracy: 0.8934 - val_loss: 0.2793 - val_accuracy: 0.8973 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 5s - loss: 0.2848 - accuracy: 0.8970 - val_loss: 0.2973 - val_accuracy: 0.8921 - 5s/epoch - 3ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 5s - loss: 0.2803 - accuracy: 0.8981 - val_loss: 0.3008 - val_accuracy: 0.8919 - 5s/epoch - 3ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2786 - accuracy: 0.8982 - val_loss: 0.2730 - val_accuracy: 0.9021 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2713 - accuracy: 0.9015 - val_loss: 0.2807 - val_accuracy: 0.8996 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.2702 - accuracy: 0.9016 - val_loss: 0.2805 - val_accuracy: 0.8993 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2667 - accuracy: 0.9036 - val_loss: 0.2821 - val_accuracy: 0.9004 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2649 - accuracy: 0.9035 - val_loss: 0.2762 - val_accuracy: 0.9000 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2627 - accuracy: 0.9057 - val_loss: 0.2932 - val_accuracy: 0.8937 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.5936 - accuracy: 0.7838 - val_loss: 0.4404 - val_accuracy: 0.8396 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4726 - accuracy: 0.8307 - val_loss: 0.4125 - val_accuracy: 0.8492 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4472 - accuracy: 0.8418 - val_loss: 0.4009 - val_accuracy: 0.8533 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 5s - loss: 0.4277 - accuracy: 0.8458 - val_loss: 0.3970 - val_accuracy: 0.8614 - 5s/epoch - 3ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 5s - loss: 0.4196 - accuracy: 0.8506 - val_loss: 0.3920 - val_accuracy: 0.8537 - 5s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.4098 - accuracy: 0.8523 - val_loss: 0.3745 - val_accuracy: 0.8642 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3995 - accuracy: 0.8563 - val_loss: 0.3671 - val_accuracy: 0.8681 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 5s - loss: 0.3973 - accuracy: 0.8585 - val_loss: 0.3609 - val_accuracy: 0.8695 - 5s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 5s - loss: 0.3941 - accuracy: 0.8587 - val_loss: 0.3771 - val_accuracy: 0.8616 - 5s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 5s - loss: 0.3911 - accuracy: 0.8593 - val_loss: 0.3631 - val_accuracy: 0.8671 - 5s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 5s - loss: 0.3846 - accuracy: 0.8632 - val_loss: 0.3781 - val_accuracy: 0.8646 - 5s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3804 - accuracy: 0.8650 - val_loss: 0.3514 - val_accuracy: 0.8722 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3783 - accuracy: 0.8645 - val_loss: 0.3474 - val_accuracy: 0.8735 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 5s - loss: 0.3711 - accuracy: 0.8681 - val_loss: 0.3616 - val_accuracy: 0.8697 - 5s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 5s - loss: 0.3700 - accuracy: 0.8670 - val_loss: 0.3687 - val_accuracy: 0.8696 - 5s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 5s - loss: 0.3738 - accuracy: 0.8671 - val_loss: 0.3625 - val_accuracy: 0.8698 - 5s/epoch - 3ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 5s - loss: 0.3644 - accuracy: 0.8702 - val_loss: 0.3541 - val_accuracy: 0.8721 - 5s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3700 - accuracy: 0.8680 - val_loss: 0.3649 - val_accuracy: 0.8720 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 5s - loss: 0.3694 - accuracy: 0.8687 - val_loss: 0.3513 - val_accuracy: 0.8731 - 5s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3595 - accuracy: 0.8719 - val_loss: 0.3532 - val_accuracy: 0.8723 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.4250 - accuracy: 0.4728 - val_loss: 0.7647 - val_accuracy: 0.7030 - 1s/epoch - 49ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7737 - accuracy: 0.7008 - val_loss: 0.6307 - val_accuracy: 0.7493 - 455ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6701 - accuracy: 0.7422 - val_loss: 0.5676 - val_accuracy: 0.7731 - 465ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6065 - accuracy: 0.7694 - val_loss: 0.5217 - val_accuracy: 0.8016 - 449ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5665 - accuracy: 0.7861 - val_loss: 0.4944 - val_accuracy: 0.8130 - 455ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5350 - accuracy: 0.7985 - val_loss: 0.4740 - val_accuracy: 0.8231 - 446ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5158 - accuracy: 0.8086 - val_loss: 0.4512 - val_accuracy: 0.8350 - 447ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4955 - accuracy: 0.8171 - val_loss: 0.4340 - val_accuracy: 0.8421 - 471ms/epoch - 20ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4782 - accuracy: 0.8241 - val_loss: 0.4270 - val_accuracy: 0.8382 - 465ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4679 - accuracy: 0.8282 - val_loss: 0.4167 - val_accuracy: 0.8471 - 450ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4562 - accuracy: 0.8346 - val_loss: 0.4012 - val_accuracy: 0.8476 - 454ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4433 - accuracy: 0.8386 - val_loss: 0.3902 - val_accuracy: 0.8562 - 443ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4289 - accuracy: 0.8432 - val_loss: 0.3752 - val_accuracy: 0.8602 - 456ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4199 - accuracy: 0.8469 - val_loss: 0.3612 - val_accuracy: 0.8624 - 452ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4066 - accuracy: 0.8540 - val_loss: 0.3605 - val_accuracy: 0.8640 - 434ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4041 - accuracy: 0.8544 - val_loss: 0.3629 - val_accuracy: 0.8637 - 446ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3948 - accuracy: 0.8575 - val_loss: 0.3475 - val_accuracy: 0.8692 - 445ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3871 - accuracy: 0.8591 - val_loss: 0.3426 - val_accuracy: 0.8730 - 469ms/epoch - 20ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3814 - accuracy: 0.8618 - val_loss: 0.3356 - val_accuracy: 0.8765 - 466ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3696 - accuracy: 0.8666 - val_loss: 0.3334 - val_accuracy: 0.8742 - 445ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.1327 - accuracy: 0.5986 - val_loss: 0.6553 - val_accuracy: 0.7452 - 1s/epoch - 45ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.6330 - accuracy: 0.7642 - val_loss: 0.5522 - val_accuracy: 0.7916 - 453ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.5574 - accuracy: 0.7914 - val_loss: 0.5061 - val_accuracy: 0.8138 - 472ms/epoch - 20ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5123 - accuracy: 0.8130 - val_loss: 0.4723 - val_accuracy: 0.8260 - 446ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.4838 - accuracy: 0.8249 - val_loss: 0.4441 - val_accuracy: 0.8361 - 464ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4615 - accuracy: 0.8362 - val_loss: 0.4380 - val_accuracy: 0.8393 - 452ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4528 - accuracy: 0.8358 - val_loss: 0.4261 - val_accuracy: 0.8436 - 456ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4346 - accuracy: 0.8450 - val_loss: 0.4069 - val_accuracy: 0.8481 - 445ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4231 - accuracy: 0.8473 - val_loss: 0.4014 - val_accuracy: 0.8502 - 455ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4143 - accuracy: 0.8506 - val_loss: 0.3958 - val_accuracy: 0.8541 - 447ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4088 - accuracy: 0.8533 - val_loss: 0.3873 - val_accuracy: 0.8575 - 462ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3961 - accuracy: 0.8582 - val_loss: 0.3826 - val_accuracy: 0.8581 - 443ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3910 - accuracy: 0.8591 - val_loss: 0.3736 - val_accuracy: 0.8648 - 448ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3904 - accuracy: 0.8602 - val_loss: 0.3788 - val_accuracy: 0.8595 - 448ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3831 - accuracy: 0.8616 - val_loss: 0.3723 - val_accuracy: 0.8623 - 468ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3764 - accuracy: 0.8647 - val_loss: 0.3671 - val_accuracy: 0.8636 - 453ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3690 - accuracy: 0.8660 - val_loss: 0.3588 - val_accuracy: 0.8697 - 463ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3622 - accuracy: 0.8697 - val_loss: 0.3532 - val_accuracy: 0.8693 - 465ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3654 - accuracy: 0.8687 - val_loss: 0.3560 - val_accuracy: 0.8657 - 442ms/epoch - 18ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3644 - accuracy: 0.8686 - val_loss: 0.3592 - val_accuracy: 0.8675 - 462ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0958 - accuracy: 0.2617 - val_loss: 1.4091 - val_accuracy: 0.5730 - 1s/epoch - 176ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.3150 - accuracy: 0.5119 - val_loss: 0.9194 - val_accuracy: 0.6600 - 421ms/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9697 - accuracy: 0.6337 - val_loss: 0.7622 - val_accuracy: 0.7221 - 424ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8246 - accuracy: 0.6891 - val_loss: 0.6953 - val_accuracy: 0.7374 - 422ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7494 - accuracy: 0.7162 - val_loss: 0.6493 - val_accuracy: 0.7478 - 420ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.6981 - accuracy: 0.7314 - val_loss: 0.6135 - val_accuracy: 0.7616 - 435ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6598 - accuracy: 0.7501 - val_loss: 0.5864 - val_accuracy: 0.7659 - 416ms/epoch - 69ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6347 - accuracy: 0.7589 - val_loss: 0.5647 - val_accuracy: 0.7789 - 419ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6077 - accuracy: 0.7699 - val_loss: 0.5460 - val_accuracy: 0.7930 - 438ms/epoch - 73ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.5886 - accuracy: 0.7769 - val_loss: 0.5305 - val_accuracy: 0.7961 - 425ms/epoch - 71ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.5733 - accuracy: 0.7833 - val_loss: 0.5222 - val_accuracy: 0.8030 - 421ms/epoch - 70ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5577 - accuracy: 0.7915 - val_loss: 0.5090 - val_accuracy: 0.8075 - 432ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5486 - accuracy: 0.7952 - val_loss: 0.4968 - val_accuracy: 0.8130 - 423ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5393 - accuracy: 0.8003 - val_loss: 0.4840 - val_accuracy: 0.8175 - 421ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5294 - accuracy: 0.8069 - val_loss: 0.4820 - val_accuracy: 0.8186 - 422ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5203 - accuracy: 0.8082 - val_loss: 0.4663 - val_accuracy: 0.8289 - 442ms/epoch - 74ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5112 - accuracy: 0.8121 - val_loss: 0.4617 - val_accuracy: 0.8280 - 439ms/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5007 - accuracy: 0.8172 - val_loss: 0.4510 - val_accuracy: 0.8336 - 422ms/epoch - 70ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4949 - accuracy: 0.8196 - val_loss: 0.4492 - val_accuracy: 0.8336 - 437ms/epoch - 73ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4928 - accuracy: 0.8192 - val_loss: 0.4394 - val_accuracy: 0.8381 - 419ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.7612 - accuracy: 0.3981 - val_loss: 1.0753 - val_accuracy: 0.6187 - 1s/epoch - 190ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.0394 - accuracy: 0.5969 - val_loss: 0.8695 - val_accuracy: 0.6759 - 433ms/epoch - 72ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.8774 - accuracy: 0.6651 - val_loss: 0.7471 - val_accuracy: 0.7181 - 438ms/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.7697 - accuracy: 0.7081 - val_loss: 0.6939 - val_accuracy: 0.7332 - 421ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7086 - accuracy: 0.7324 - val_loss: 0.6481 - val_accuracy: 0.7489 - 435ms/epoch - 72ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.6646 - accuracy: 0.7490 - val_loss: 0.6233 - val_accuracy: 0.7628 - 439ms/epoch - 73ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6294 - accuracy: 0.7643 - val_loss: 0.6019 - val_accuracy: 0.7767 - 441ms/epoch - 74ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6040 - accuracy: 0.7761 - val_loss: 0.5757 - val_accuracy: 0.7845 - 436ms/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.5787 - accuracy: 0.7839 - val_loss: 0.5510 - val_accuracy: 0.7964 - 448ms/epoch - 75ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.5612 - accuracy: 0.7936 - val_loss: 0.5348 - val_accuracy: 0.8018 - 426ms/epoch - 71ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.5468 - accuracy: 0.8000 - val_loss: 0.5304 - val_accuracy: 0.8010 - 425ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5366 - accuracy: 0.8052 - val_loss: 0.5120 - val_accuracy: 0.8115 - 442ms/epoch - 74ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5253 - accuracy: 0.8091 - val_loss: 0.5035 - val_accuracy: 0.8145 - 422ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5180 - accuracy: 0.8130 - val_loss: 0.4924 - val_accuracy: 0.8185 - 435ms/epoch - 73ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5116 - accuracy: 0.8133 - val_loss: 0.4833 - val_accuracy: 0.8214 - 436ms/epoch - 73ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.4979 - accuracy: 0.8194 - val_loss: 0.4728 - val_accuracy: 0.8257 - 437ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.4899 - accuracy: 0.8229 - val_loss: 0.4701 - val_accuracy: 0.8239 - 441ms/epoch - 74ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.4852 - accuracy: 0.8249 - val_loss: 0.4623 - val_accuracy: 0.8286 - 425ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4789 - accuracy: 0.8283 - val_loss: 0.4543 - val_accuracy: 0.8317 - 427ms/epoch - 71ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4779 - accuracy: 0.8266 - val_loss: 0.4488 - val_accuracy: 0.8333 - 429ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6945 - accuracy: 0.7402 - val_loss: 0.4827 - val_accuracy: 0.8167 - 7s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4894 - accuracy: 0.8186 - val_loss: 0.3978 - val_accuracy: 0.8511 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4329 - accuracy: 0.8394 - val_loss: 0.3611 - val_accuracy: 0.8660 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3988 - accuracy: 0.8525 - val_loss: 0.3422 - val_accuracy: 0.8700 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3774 - accuracy: 0.8601 - val_loss: 0.3344 - val_accuracy: 0.8776 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3600 - accuracy: 0.8665 - val_loss: 0.3059 - val_accuracy: 0.8844 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3459 - accuracy: 0.8715 - val_loss: 0.2982 - val_accuracy: 0.8879 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3333 - accuracy: 0.8772 - val_loss: 0.2917 - val_accuracy: 0.8923 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3244 - accuracy: 0.8798 - val_loss: 0.2899 - val_accuracy: 0.8904 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3186 - accuracy: 0.8839 - val_loss: 0.2776 - val_accuracy: 0.8971 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3100 - accuracy: 0.8848 - val_loss: 0.2697 - val_accuracy: 0.9006 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3025 - accuracy: 0.8884 - val_loss: 0.2793 - val_accuracy: 0.8956 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.2976 - accuracy: 0.8904 - val_loss: 0.2601 - val_accuracy: 0.9015 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.2936 - accuracy: 0.8918 - val_loss: 0.2643 - val_accuracy: 0.9013 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2893 - accuracy: 0.8934 - val_loss: 0.2594 - val_accuracy: 0.9066 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2871 - accuracy: 0.8944 - val_loss: 0.2540 - val_accuracy: 0.9057 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.2820 - accuracy: 0.8953 - val_loss: 0.2535 - val_accuracy: 0.9084 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2803 - accuracy: 0.8967 - val_loss: 0.2542 - val_accuracy: 0.9058 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2736 - accuracy: 0.8988 - val_loss: 0.2496 - val_accuracy: 0.9078 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2712 - accuracy: 0.9000 - val_loss: 0.2505 - val_accuracy: 0.9076 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6521 - accuracy: 0.7573 - val_loss: 0.4854 - val_accuracy: 0.8225 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.5069 - accuracy: 0.8130 - val_loss: 0.4476 - val_accuracy: 0.8329 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4706 - accuracy: 0.8273 - val_loss: 0.4257 - val_accuracy: 0.8435 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4506 - accuracy: 0.8350 - val_loss: 0.4202 - val_accuracy: 0.8436 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.4379 - accuracy: 0.8402 - val_loss: 0.3884 - val_accuracy: 0.8598 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.4298 - accuracy: 0.8433 - val_loss: 0.3871 - val_accuracy: 0.8576 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.4178 - accuracy: 0.8478 - val_loss: 0.3765 - val_accuracy: 0.8604 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.4099 - accuracy: 0.8489 - val_loss: 0.3791 - val_accuracy: 0.8596 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.4052 - accuracy: 0.8511 - val_loss: 0.3671 - val_accuracy: 0.8622 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3993 - accuracy: 0.8533 - val_loss: 0.3601 - val_accuracy: 0.8667 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3955 - accuracy: 0.8565 - val_loss: 0.3687 - val_accuracy: 0.8660 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3935 - accuracy: 0.8560 - val_loss: 0.3563 - val_accuracy: 0.8696 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3933 - accuracy: 0.8561 - val_loss: 0.3543 - val_accuracy: 0.8719 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.3898 - accuracy: 0.8578 - val_loss: 0.3537 - val_accuracy: 0.8694 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 8s - loss: 0.3869 - accuracy: 0.8591 - val_loss: 0.3548 - val_accuracy: 0.8689 - 8s/epoch - 5ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3854 - accuracy: 0.8596 - val_loss: 0.3564 - val_accuracy: 0.8703 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3846 - accuracy: 0.8595 - val_loss: 0.3536 - val_accuracy: 0.8698 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3807 - accuracy: 0.8613 - val_loss: 0.3509 - val_accuracy: 0.8677 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3805 - accuracy: 0.8597 - val_loss: 0.3494 - val_accuracy: 0.8687 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3775 - accuracy: 0.8633 - val_loss: 0.3401 - val_accuracy: 0.8727 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.7131 - accuracy: 0.4290 - val_loss: 0.8570 - val_accuracy: 0.6852 - 1s/epoch - 49ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8693 - accuracy: 0.6792 - val_loss: 0.7105 - val_accuracy: 0.7380 - 437ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7255 - accuracy: 0.7284 - val_loss: 0.6307 - val_accuracy: 0.7627 - 446ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6560 - accuracy: 0.7543 - val_loss: 0.5853 - val_accuracy: 0.7788 - 450ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6084 - accuracy: 0.7728 - val_loss: 0.5444 - val_accuracy: 0.7954 - 457ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5812 - accuracy: 0.7830 - val_loss: 0.5209 - val_accuracy: 0.8080 - 434ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5529 - accuracy: 0.7955 - val_loss: 0.4953 - val_accuracy: 0.8170 - 445ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5338 - accuracy: 0.8030 - val_loss: 0.4812 - val_accuracy: 0.8194 - 464ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5162 - accuracy: 0.8089 - val_loss: 0.4663 - val_accuracy: 0.8301 - 451ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5025 - accuracy: 0.8138 - val_loss: 0.4540 - val_accuracy: 0.8333 - 441ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4933 - accuracy: 0.8169 - val_loss: 0.4444 - val_accuracy: 0.8330 - 454ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4826 - accuracy: 0.8225 - val_loss: 0.4321 - val_accuracy: 0.8401 - 460ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4722 - accuracy: 0.8263 - val_loss: 0.4279 - val_accuracy: 0.8436 - 454ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4615 - accuracy: 0.8320 - val_loss: 0.4193 - val_accuracy: 0.8456 - 452ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4586 - accuracy: 0.8306 - val_loss: 0.4138 - val_accuracy: 0.8466 - 445ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4507 - accuracy: 0.8333 - val_loss: 0.4046 - val_accuracy: 0.8499 - 459ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4435 - accuracy: 0.8358 - val_loss: 0.3978 - val_accuracy: 0.8538 - 448ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4383 - accuracy: 0.8387 - val_loss: 0.3928 - val_accuracy: 0.8571 - 441ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4311 - accuracy: 0.8416 - val_loss: 0.3879 - val_accuracy: 0.8588 - 465ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4267 - accuracy: 0.8425 - val_loss: 0.3835 - val_accuracy: 0.8607 - 434ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.3771 - accuracy: 0.5242 - val_loss: 0.8200 - val_accuracy: 0.6926 - 1s/epoch - 48ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7684 - accuracy: 0.7128 - val_loss: 0.6518 - val_accuracy: 0.7579 - 442ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6580 - accuracy: 0.7538 - val_loss: 0.5951 - val_accuracy: 0.7775 - 464ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6088 - accuracy: 0.7766 - val_loss: 0.5560 - val_accuracy: 0.7931 - 445ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5758 - accuracy: 0.7873 - val_loss: 0.5277 - val_accuracy: 0.8090 - 462ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5528 - accuracy: 0.7970 - val_loss: 0.5070 - val_accuracy: 0.8135 - 444ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5333 - accuracy: 0.8056 - val_loss: 0.4928 - val_accuracy: 0.8208 - 449ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5171 - accuracy: 0.8106 - val_loss: 0.4800 - val_accuracy: 0.8235 - 454ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5035 - accuracy: 0.8169 - val_loss: 0.4689 - val_accuracy: 0.8291 - 458ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4915 - accuracy: 0.8219 - val_loss: 0.4561 - val_accuracy: 0.8349 - 452ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4802 - accuracy: 0.8261 - val_loss: 0.4482 - val_accuracy: 0.8376 - 461ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4719 - accuracy: 0.8289 - val_loss: 0.4401 - val_accuracy: 0.8397 - 455ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4629 - accuracy: 0.8321 - val_loss: 0.4377 - val_accuracy: 0.8397 - 452ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4574 - accuracy: 0.8353 - val_loss: 0.4219 - val_accuracy: 0.8444 - 446ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4462 - accuracy: 0.8378 - val_loss: 0.4187 - val_accuracy: 0.8459 - 452ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4411 - accuracy: 0.8401 - val_loss: 0.4097 - val_accuracy: 0.8507 - 450ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4321 - accuracy: 0.8444 - val_loss: 0.4065 - val_accuracy: 0.8503 - 456ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4312 - accuracy: 0.8438 - val_loss: 0.3999 - val_accuracy: 0.8539 - 442ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4224 - accuracy: 0.8477 - val_loss: 0.3975 - val_accuracy: 0.8529 - 444ms/epoch - 18ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4169 - accuracy: 0.8493 - val_loss: 0.3919 - val_accuracy: 0.8558 - 449ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.2237 - accuracy: 0.2324 - val_loss: 1.9780 - val_accuracy: 0.4710 - 1s/epoch - 179ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.7901 - accuracy: 0.4434 - val_loss: 1.3019 - val_accuracy: 0.6387 - 428ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.2496 - accuracy: 0.5550 - val_loss: 0.9017 - val_accuracy: 0.6964 - 425ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.9684 - accuracy: 0.6411 - val_loss: 0.7697 - val_accuracy: 0.7232 - 437ms/epoch - 73ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8354 - accuracy: 0.6904 - val_loss: 0.7261 - val_accuracy: 0.7312 - 410ms/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7708 - accuracy: 0.7130 - val_loss: 0.6879 - val_accuracy: 0.7397 - 415ms/epoch - 69ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7249 - accuracy: 0.7281 - val_loss: 0.6556 - val_accuracy: 0.7474 - 431ms/epoch - 72ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6901 - accuracy: 0.7407 - val_loss: 0.6264 - val_accuracy: 0.7580 - 428ms/epoch - 71ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6637 - accuracy: 0.7512 - val_loss: 0.6040 - val_accuracy: 0.7719 - 430ms/epoch - 72ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6400 - accuracy: 0.7610 - val_loss: 0.5872 - val_accuracy: 0.7794 - 422ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6251 - accuracy: 0.7672 - val_loss: 0.5705 - val_accuracy: 0.7854 - 437ms/epoch - 73ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6081 - accuracy: 0.7734 - val_loss: 0.5572 - val_accuracy: 0.7919 - 412ms/epoch - 69ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5951 - accuracy: 0.7776 - val_loss: 0.5454 - val_accuracy: 0.7956 - 419ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5802 - accuracy: 0.7832 - val_loss: 0.5359 - val_accuracy: 0.8018 - 433ms/epoch - 72ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5724 - accuracy: 0.7857 - val_loss: 0.5261 - val_accuracy: 0.8017 - 430ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5613 - accuracy: 0.7912 - val_loss: 0.5181 - val_accuracy: 0.8060 - 424ms/epoch - 71ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5519 - accuracy: 0.7946 - val_loss: 0.5076 - val_accuracy: 0.8103 - 415ms/epoch - 69ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5444 - accuracy: 0.7976 - val_loss: 0.5008 - val_accuracy: 0.8136 - 423ms/epoch - 70ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5401 - accuracy: 0.8011 - val_loss: 0.4941 - val_accuracy: 0.8174 - 420ms/epoch - 70ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5298 - accuracy: 0.8039 - val_loss: 0.4869 - val_accuracy: 0.8191 - 435ms/epoch - 73ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.9897 - accuracy: 0.2938 - val_loss: 1.4682 - val_accuracy: 0.5330 - 1s/epoch - 187ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.3248 - accuracy: 0.5388 - val_loss: 1.0328 - val_accuracy: 0.6479 - 426ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.0251 - accuracy: 0.6194 - val_loss: 0.8770 - val_accuracy: 0.6900 - 423ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8953 - accuracy: 0.6665 - val_loss: 0.7893 - val_accuracy: 0.7167 - 419ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8137 - accuracy: 0.6970 - val_loss: 0.7267 - val_accuracy: 0.7357 - 439ms/epoch - 73ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7558 - accuracy: 0.7189 - val_loss: 0.6825 - val_accuracy: 0.7457 - 423ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7109 - accuracy: 0.7357 - val_loss: 0.6601 - val_accuracy: 0.7554 - 428ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6812 - accuracy: 0.7472 - val_loss: 0.6406 - val_accuracy: 0.7638 - 420ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6607 - accuracy: 0.7566 - val_loss: 0.6224 - val_accuracy: 0.7728 - 413ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6425 - accuracy: 0.7625 - val_loss: 0.6064 - val_accuracy: 0.7787 - 435ms/epoch - 72ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6285 - accuracy: 0.7692 - val_loss: 0.5957 - val_accuracy: 0.7823 - 419ms/epoch - 70ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6147 - accuracy: 0.7726 - val_loss: 0.5828 - val_accuracy: 0.7876 - 433ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6028 - accuracy: 0.7795 - val_loss: 0.5767 - val_accuracy: 0.7883 - 415ms/epoch - 69ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5952 - accuracy: 0.7816 - val_loss: 0.5627 - val_accuracy: 0.7941 - 428ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5888 - accuracy: 0.7854 - val_loss: 0.5537 - val_accuracy: 0.7981 - 419ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5761 - accuracy: 0.7887 - val_loss: 0.5459 - val_accuracy: 0.8009 - 432ms/epoch - 72ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5668 - accuracy: 0.7934 - val_loss: 0.5380 - val_accuracy: 0.8047 - 434ms/epoch - 72ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5609 - accuracy: 0.7964 - val_loss: 0.5303 - val_accuracy: 0.8062 - 422ms/epoch - 70ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5559 - accuracy: 0.7972 - val_loss: 0.5245 - val_accuracy: 0.8054 - 419ms/epoch - 70ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5474 - accuracy: 0.7995 - val_loss: 0.5167 - val_accuracy: 0.8090 - 417ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.9967 - accuracy: 0.6120 - val_loss: 0.5914 - val_accuracy: 0.7708 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.7100 - accuracy: 0.7278 - val_loss: 0.5353 - val_accuracy: 0.8047 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.6307 - accuracy: 0.7593 - val_loss: 0.5254 - val_accuracy: 0.7834 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5840 - accuracy: 0.7808 - val_loss: 0.4811 - val_accuracy: 0.8161 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 5s - loss: 0.5480 - accuracy: 0.7986 - val_loss: 0.4610 - val_accuracy: 0.8258 - 5s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 5s - loss: 0.5223 - accuracy: 0.8096 - val_loss: 0.5051 - val_accuracy: 0.7935 - 5s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.4982 - accuracy: 0.8184 - val_loss: 0.4929 - val_accuracy: 0.7999 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 5s - loss: 0.4802 - accuracy: 0.8250 - val_loss: 0.5152 - val_accuracy: 0.7951 - 5s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.4661 - accuracy: 0.8316 - val_loss: 0.4907 - val_accuracy: 0.7957 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4480 - accuracy: 0.8371 - val_loss: 0.5211 - val_accuracy: 0.7916 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4371 - accuracy: 0.8406 - val_loss: 0.5082 - val_accuracy: 0.7954 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4252 - accuracy: 0.8462 - val_loss: 0.4884 - val_accuracy: 0.8077 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 5s - loss: 0.4197 - accuracy: 0.8487 - val_loss: 0.5069 - val_accuracy: 0.8032 - 5s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 5s - loss: 0.4091 - accuracy: 0.8523 - val_loss: 0.5703 - val_accuracy: 0.7590 - 5s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 5s - loss: 0.4046 - accuracy: 0.8538 - val_loss: 0.6011 - val_accuracy: 0.7633 - 5s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3991 - accuracy: 0.8561 - val_loss: 0.5513 - val_accuracy: 0.7976 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3961 - accuracy: 0.8565 - val_loss: 0.5941 - val_accuracy: 0.7738 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3905 - accuracy: 0.8606 - val_loss: 0.5469 - val_accuracy: 0.7925 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 5s - loss: 0.3833 - accuracy: 0.8618 - val_loss: 0.6250 - val_accuracy: 0.7627 - 5s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3836 - accuracy: 0.8644 - val_loss: 0.6029 - val_accuracy: 0.7576 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.7720 - accuracy: 0.7135 - val_loss: 0.5464 - val_accuracy: 0.7984 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.6080 - accuracy: 0.7794 - val_loss: 0.4898 - val_accuracy: 0.8174 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.5673 - accuracy: 0.7980 - val_loss: 0.4645 - val_accuracy: 0.8284 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5354 - accuracy: 0.8093 - val_loss: 0.4633 - val_accuracy: 0.8330 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 5s - loss: 0.5174 - accuracy: 0.8184 - val_loss: 0.4435 - val_accuracy: 0.8349 - 5s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 5s - loss: 0.5007 - accuracy: 0.8238 - val_loss: 0.4151 - val_accuracy: 0.8510 - 5s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 5s - loss: 0.4886 - accuracy: 0.8276 - val_loss: 0.4016 - val_accuracy: 0.8559 - 5s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 5s - loss: 0.4736 - accuracy: 0.8336 - val_loss: 0.4003 - val_accuracy: 0.8576 - 5s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 5s - loss: 0.4661 - accuracy: 0.8379 - val_loss: 0.4045 - val_accuracy: 0.8527 - 5s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4544 - accuracy: 0.8416 - val_loss: 0.3948 - val_accuracy: 0.8593 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 5s - loss: 0.4446 - accuracy: 0.8437 - val_loss: 0.3924 - val_accuracy: 0.8611 - 5s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4349 - accuracy: 0.8474 - val_loss: 0.3848 - val_accuracy: 0.8671 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 5s - loss: 0.4329 - accuracy: 0.8488 - val_loss: 0.3836 - val_accuracy: 0.8652 - 5s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 5s - loss: 0.4215 - accuracy: 0.8534 - val_loss: 0.3752 - val_accuracy: 0.8699 - 5s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.4156 - accuracy: 0.8547 - val_loss: 0.3759 - val_accuracy: 0.8690 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.4106 - accuracy: 0.8565 - val_loss: 0.3613 - val_accuracy: 0.8714 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.4030 - accuracy: 0.8608 - val_loss: 0.3637 - val_accuracy: 0.8753 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 5s - loss: 0.3991 - accuracy: 0.8593 - val_loss: 0.3633 - val_accuracy: 0.8698 - 5s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3998 - accuracy: 0.8615 - val_loss: 0.3520 - val_accuracy: 0.8749 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 5s - loss: 0.3951 - accuracy: 0.8617 - val_loss: 0.3559 - val_accuracy: 0.8773 - 5s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 2.0477 - accuracy: 0.2442 - val_loss: 1.3777 - val_accuracy: 0.5086 - 1s/epoch - 49ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 1.3224 - accuracy: 0.4918 - val_loss: 0.8657 - val_accuracy: 0.6882 - 498ms/epoch - 21ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 1.0308 - accuracy: 0.6011 - val_loss: 0.7391 - val_accuracy: 0.7220 - 456ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.9340 - accuracy: 0.6350 - val_loss: 0.6866 - val_accuracy: 0.7364 - 455ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.8708 - accuracy: 0.6628 - val_loss: 0.6513 - val_accuracy: 0.7430 - 463ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.8285 - accuracy: 0.6780 - val_loss: 0.6219 - val_accuracy: 0.7568 - 440ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.7980 - accuracy: 0.6892 - val_loss: 0.6263 - val_accuracy: 0.7491 - 451ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.7691 - accuracy: 0.7022 - val_loss: 0.5932 - val_accuracy: 0.7657 - 457ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.7481 - accuracy: 0.7126 - val_loss: 0.5848 - val_accuracy: 0.7732 - 460ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.7361 - accuracy: 0.7161 - val_loss: 0.5705 - val_accuracy: 0.7747 - 452ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.7192 - accuracy: 0.7229 - val_loss: 0.5577 - val_accuracy: 0.7851 - 469ms/epoch - 20ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.7096 - accuracy: 0.7291 - val_loss: 0.5605 - val_accuracy: 0.7904 - 442ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.6944 - accuracy: 0.7352 - val_loss: 0.5566 - val_accuracy: 0.7878 - 476ms/epoch - 20ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.6816 - accuracy: 0.7390 - val_loss: 0.5549 - val_accuracy: 0.7921 - 449ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.6721 - accuracy: 0.7422 - val_loss: 0.5426 - val_accuracy: 0.8039 - 454ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.6618 - accuracy: 0.7489 - val_loss: 0.5279 - val_accuracy: 0.7989 - 446ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.6550 - accuracy: 0.7501 - val_loss: 0.5385 - val_accuracy: 0.8022 - 447ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.6476 - accuracy: 0.7545 - val_loss: 0.5329 - val_accuracy: 0.8084 - 461ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.6350 - accuracy: 0.7596 - val_loss: 0.5259 - val_accuracy: 0.8133 - 445ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.6295 - accuracy: 0.7599 - val_loss: 0.5284 - val_accuracy: 0.8127 - 462ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.5942 - accuracy: 0.4482 - val_loss: 0.9418 - val_accuracy: 0.6464 - 1s/epoch - 45ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.9782 - accuracy: 0.6264 - val_loss: 0.7525 - val_accuracy: 0.7129 - 438ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.8146 - accuracy: 0.6996 - val_loss: 0.6654 - val_accuracy: 0.7475 - 461ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.7416 - accuracy: 0.7260 - val_loss: 0.6243 - val_accuracy: 0.7628 - 441ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6951 - accuracy: 0.7435 - val_loss: 0.5936 - val_accuracy: 0.7770 - 467ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6557 - accuracy: 0.7601 - val_loss: 0.5676 - val_accuracy: 0.7875 - 438ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.6341 - accuracy: 0.7695 - val_loss: 0.5494 - val_accuracy: 0.7914 - 442ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.6134 - accuracy: 0.7788 - val_loss: 0.5319 - val_accuracy: 0.8052 - 445ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5907 - accuracy: 0.7879 - val_loss: 0.5160 - val_accuracy: 0.8125 - 477ms/epoch - 20ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5793 - accuracy: 0.7922 - val_loss: 0.5071 - val_accuracy: 0.8170 - 467ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5678 - accuracy: 0.7991 - val_loss: 0.4955 - val_accuracy: 0.8233 - 445ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5595 - accuracy: 0.8010 - val_loss: 0.4904 - val_accuracy: 0.8240 - 456ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.5513 - accuracy: 0.8043 - val_loss: 0.4790 - val_accuracy: 0.8251 - 440ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.5392 - accuracy: 0.8092 - val_loss: 0.4720 - val_accuracy: 0.8274 - 463ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.5298 - accuracy: 0.8116 - val_loss: 0.4633 - val_accuracy: 0.8331 - 443ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.5215 - accuracy: 0.8155 - val_loss: 0.4577 - val_accuracy: 0.8351 - 455ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.5141 - accuracy: 0.8210 - val_loss: 0.4539 - val_accuracy: 0.8369 - 442ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.5110 - accuracy: 0.8207 - val_loss: 0.4550 - val_accuracy: 0.8371 - 460ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.5049 - accuracy: 0.8228 - val_loss: 0.4439 - val_accuracy: 0.8393 - 444ms/epoch - 18ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4981 - accuracy: 0.8256 - val_loss: 0.4359 - val_accuracy: 0.8435 - 439ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.2741 - accuracy: 0.1755 - val_loss: 2.1943 - val_accuracy: 0.4148 - 1s/epoch - 182ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 2.1295 - accuracy: 0.2797 - val_loss: 1.8827 - val_accuracy: 0.5494 - 438ms/epoch - 73ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.8563 - accuracy: 0.3508 - val_loss: 1.4832 - val_accuracy: 0.5488 - 418ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.5623 - accuracy: 0.4210 - val_loss: 1.1586 - val_accuracy: 0.6036 - 417ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.3568 - accuracy: 0.4786 - val_loss: 0.9981 - val_accuracy: 0.6378 - 421ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 1.2199 - accuracy: 0.5302 - val_loss: 0.9001 - val_accuracy: 0.6603 - 435ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 1.1240 - accuracy: 0.5687 - val_loss: 0.8338 - val_accuracy: 0.6768 - 423ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 1.0568 - accuracy: 0.5933 - val_loss: 0.7919 - val_accuracy: 0.6940 - 426ms/epoch - 71ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 1.0007 - accuracy: 0.6137 - val_loss: 0.7527 - val_accuracy: 0.7030 - 426ms/epoch - 71ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.9643 - accuracy: 0.6270 - val_loss: 0.7206 - val_accuracy: 0.7190 - 434ms/epoch - 72ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.9360 - accuracy: 0.6404 - val_loss: 0.6989 - val_accuracy: 0.7273 - 441ms/epoch - 73ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.9046 - accuracy: 0.6528 - val_loss: 0.6809 - val_accuracy: 0.7370 - 425ms/epoch - 71ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.8782 - accuracy: 0.6634 - val_loss: 0.6648 - val_accuracy: 0.7352 - 435ms/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.8619 - accuracy: 0.6703 - val_loss: 0.6515 - val_accuracy: 0.7453 - 413ms/epoch - 69ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.8452 - accuracy: 0.6766 - val_loss: 0.6370 - val_accuracy: 0.7524 - 432ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.8342 - accuracy: 0.6809 - val_loss: 0.6304 - val_accuracy: 0.7532 - 439ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.8173 - accuracy: 0.6890 - val_loss: 0.6232 - val_accuracy: 0.7617 - 436ms/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.8106 - accuracy: 0.6914 - val_loss: 0.6148 - val_accuracy: 0.7604 - 438ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.7930 - accuracy: 0.6949 - val_loss: 0.6046 - val_accuracy: 0.7653 - 414ms/epoch - 69ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.7853 - accuracy: 0.6996 - val_loss: 0.5984 - val_accuracy: 0.7644 - 412ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1256 - accuracy: 0.2688 - val_loss: 1.7656 - val_accuracy: 0.5484 - 1s/epoch - 193ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.6231 - accuracy: 0.4707 - val_loss: 1.2402 - val_accuracy: 0.6122 - 430ms/epoch - 72ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.2984 - accuracy: 0.5178 - val_loss: 1.0199 - val_accuracy: 0.6363 - 422ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.1461 - accuracy: 0.5569 - val_loss: 0.9182 - val_accuracy: 0.6545 - 421ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.0370 - accuracy: 0.6002 - val_loss: 0.8437 - val_accuracy: 0.6820 - 439ms/epoch - 73ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.9583 - accuracy: 0.6327 - val_loss: 0.7867 - val_accuracy: 0.7044 - 424ms/epoch - 71ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8959 - accuracy: 0.6598 - val_loss: 0.7446 - val_accuracy: 0.7150 - 453ms/epoch - 75ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.8474 - accuracy: 0.6834 - val_loss: 0.7112 - val_accuracy: 0.7307 - 422ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.8127 - accuracy: 0.6955 - val_loss: 0.6890 - val_accuracy: 0.7345 - 443ms/epoch - 74ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7873 - accuracy: 0.7083 - val_loss: 0.6675 - val_accuracy: 0.7440 - 439ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.7609 - accuracy: 0.7172 - val_loss: 0.6509 - val_accuracy: 0.7500 - 446ms/epoch - 74ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7381 - accuracy: 0.7268 - val_loss: 0.6381 - val_accuracy: 0.7561 - 420ms/epoch - 70ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.7199 - accuracy: 0.7328 - val_loss: 0.6190 - val_accuracy: 0.7670 - 424ms/epoch - 71ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.7048 - accuracy: 0.7421 - val_loss: 0.6043 - val_accuracy: 0.7722 - 419ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6887 - accuracy: 0.7476 - val_loss: 0.5945 - val_accuracy: 0.7779 - 436ms/epoch - 73ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6734 - accuracy: 0.7534 - val_loss: 0.5847 - val_accuracy: 0.7813 - 438ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.6630 - accuracy: 0.7603 - val_loss: 0.5766 - val_accuracy: 0.7862 - 423ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6489 - accuracy: 0.7648 - val_loss: 0.5696 - val_accuracy: 0.7894 - 425ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6415 - accuracy: 0.7667 - val_loss: 0.5607 - val_accuracy: 0.7885 - 427ms/epoch - 71ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.6346 - accuracy: 0.7709 - val_loss: 0.5560 - val_accuracy: 0.7918 - 426ms/epoch - 71ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6555 - accuracy: 0.7514 - val_loss: 0.4445 - val_accuracy: 0.8330 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4870 - accuracy: 0.8188 - val_loss: 0.4131 - val_accuracy: 0.8467 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 7s - loss: 0.4456 - accuracy: 0.8347 - val_loss: 0.3724 - val_accuracy: 0.8632 - 7s/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4222 - accuracy: 0.8426 - val_loss: 0.3807 - val_accuracy: 0.8537 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.4072 - accuracy: 0.8486 - val_loss: 0.3627 - val_accuracy: 0.8686 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3930 - accuracy: 0.8541 - val_loss: 0.3187 - val_accuracy: 0.8776 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3794 - accuracy: 0.8589 - val_loss: 0.3287 - val_accuracy: 0.8797 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3716 - accuracy: 0.8613 - val_loss: 0.3268 - val_accuracy: 0.8758 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3646 - accuracy: 0.8633 - val_loss: 0.3195 - val_accuracy: 0.8859 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3583 - accuracy: 0.8686 - val_loss: 0.2929 - val_accuracy: 0.8887 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3559 - accuracy: 0.8668 - val_loss: 0.3001 - val_accuracy: 0.8879 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3500 - accuracy: 0.8701 - val_loss: 0.2921 - val_accuracy: 0.8916 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3431 - accuracy: 0.8713 - val_loss: 0.2870 - val_accuracy: 0.8936 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.3387 - accuracy: 0.8747 - val_loss: 0.2809 - val_accuracy: 0.8974 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3379 - accuracy: 0.8747 - val_loss: 0.2837 - val_accuracy: 0.8954 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3326 - accuracy: 0.8780 - val_loss: 0.2779 - val_accuracy: 0.9000 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3314 - accuracy: 0.8778 - val_loss: 0.2897 - val_accuracy: 0.8971 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3284 - accuracy: 0.8807 - val_loss: 0.2820 - val_accuracy: 0.8979 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3268 - accuracy: 0.8767 - val_loss: 0.2741 - val_accuracy: 0.8980 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3213 - accuracy: 0.8830 - val_loss: 0.2738 - val_accuracy: 0.8983 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6301 - accuracy: 0.7668 - val_loss: 0.5116 - val_accuracy: 0.8108 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.5564 - accuracy: 0.7968 - val_loss: 0.4793 - val_accuracy: 0.8240 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.5405 - accuracy: 0.8013 - val_loss: 0.4810 - val_accuracy: 0.8209 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5377 - accuracy: 0.8028 - val_loss: 0.4778 - val_accuracy: 0.8250 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.5382 - accuracy: 0.8017 - val_loss: 0.4764 - val_accuracy: 0.8260 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.5288 - accuracy: 0.8068 - val_loss: 0.4612 - val_accuracy: 0.8342 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.5239 - accuracy: 0.8082 - val_loss: 0.4595 - val_accuracy: 0.8313 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.5214 - accuracy: 0.8103 - val_loss: 0.4663 - val_accuracy: 0.8328 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.5166 - accuracy: 0.8108 - val_loss: 0.4412 - val_accuracy: 0.8383 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.5105 - accuracy: 0.8124 - val_loss: 0.4473 - val_accuracy: 0.8376 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 7s - loss: 0.5026 - accuracy: 0.8159 - val_loss: 0.4507 - val_accuracy: 0.8318 - 7s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4999 - accuracy: 0.8170 - val_loss: 0.4336 - val_accuracy: 0.8419 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.5011 - accuracy: 0.8150 - val_loss: 0.4550 - val_accuracy: 0.8317 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.5060 - accuracy: 0.8151 - val_loss: 0.4422 - val_accuracy: 0.8369 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.5110 - accuracy: 0.8150 - val_loss: 0.4393 - val_accuracy: 0.8381 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.5136 - accuracy: 0.8123 - val_loss: 0.4406 - val_accuracy: 0.8386 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.5055 - accuracy: 0.8156 - val_loss: 0.4442 - val_accuracy: 0.8340 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.5071 - accuracy: 0.8164 - val_loss: 0.4511 - val_accuracy: 0.8385 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.4987 - accuracy: 0.8180 - val_loss: 0.4364 - val_accuracy: 0.8426 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.4967 - accuracy: 0.8172 - val_loss: 0.4385 - val_accuracy: 0.8382 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.2330 - accuracy: 0.5584 - val_loss: 0.7267 - val_accuracy: 0.7155 - 1s/epoch - 48ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7067 - accuracy: 0.7308 - val_loss: 0.5928 - val_accuracy: 0.7693 - 460ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6173 - accuracy: 0.7667 - val_loss: 0.5361 - val_accuracy: 0.8045 - 453ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5723 - accuracy: 0.7864 - val_loss: 0.5040 - val_accuracy: 0.8082 - 459ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5398 - accuracy: 0.7982 - val_loss: 0.4768 - val_accuracy: 0.8208 - 447ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5130 - accuracy: 0.8103 - val_loss: 0.4459 - val_accuracy: 0.8361 - 452ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4949 - accuracy: 0.8144 - val_loss: 0.4308 - val_accuracy: 0.8396 - 450ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4788 - accuracy: 0.8227 - val_loss: 0.4177 - val_accuracy: 0.8478 - 447ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4619 - accuracy: 0.8278 - val_loss: 0.4018 - val_accuracy: 0.8517 - 444ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4462 - accuracy: 0.8364 - val_loss: 0.3891 - val_accuracy: 0.8552 - 451ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4333 - accuracy: 0.8400 - val_loss: 0.3775 - val_accuracy: 0.8618 - 452ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4230 - accuracy: 0.8456 - val_loss: 0.3680 - val_accuracy: 0.8662 - 457ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4151 - accuracy: 0.8482 - val_loss: 0.3540 - val_accuracy: 0.8695 - 438ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4055 - accuracy: 0.8512 - val_loss: 0.3501 - val_accuracy: 0.8705 - 448ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3969 - accuracy: 0.8537 - val_loss: 0.3435 - val_accuracy: 0.8732 - 446ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3896 - accuracy: 0.8551 - val_loss: 0.3477 - val_accuracy: 0.8727 - 456ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3852 - accuracy: 0.8588 - val_loss: 0.3345 - val_accuracy: 0.8777 - 438ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3783 - accuracy: 0.8626 - val_loss: 0.3266 - val_accuracy: 0.8788 - 471ms/epoch - 20ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3654 - accuracy: 0.8661 - val_loss: 0.3238 - val_accuracy: 0.8837 - 448ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3629 - accuracy: 0.8665 - val_loss: 0.3099 - val_accuracy: 0.8859 - 456ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.0589 - accuracy: 0.6107 - val_loss: 0.6720 - val_accuracy: 0.7487 - 1s/epoch - 45ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.6436 - accuracy: 0.7563 - val_loss: 0.5662 - val_accuracy: 0.7857 - 449ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.5710 - accuracy: 0.7887 - val_loss: 0.5151 - val_accuracy: 0.8087 - 460ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5320 - accuracy: 0.8039 - val_loss: 0.4803 - val_accuracy: 0.8235 - 448ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.4998 - accuracy: 0.8182 - val_loss: 0.4623 - val_accuracy: 0.8276 - 456ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4786 - accuracy: 0.8252 - val_loss: 0.4375 - val_accuracy: 0.8403 - 454ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4621 - accuracy: 0.8325 - val_loss: 0.4208 - val_accuracy: 0.8481 - 456ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4501 - accuracy: 0.8376 - val_loss: 0.4110 - val_accuracy: 0.8502 - 458ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4386 - accuracy: 0.8405 - val_loss: 0.3986 - val_accuracy: 0.8522 - 462ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4256 - accuracy: 0.8457 - val_loss: 0.3916 - val_accuracy: 0.8547 - 465ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4171 - accuracy: 0.8486 - val_loss: 0.3830 - val_accuracy: 0.8605 - 459ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4134 - accuracy: 0.8497 - val_loss: 0.3892 - val_accuracy: 0.8551 - 449ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4097 - accuracy: 0.8518 - val_loss: 0.3776 - val_accuracy: 0.8596 - 445ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4026 - accuracy: 0.8537 - val_loss: 0.3688 - val_accuracy: 0.8641 - 459ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3941 - accuracy: 0.8574 - val_loss: 0.3660 - val_accuracy: 0.8665 - 456ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3885 - accuracy: 0.8591 - val_loss: 0.3617 - val_accuracy: 0.8665 - 447ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3809 - accuracy: 0.8623 - val_loss: 0.3584 - val_accuracy: 0.8656 - 453ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3804 - accuracy: 0.8617 - val_loss: 0.3537 - val_accuracy: 0.8671 - 444ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3774 - accuracy: 0.8616 - val_loss: 0.3551 - val_accuracy: 0.8672 - 450ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3771 - accuracy: 0.8643 - val_loss: 0.3508 - val_accuracy: 0.8699 - 456ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0624 - accuracy: 0.3022 - val_loss: 1.3409 - val_accuracy: 0.6473 - 1s/epoch - 179ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.2045 - accuracy: 0.5746 - val_loss: 0.9718 - val_accuracy: 0.6234 - 415ms/epoch - 69ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9565 - accuracy: 0.6432 - val_loss: 0.8434 - val_accuracy: 0.6813 - 440ms/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8453 - accuracy: 0.6866 - val_loss: 0.7279 - val_accuracy: 0.7268 - 414ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7642 - accuracy: 0.7096 - val_loss: 0.6781 - val_accuracy: 0.7359 - 413ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7098 - accuracy: 0.7321 - val_loss: 0.6397 - val_accuracy: 0.7537 - 421ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6681 - accuracy: 0.7482 - val_loss: 0.6015 - val_accuracy: 0.7722 - 418ms/epoch - 70ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6387 - accuracy: 0.7612 - val_loss: 0.5723 - val_accuracy: 0.7880 - 419ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6123 - accuracy: 0.7711 - val_loss: 0.5575 - val_accuracy: 0.7957 - 420ms/epoch - 70ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.5907 - accuracy: 0.7790 - val_loss: 0.5339 - val_accuracy: 0.8023 - 436ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.5748 - accuracy: 0.7860 - val_loss: 0.5178 - val_accuracy: 0.8085 - 434ms/epoch - 72ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5578 - accuracy: 0.7947 - val_loss: 0.4992 - val_accuracy: 0.8174 - 432ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5409 - accuracy: 0.8003 - val_loss: 0.4883 - val_accuracy: 0.8210 - 427ms/epoch - 71ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5275 - accuracy: 0.8050 - val_loss: 0.4775 - val_accuracy: 0.8220 - 424ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5180 - accuracy: 0.8098 - val_loss: 0.4653 - val_accuracy: 0.8259 - 420ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5087 - accuracy: 0.8134 - val_loss: 0.4597 - val_accuracy: 0.8274 - 414ms/epoch - 69ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.4991 - accuracy: 0.8150 - val_loss: 0.4476 - val_accuracy: 0.8312 - 430ms/epoch - 72ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.4902 - accuracy: 0.8189 - val_loss: 0.4416 - val_accuracy: 0.8375 - 426ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4799 - accuracy: 0.8219 - val_loss: 0.4325 - val_accuracy: 0.8374 - 433ms/epoch - 72ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4725 - accuracy: 0.8279 - val_loss: 0.4277 - val_accuracy: 0.8405 - 427ms/epoch - 71ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8052 - accuracy: 0.3835 - val_loss: 1.0941 - val_accuracy: 0.6122 - 1s/epoch - 184ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 0.9801 - accuracy: 0.6297 - val_loss: 0.8138 - val_accuracy: 0.7020 - 438ms/epoch - 73ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.8060 - accuracy: 0.6972 - val_loss: 0.7164 - val_accuracy: 0.7379 - 435ms/epoch - 72ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.7200 - accuracy: 0.7272 - val_loss: 0.6689 - val_accuracy: 0.7481 - 411ms/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.6746 - accuracy: 0.7431 - val_loss: 0.6331 - val_accuracy: 0.7592 - 427ms/epoch - 71ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.6403 - accuracy: 0.7586 - val_loss: 0.6029 - val_accuracy: 0.7689 - 422ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6124 - accuracy: 0.7714 - val_loss: 0.5674 - val_accuracy: 0.7911 - 434ms/epoch - 72ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.5873 - accuracy: 0.7842 - val_loss: 0.5481 - val_accuracy: 0.7985 - 433ms/epoch - 72ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.5699 - accuracy: 0.7917 - val_loss: 0.5323 - val_accuracy: 0.8046 - 419ms/epoch - 70ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.5535 - accuracy: 0.7989 - val_loss: 0.5158 - val_accuracy: 0.8100 - 436ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.5374 - accuracy: 0.8036 - val_loss: 0.5017 - val_accuracy: 0.8165 - 443ms/epoch - 74ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5245 - accuracy: 0.8096 - val_loss: 0.4934 - val_accuracy: 0.8200 - 430ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5163 - accuracy: 0.8118 - val_loss: 0.4845 - val_accuracy: 0.8219 - 422ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5082 - accuracy: 0.8156 - val_loss: 0.4709 - val_accuracy: 0.8297 - 419ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.4979 - accuracy: 0.8191 - val_loss: 0.4637 - val_accuracy: 0.8321 - 415ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.4907 - accuracy: 0.8226 - val_loss: 0.4540 - val_accuracy: 0.8361 - 433ms/epoch - 72ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.4814 - accuracy: 0.8263 - val_loss: 0.4499 - val_accuracy: 0.8361 - 419ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.4730 - accuracy: 0.8295 - val_loss: 0.4426 - val_accuracy: 0.8381 - 432ms/epoch - 72ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4669 - accuracy: 0.8325 - val_loss: 0.4369 - val_accuracy: 0.8421 - 435ms/epoch - 72ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4589 - accuracy: 0.8343 - val_loss: 0.4295 - val_accuracy: 0.8426 - 428ms/epoch - 71ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.9297 - accuracy: 0.6339 - val_loss: 0.5488 - val_accuracy: 0.7999 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 5s - loss: 0.6779 - accuracy: 0.7419 - val_loss: 0.5588 - val_accuracy: 0.7827 - 5s/epoch - 3ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.6151 - accuracy: 0.7676 - val_loss: 0.5535 - val_accuracy: 0.7632 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5751 - accuracy: 0.7868 - val_loss: 0.5719 - val_accuracy: 0.8085 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.5477 - accuracy: 0.7967 - val_loss: 0.5447 - val_accuracy: 0.8052 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 5s - loss: 0.5226 - accuracy: 0.8087 - val_loss: 0.5704 - val_accuracy: 0.7658 - 5s/epoch - 3ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.5104 - accuracy: 0.8131 - val_loss: 0.5250 - val_accuracy: 0.8318 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.4974 - accuracy: 0.8200 - val_loss: 0.5277 - val_accuracy: 0.8165 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 5s - loss: 0.4897 - accuracy: 0.8233 - val_loss: 0.5518 - val_accuracy: 0.7770 - 5s/epoch - 3ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4742 - accuracy: 0.8289 - val_loss: 0.5520 - val_accuracy: 0.7591 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4665 - accuracy: 0.8320 - val_loss: 0.6034 - val_accuracy: 0.7822 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 5s - loss: 0.4583 - accuracy: 0.8359 - val_loss: 0.6431 - val_accuracy: 0.7462 - 5s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.4527 - accuracy: 0.8382 - val_loss: 0.6146 - val_accuracy: 0.7694 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.4471 - accuracy: 0.8388 - val_loss: 0.5688 - val_accuracy: 0.7928 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 5s - loss: 0.4421 - accuracy: 0.8424 - val_loss: 0.6619 - val_accuracy: 0.7018 - 5s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.4438 - accuracy: 0.8436 - val_loss: 0.6051 - val_accuracy: 0.7497 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 5s - loss: 0.4349 - accuracy: 0.8475 - val_loss: 0.5615 - val_accuracy: 0.8206 - 5s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 5s - loss: 0.4349 - accuracy: 0.8455 - val_loss: 0.5438 - val_accuracy: 0.8155 - 5s/epoch - 3ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 5s - loss: 0.4267 - accuracy: 0.8472 - val_loss: 0.5322 - val_accuracy: 0.8369 - 5s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 5s - loss: 0.4243 - accuracy: 0.8507 - val_loss: 0.5884 - val_accuracy: 0.7814 - 5s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 6s - loss: 0.7573 - accuracy: 0.7208 - val_loss: 0.5287 - val_accuracy: 0.8060 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 5s - loss: 0.6197 - accuracy: 0.7766 - val_loss: 0.5098 - val_accuracy: 0.8189 - 5s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.5886 - accuracy: 0.7921 - val_loss: 0.4736 - val_accuracy: 0.8306 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 5s - loss: 0.5735 - accuracy: 0.8000 - val_loss: 0.4767 - val_accuracy: 0.8334 - 5s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.5597 - accuracy: 0.8045 - val_loss: 0.4551 - val_accuracy: 0.8340 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.5453 - accuracy: 0.8099 - val_loss: 0.4436 - val_accuracy: 0.8459 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 5s - loss: 0.5395 - accuracy: 0.8119 - val_loss: 0.4473 - val_accuracy: 0.8415 - 5s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 5s - loss: 0.5259 - accuracy: 0.8182 - val_loss: 0.4574 - val_accuracy: 0.8432 - 5s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.5202 - accuracy: 0.8221 - val_loss: 0.4355 - val_accuracy: 0.8476 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.5186 - accuracy: 0.8208 - val_loss: 0.4164 - val_accuracy: 0.8509 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 5s - loss: 0.5118 - accuracy: 0.8222 - val_loss: 0.4231 - val_accuracy: 0.8550 - 5s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 5s - loss: 0.5043 - accuracy: 0.8264 - val_loss: 0.4216 - val_accuracy: 0.8566 - 5s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.5005 - accuracy: 0.8280 - val_loss: 0.4290 - val_accuracy: 0.8538 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.5021 - accuracy: 0.8265 - val_loss: 0.4262 - val_accuracy: 0.8537 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 5s - loss: 0.5059 - accuracy: 0.8239 - val_loss: 0.4103 - val_accuracy: 0.8553 - 5s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.4972 - accuracy: 0.8274 - val_loss: 0.4230 - val_accuracy: 0.8522 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 5s - loss: 0.4896 - accuracy: 0.8333 - val_loss: 0.4069 - val_accuracy: 0.8564 - 5s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.4940 - accuracy: 0.8305 - val_loss: 0.4139 - val_accuracy: 0.8601 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 5s - loss: 0.4896 - accuracy: 0.8320 - val_loss: 0.4010 - val_accuracy: 0.8607 - 5s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.4890 - accuracy: 0.8316 - val_loss: 0.4159 - val_accuracy: 0.8559 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.5895 - accuracy: 0.4004 - val_loss: 0.8041 - val_accuracy: 0.6940 - 1s/epoch - 50ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.9526 - accuracy: 0.6283 - val_loss: 0.6576 - val_accuracy: 0.7505 - 455ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.8275 - accuracy: 0.6806 - val_loss: 0.5988 - val_accuracy: 0.7658 - 443ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.7591 - accuracy: 0.7061 - val_loss: 0.5643 - val_accuracy: 0.7823 - 469ms/epoch - 20ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.7136 - accuracy: 0.7266 - val_loss: 0.5715 - val_accuracy: 0.7861 - 447ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6873 - accuracy: 0.7377 - val_loss: 0.5266 - val_accuracy: 0.8019 - 447ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.6586 - accuracy: 0.7484 - val_loss: 0.5115 - val_accuracy: 0.8072 - 467ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.6386 - accuracy: 0.7560 - val_loss: 0.5150 - val_accuracy: 0.8056 - 440ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.6225 - accuracy: 0.7623 - val_loss: 0.4966 - val_accuracy: 0.8162 - 469ms/epoch - 20ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.6087 - accuracy: 0.7688 - val_loss: 0.5131 - val_accuracy: 0.7969 - 453ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5919 - accuracy: 0.7768 - val_loss: 0.4880 - val_accuracy: 0.8205 - 448ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5807 - accuracy: 0.7808 - val_loss: 0.4842 - val_accuracy: 0.8094 - 443ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.5672 - accuracy: 0.7872 - val_loss: 0.5010 - val_accuracy: 0.7904 - 451ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.5629 - accuracy: 0.7890 - val_loss: 0.4989 - val_accuracy: 0.7901 - 470ms/epoch - 20ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.5518 - accuracy: 0.7951 - val_loss: 0.4807 - val_accuracy: 0.8030 - 460ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.5400 - accuracy: 0.7983 - val_loss: 0.4485 - val_accuracy: 0.8301 - 459ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.5345 - accuracy: 0.8003 - val_loss: 0.4782 - val_accuracy: 0.8050 - 448ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.5284 - accuracy: 0.8033 - val_loss: 0.4867 - val_accuracy: 0.7879 - 445ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.5202 - accuracy: 0.8074 - val_loss: 0.4562 - val_accuracy: 0.8235 - 468ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.5155 - accuracy: 0.8093 - val_loss: 0.4845 - val_accuracy: 0.7898 - 459ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.2665 - accuracy: 0.5315 - val_loss: 0.7330 - val_accuracy: 0.7235 - 1s/epoch - 46ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7658 - accuracy: 0.7112 - val_loss: 0.6207 - val_accuracy: 0.7629 - 462ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6769 - accuracy: 0.7455 - val_loss: 0.5744 - val_accuracy: 0.7807 - 474ms/epoch - 20ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6308 - accuracy: 0.7658 - val_loss: 0.5446 - val_accuracy: 0.7939 - 464ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6030 - accuracy: 0.7799 - val_loss: 0.5251 - val_accuracy: 0.8059 - 445ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5823 - accuracy: 0.7900 - val_loss: 0.5083 - val_accuracy: 0.8119 - 452ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5595 - accuracy: 0.7989 - val_loss: 0.4892 - val_accuracy: 0.8221 - 472ms/epoch - 20ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5501 - accuracy: 0.8048 - val_loss: 0.4748 - val_accuracy: 0.8291 - 451ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5336 - accuracy: 0.8101 - val_loss: 0.4668 - val_accuracy: 0.8301 - 462ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5215 - accuracy: 0.8157 - val_loss: 0.4576 - val_accuracy: 0.8355 - 463ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5139 - accuracy: 0.8155 - val_loss: 0.4450 - val_accuracy: 0.8361 - 441ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5044 - accuracy: 0.8217 - val_loss: 0.4430 - val_accuracy: 0.8403 - 457ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4949 - accuracy: 0.8255 - val_loss: 0.4253 - val_accuracy: 0.8451 - 465ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4824 - accuracy: 0.8305 - val_loss: 0.4195 - val_accuracy: 0.8473 - 450ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4784 - accuracy: 0.8315 - val_loss: 0.4136 - val_accuracy: 0.8482 - 464ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4736 - accuracy: 0.8352 - val_loss: 0.4081 - val_accuracy: 0.8514 - 455ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4630 - accuracy: 0.8392 - val_loss: 0.4075 - val_accuracy: 0.8537 - 450ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4581 - accuracy: 0.8391 - val_loss: 0.3999 - val_accuracy: 0.8536 - 468ms/epoch - 20ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4549 - accuracy: 0.8397 - val_loss: 0.4084 - val_accuracy: 0.8530 - 472ms/epoch - 20ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4532 - accuracy: 0.8412 - val_loss: 0.3964 - val_accuracy: 0.8542 - 470ms/epoch - 20ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1923 - accuracy: 0.1905 - val_loss: 1.7671 - val_accuracy: 0.4821 - 1s/epoch - 178ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.6852 - accuracy: 0.3889 - val_loss: 1.1233 - val_accuracy: 0.6298 - 426ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.3010 - accuracy: 0.5115 - val_loss: 0.9006 - val_accuracy: 0.6629 - 430ms/epoch - 72ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.1275 - accuracy: 0.5682 - val_loss: 0.8019 - val_accuracy: 0.6870 - 436ms/epoch - 73ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.0313 - accuracy: 0.5996 - val_loss: 0.7360 - val_accuracy: 0.7197 - 417ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.9656 - accuracy: 0.6262 - val_loss: 0.7003 - val_accuracy: 0.7365 - 420ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.9150 - accuracy: 0.6475 - val_loss: 0.6725 - val_accuracy: 0.7445 - 424ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.8802 - accuracy: 0.6607 - val_loss: 0.6567 - val_accuracy: 0.7482 - 436ms/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.8538 - accuracy: 0.6704 - val_loss: 0.6351 - val_accuracy: 0.7595 - 432ms/epoch - 72ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.8261 - accuracy: 0.6812 - val_loss: 0.6355 - val_accuracy: 0.7442 - 434ms/epoch - 72ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.8047 - accuracy: 0.6858 - val_loss: 0.6165 - val_accuracy: 0.7535 - 433ms/epoch - 72ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7952 - accuracy: 0.6905 - val_loss: 0.6065 - val_accuracy: 0.7646 - 430ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.7701 - accuracy: 0.7020 - val_loss: 0.5888 - val_accuracy: 0.7762 - 429ms/epoch - 71ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.7541 - accuracy: 0.7067 - val_loss: 0.5890 - val_accuracy: 0.7769 - 444ms/epoch - 74ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.7411 - accuracy: 0.7146 - val_loss: 0.5981 - val_accuracy: 0.7327 - 445ms/epoch - 74ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.7355 - accuracy: 0.7162 - val_loss: 0.5896 - val_accuracy: 0.7490 - 425ms/epoch - 71ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.7256 - accuracy: 0.7202 - val_loss: 0.5709 - val_accuracy: 0.7769 - 423ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.7141 - accuracy: 0.7251 - val_loss: 0.5587 - val_accuracy: 0.7922 - 437ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6994 - accuracy: 0.7301 - val_loss: 0.5695 - val_accuracy: 0.7335 - 429ms/epoch - 72ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.6988 - accuracy: 0.7297 - val_loss: 0.5589 - val_accuracy: 0.7688 - 416ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 2s - loss: 1.8146 - accuracy: 0.3645 - val_loss: 1.0906 - val_accuracy: 0.6152 - 2s/epoch - 270ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1637 - accuracy: 0.5510 - val_loss: 0.8767 - val_accuracy: 0.6581 - 436ms/epoch - 73ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9816 - accuracy: 0.6163 - val_loss: 0.7749 - val_accuracy: 0.7015 - 420ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8682 - accuracy: 0.6656 - val_loss: 0.7083 - val_accuracy: 0.7285 - 436ms/epoch - 73ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7937 - accuracy: 0.6996 - val_loss: 0.6554 - val_accuracy: 0.7486 - 433ms/epoch - 72ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7404 - accuracy: 0.7251 - val_loss: 0.6256 - val_accuracy: 0.7640 - 420ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7077 - accuracy: 0.7374 - val_loss: 0.6060 - val_accuracy: 0.7689 - 432ms/epoch - 72ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6842 - accuracy: 0.7461 - val_loss: 0.5940 - val_accuracy: 0.7791 - 418ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6638 - accuracy: 0.7534 - val_loss: 0.5753 - val_accuracy: 0.7845 - 434ms/epoch - 72ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6414 - accuracy: 0.7642 - val_loss: 0.5614 - val_accuracy: 0.7924 - 441ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6243 - accuracy: 0.7702 - val_loss: 0.5501 - val_accuracy: 0.7980 - 446ms/epoch - 74ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6118 - accuracy: 0.7776 - val_loss: 0.5452 - val_accuracy: 0.7979 - 450ms/epoch - 75ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6004 - accuracy: 0.7805 - val_loss: 0.5346 - val_accuracy: 0.8024 - 411ms/epoch - 69ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5920 - accuracy: 0.7864 - val_loss: 0.5199 - val_accuracy: 0.8110 - 438ms/epoch - 73ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5824 - accuracy: 0.7899 - val_loss: 0.5222 - val_accuracy: 0.8067 - 437ms/epoch - 73ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5797 - accuracy: 0.7923 - val_loss: 0.5121 - val_accuracy: 0.8095 - 435ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5698 - accuracy: 0.7943 - val_loss: 0.5001 - val_accuracy: 0.8162 - 417ms/epoch - 69ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5601 - accuracy: 0.7980 - val_loss: 0.4925 - val_accuracy: 0.8203 - 418ms/epoch - 70ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5538 - accuracy: 0.8042 - val_loss: 0.4883 - val_accuracy: 0.8205 - 440ms/epoch - 73ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5428 - accuracy: 0.8082 - val_loss: 0.4787 - val_accuracy: 0.8261 - 433ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5413 - accuracy: 0.8006 - val_loss: 0.4379 - val_accuracy: 0.8396 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.3587 - accuracy: 0.8689 - val_loss: 0.3572 - val_accuracy: 0.8684 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3110 - accuracy: 0.8865 - val_loss: 0.3180 - val_accuracy: 0.8824 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.2800 - accuracy: 0.8984 - val_loss: 0.3134 - val_accuracy: 0.8897 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.2605 - accuracy: 0.9054 - val_loss: 0.3000 - val_accuracy: 0.8941 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 7s - loss: 0.2451 - accuracy: 0.9112 - val_loss: 0.2865 - val_accuracy: 0.8980 - 7s/epoch - 5ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.2324 - accuracy: 0.9159 - val_loss: 0.2642 - val_accuracy: 0.9091 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.2238 - accuracy: 0.9198 - val_loss: 0.2868 - val_accuracy: 0.9005 - 7s/epoch - 5ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.2177 - accuracy: 0.9218 - val_loss: 0.2738 - val_accuracy: 0.9057 - 7s/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.2104 - accuracy: 0.9246 - val_loss: 0.2740 - val_accuracy: 0.9035 - 7s/epoch - 5ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2085 - accuracy: 0.9259 - val_loss: 0.3184 - val_accuracy: 0.9024 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.2054 - accuracy: 0.9277 - val_loss: 0.2776 - val_accuracy: 0.9071 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.2031 - accuracy: 0.9288 - val_loss: 0.3030 - val_accuracy: 0.8913 - 7s/epoch - 5ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.2019 - accuracy: 0.9291 - val_loss: 0.3715 - val_accuracy: 0.8944 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2015 - accuracy: 0.9278 - val_loss: 0.2850 - val_accuracy: 0.9075 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.2022 - accuracy: 0.9297 - val_loss: 0.3393 - val_accuracy: 0.8979 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.2050 - accuracy: 0.9289 - val_loss: 0.2728 - val_accuracy: 0.9060 - 7s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.2056 - accuracy: 0.9278 - val_loss: 0.2984 - val_accuracy: 0.9015 - 7s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2061 - accuracy: 0.9285 - val_loss: 0.3783 - val_accuracy: 0.9066 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.2081 - accuracy: 0.9277 - val_loss: 0.3249 - val_accuracy: 0.8973 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5285 - accuracy: 0.8078 - val_loss: 0.4432 - val_accuracy: 0.8415 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.3986 - accuracy: 0.8576 - val_loss: 0.3900 - val_accuracy: 0.8583 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 7s - loss: 0.3653 - accuracy: 0.8691 - val_loss: 0.3840 - val_accuracy: 0.8611 - 7s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3461 - accuracy: 0.8761 - val_loss: 0.3742 - val_accuracy: 0.8631 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 7s - loss: 0.3302 - accuracy: 0.8828 - val_loss: 0.3633 - val_accuracy: 0.8632 - 7s/epoch - 5ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3159 - accuracy: 0.8876 - val_loss: 0.3566 - val_accuracy: 0.8668 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.3077 - accuracy: 0.8890 - val_loss: 0.3411 - val_accuracy: 0.8762 - 7s/epoch - 5ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.2970 - accuracy: 0.8930 - val_loss: 0.3396 - val_accuracy: 0.8753 - 7s/epoch - 5ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.2898 - accuracy: 0.8961 - val_loss: 0.3360 - val_accuracy: 0.8770 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.2829 - accuracy: 0.8977 - val_loss: 0.3359 - val_accuracy: 0.8772 - 7s/epoch - 5ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 7s - loss: 0.2778 - accuracy: 0.9001 - val_loss: 0.3369 - val_accuracy: 0.8776 - 7s/epoch - 5ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.2718 - accuracy: 0.9017 - val_loss: 0.3280 - val_accuracy: 0.8827 - 7s/epoch - 5ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.2646 - accuracy: 0.9043 - val_loss: 0.3292 - val_accuracy: 0.8813 - 7s/epoch - 5ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.2586 - accuracy: 0.9067 - val_loss: 0.3214 - val_accuracy: 0.8830 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2574 - accuracy: 0.9073 - val_loss: 0.3312 - val_accuracy: 0.8822 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2487 - accuracy: 0.9111 - val_loss: 0.3304 - val_accuracy: 0.8820 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.2429 - accuracy: 0.9111 - val_loss: 0.3215 - val_accuracy: 0.8828 - 7s/epoch - 5ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2379 - accuracy: 0.9145 - val_loss: 0.3330 - val_accuracy: 0.8833 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2371 - accuracy: 0.9142 - val_loss: 0.3474 - val_accuracy: 0.8778 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2300 - accuracy: 0.9178 - val_loss: 0.3154 - val_accuracy: 0.8876 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.4188 - accuracy: 0.5375 - val_loss: 0.9576 - val_accuracy: 0.6685 - 1s/epoch - 55ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8407 - accuracy: 0.6924 - val_loss: 0.8479 - val_accuracy: 0.6766 - 421ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7309 - accuracy: 0.7180 - val_loss: 0.7095 - val_accuracy: 0.7307 - 427ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6658 - accuracy: 0.7454 - val_loss: 0.6448 - val_accuracy: 0.7467 - 422ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6146 - accuracy: 0.7648 - val_loss: 0.6871 - val_accuracy: 0.7294 - 440ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5818 - accuracy: 0.7782 - val_loss: 0.6327 - val_accuracy: 0.7560 - 422ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5490 - accuracy: 0.7917 - val_loss: 0.5455 - val_accuracy: 0.7985 - 421ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5285 - accuracy: 0.8035 - val_loss: 0.5657 - val_accuracy: 0.7808 - 423ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5017 - accuracy: 0.8127 - val_loss: 0.5330 - val_accuracy: 0.8011 - 437ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4869 - accuracy: 0.8200 - val_loss: 0.4858 - val_accuracy: 0.8203 - 423ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4666 - accuracy: 0.8276 - val_loss: 0.4668 - val_accuracy: 0.8313 - 423ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4535 - accuracy: 0.8339 - val_loss: 0.4769 - val_accuracy: 0.8235 - 424ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4352 - accuracy: 0.8392 - val_loss: 0.4627 - val_accuracy: 0.8306 - 434ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4283 - accuracy: 0.8429 - val_loss: 0.4633 - val_accuracy: 0.8251 - 420ms/epoch - 17ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4158 - accuracy: 0.8484 - val_loss: 0.4619 - val_accuracy: 0.8219 - 436ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4075 - accuracy: 0.8503 - val_loss: 0.4080 - val_accuracy: 0.8529 - 440ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3901 - accuracy: 0.8568 - val_loss: 0.4586 - val_accuracy: 0.8290 - 421ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3896 - accuracy: 0.8568 - val_loss: 0.3954 - val_accuracy: 0.8573 - 437ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3782 - accuracy: 0.8619 - val_loss: 0.3825 - val_accuracy: 0.8614 - 436ms/epoch - 18ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3717 - accuracy: 0.8634 - val_loss: 0.3931 - val_accuracy: 0.8542 - 441ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.2342 - accuracy: 0.5745 - val_loss: 0.8449 - val_accuracy: 0.6818 - 1s/epoch - 55ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7532 - accuracy: 0.7170 - val_loss: 0.6594 - val_accuracy: 0.7615 - 418ms/epoch - 17ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6549 - accuracy: 0.7529 - val_loss: 0.6291 - val_accuracy: 0.7644 - 440ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5982 - accuracy: 0.7785 - val_loss: 0.5918 - val_accuracy: 0.7779 - 454ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5583 - accuracy: 0.7931 - val_loss: 0.5505 - val_accuracy: 0.7954 - 442ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5311 - accuracy: 0.8036 - val_loss: 0.5294 - val_accuracy: 0.8082 - 422ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5016 - accuracy: 0.8166 - val_loss: 0.5346 - val_accuracy: 0.8063 - 435ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4871 - accuracy: 0.8220 - val_loss: 0.4849 - val_accuracy: 0.8167 - 428ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4655 - accuracy: 0.8322 - val_loss: 0.4761 - val_accuracy: 0.8249 - 420ms/epoch - 17ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4472 - accuracy: 0.8382 - val_loss: 0.4850 - val_accuracy: 0.8229 - 418ms/epoch - 17ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4371 - accuracy: 0.8417 - val_loss: 0.4422 - val_accuracy: 0.8405 - 437ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4214 - accuracy: 0.8481 - val_loss: 0.4528 - val_accuracy: 0.8332 - 441ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4122 - accuracy: 0.8506 - val_loss: 0.4235 - val_accuracy: 0.8462 - 431ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4011 - accuracy: 0.8560 - val_loss: 0.4212 - val_accuracy: 0.8454 - 427ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3916 - accuracy: 0.8587 - val_loss: 0.4285 - val_accuracy: 0.8441 - 435ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3865 - accuracy: 0.8605 - val_loss: 0.3939 - val_accuracy: 0.8569 - 422ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3764 - accuracy: 0.8646 - val_loss: 0.4094 - val_accuracy: 0.8523 - 421ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3718 - accuracy: 0.8663 - val_loss: 0.3885 - val_accuracy: 0.8577 - 430ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3626 - accuracy: 0.8706 - val_loss: 0.3869 - val_accuracy: 0.8571 - 430ms/epoch - 18ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3580 - accuracy: 0.8721 - val_loss: 0.3851 - val_accuracy: 0.8617 - 451ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1047 - accuracy: 0.3004 - val_loss: 1.6429 - val_accuracy: 0.4772 - 1s/epoch - 195ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.5745 - accuracy: 0.5271 - val_loss: 1.2714 - val_accuracy: 0.5384 - 397ms/epoch - 66ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.2401 - accuracy: 0.5594 - val_loss: 1.0110 - val_accuracy: 0.6578 - 421ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.0166 - accuracy: 0.6414 - val_loss: 0.8929 - val_accuracy: 0.6783 - 408ms/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9497 - accuracy: 0.6536 - val_loss: 0.9741 - val_accuracy: 0.6331 - 399ms/epoch - 66ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8567 - accuracy: 0.6834 - val_loss: 0.8342 - val_accuracy: 0.6745 - 390ms/epoch - 65ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8943 - accuracy: 0.6571 - val_loss: 0.8611 - val_accuracy: 0.6694 - 409ms/epoch - 68ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7792 - accuracy: 0.7060 - val_loss: 0.7894 - val_accuracy: 0.7031 - 414ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7852 - accuracy: 0.6938 - val_loss: 0.7644 - val_accuracy: 0.6970 - 408ms/epoch - 68ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7641 - accuracy: 0.7065 - val_loss: 0.7931 - val_accuracy: 0.6847 - 404ms/epoch - 67ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.7619 - accuracy: 0.7058 - val_loss: 0.7276 - val_accuracy: 0.7211 - 398ms/epoch - 66ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7232 - accuracy: 0.7211 - val_loss: 0.6982 - val_accuracy: 0.7465 - 406ms/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.7080 - accuracy: 0.7305 - val_loss: 0.7734 - val_accuracy: 0.7073 - 404ms/epoch - 67ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.7363 - accuracy: 0.7215 - val_loss: 0.7132 - val_accuracy: 0.7303 - 393ms/epoch - 66ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6888 - accuracy: 0.7440 - val_loss: 0.6702 - val_accuracy: 0.7372 - 430ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6566 - accuracy: 0.7403 - val_loss: 0.6490 - val_accuracy: 0.7535 - 388ms/epoch - 65ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.6406 - accuracy: 0.7574 - val_loss: 0.6799 - val_accuracy: 0.7396 - 397ms/epoch - 66ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6829 - accuracy: 0.7430 - val_loss: 0.6160 - val_accuracy: 0.7721 - 400ms/epoch - 67ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6316 - accuracy: 0.7605 - val_loss: 0.6178 - val_accuracy: 0.7737 - 399ms/epoch - 66ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.6405 - accuracy: 0.7565 - val_loss: 0.6036 - val_accuracy: 0.7785 - 390ms/epoch - 65ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.7382 - accuracy: 0.3962 - val_loss: 1.1774 - val_accuracy: 0.6473 - 1s/epoch - 203ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1085 - accuracy: 0.6297 - val_loss: 1.0849 - val_accuracy: 0.6067 - 396ms/epoch - 66ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9467 - accuracy: 0.6599 - val_loss: 0.8723 - val_accuracy: 0.6935 - 396ms/epoch - 66ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8742 - accuracy: 0.6668 - val_loss: 0.8032 - val_accuracy: 0.7109 - 398ms/epoch - 66ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7848 - accuracy: 0.7034 - val_loss: 0.8032 - val_accuracy: 0.6904 - 417ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7544 - accuracy: 0.7122 - val_loss: 0.7731 - val_accuracy: 0.7136 - 410ms/epoch - 68ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7337 - accuracy: 0.7238 - val_loss: 0.7323 - val_accuracy: 0.7190 - 403ms/epoch - 67ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6974 - accuracy: 0.7360 - val_loss: 0.6760 - val_accuracy: 0.7472 - 417ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6611 - accuracy: 0.7500 - val_loss: 0.6589 - val_accuracy: 0.7556 - 413ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6557 - accuracy: 0.7476 - val_loss: 0.6303 - val_accuracy: 0.7572 - 413ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6562 - accuracy: 0.7535 - val_loss: 0.6342 - val_accuracy: 0.7623 - 397ms/epoch - 66ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6130 - accuracy: 0.7696 - val_loss: 0.6142 - val_accuracy: 0.7682 - 412ms/epoch - 69ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5946 - accuracy: 0.7790 - val_loss: 0.5944 - val_accuracy: 0.7750 - 419ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5880 - accuracy: 0.7770 - val_loss: 0.6376 - val_accuracy: 0.7484 - 402ms/epoch - 67ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5782 - accuracy: 0.7839 - val_loss: 0.6038 - val_accuracy: 0.7642 - 409ms/epoch - 68ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5830 - accuracy: 0.7820 - val_loss: 0.5657 - val_accuracy: 0.7914 - 417ms/epoch - 69ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5633 - accuracy: 0.7889 - val_loss: 0.5689 - val_accuracy: 0.7876 - 401ms/epoch - 67ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5432 - accuracy: 0.7979 - val_loss: 0.5516 - val_accuracy: 0.7928 - 401ms/epoch - 67ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5410 - accuracy: 0.7980 - val_loss: 0.5786 - val_accuracy: 0.7819 - 412ms/epoch - 69ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5391 - accuracy: 0.7986 - val_loss: 0.5301 - val_accuracy: 0.8070 - 396ms/epoch - 66ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6055 - accuracy: 0.7733 - val_loss: 0.4294 - val_accuracy: 0.8425 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.3948 - accuracy: 0.8576 - val_loss: 0.3811 - val_accuracy: 0.8590 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3366 - accuracy: 0.8778 - val_loss: 0.3390 - val_accuracy: 0.8749 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3043 - accuracy: 0.8899 - val_loss: 0.3332 - val_accuracy: 0.8793 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.2854 - accuracy: 0.8960 - val_loss: 0.3078 - val_accuracy: 0.8868 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.2696 - accuracy: 0.9026 - val_loss: 0.3049 - val_accuracy: 0.8900 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.2582 - accuracy: 0.9064 - val_loss: 0.3227 - val_accuracy: 0.8834 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.2498 - accuracy: 0.9101 - val_loss: 0.3342 - val_accuracy: 0.8928 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.2435 - accuracy: 0.9134 - val_loss: 0.2915 - val_accuracy: 0.8952 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.2372 - accuracy: 0.9157 - val_loss: 0.3047 - val_accuracy: 0.8975 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2300 - accuracy: 0.9176 - val_loss: 0.3172 - val_accuracy: 0.8948 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.2291 - accuracy: 0.9184 - val_loss: 0.3082 - val_accuracy: 0.8963 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.2238 - accuracy: 0.9212 - val_loss: 0.3276 - val_accuracy: 0.8952 - 7s/epoch - 5ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.2184 - accuracy: 0.9234 - val_loss: 0.3198 - val_accuracy: 0.8939 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2160 - accuracy: 0.9227 - val_loss: 0.3718 - val_accuracy: 0.8935 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2125 - accuracy: 0.9243 - val_loss: 0.3349 - val_accuracy: 0.8958 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.2115 - accuracy: 0.9259 - val_loss: 0.3215 - val_accuracy: 0.8994 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2089 - accuracy: 0.9274 - val_loss: 0.3276 - val_accuracy: 0.8916 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2055 - accuracy: 0.9276 - val_loss: 0.3297 - val_accuracy: 0.9020 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2053 - accuracy: 0.9279 - val_loss: 0.3768 - val_accuracy: 0.8977 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5331 - accuracy: 0.8033 - val_loss: 0.4137 - val_accuracy: 0.8471 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.3873 - accuracy: 0.8593 - val_loss: 0.3639 - val_accuracy: 0.8683 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3432 - accuracy: 0.8754 - val_loss: 0.3500 - val_accuracy: 0.8692 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3124 - accuracy: 0.8860 - val_loss: 0.3336 - val_accuracy: 0.8770 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.2906 - accuracy: 0.8933 - val_loss: 0.3437 - val_accuracy: 0.8758 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.2739 - accuracy: 0.9001 - val_loss: 0.3205 - val_accuracy: 0.8826 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.2578 - accuracy: 0.9043 - val_loss: 0.3125 - val_accuracy: 0.8873 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.2440 - accuracy: 0.9114 - val_loss: 0.3114 - val_accuracy: 0.8873 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.2304 - accuracy: 0.9157 - val_loss: 0.3080 - val_accuracy: 0.8882 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.2187 - accuracy: 0.9200 - val_loss: 0.3107 - val_accuracy: 0.8884 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2072 - accuracy: 0.9241 - val_loss: 0.3213 - val_accuracy: 0.8860 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.1971 - accuracy: 0.9278 - val_loss: 0.3269 - val_accuracy: 0.8837 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.1881 - accuracy: 0.9301 - val_loss: 0.3132 - val_accuracy: 0.8918 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.1768 - accuracy: 0.9355 - val_loss: 0.3276 - val_accuracy: 0.8903 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.1672 - accuracy: 0.9394 - val_loss: 0.3259 - val_accuracy: 0.8888 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.1585 - accuracy: 0.9431 - val_loss: 0.3363 - val_accuracy: 0.8909 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.1510 - accuracy: 0.9450 - val_loss: 0.3367 - val_accuracy: 0.8877 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.1438 - accuracy: 0.9487 - val_loss: 0.3442 - val_accuracy: 0.8875 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.1357 - accuracy: 0.9520 - val_loss: 0.3466 - val_accuracy: 0.8921 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.1290 - accuracy: 0.9545 - val_loss: 0.3473 - val_accuracy: 0.8935 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.5692 - accuracy: 0.4540 - val_loss: 1.1073 - val_accuracy: 0.6191 - 1s/epoch - 55ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.9418 - accuracy: 0.6618 - val_loss: 1.0577 - val_accuracy: 0.6232 - 444ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.8270 - accuracy: 0.6932 - val_loss: 0.7332 - val_accuracy: 0.7305 - 457ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.7478 - accuracy: 0.7142 - val_loss: 0.7068 - val_accuracy: 0.7346 - 450ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.7050 - accuracy: 0.7320 - val_loss: 0.7377 - val_accuracy: 0.7111 - 465ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6653 - accuracy: 0.7386 - val_loss: 0.6260 - val_accuracy: 0.7674 - 456ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.6225 - accuracy: 0.7581 - val_loss: 0.6749 - val_accuracy: 0.7271 - 467ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5966 - accuracy: 0.7670 - val_loss: 0.6027 - val_accuracy: 0.7600 - 470ms/epoch - 20ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5769 - accuracy: 0.7770 - val_loss: 0.5760 - val_accuracy: 0.7835 - 460ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5547 - accuracy: 0.7869 - val_loss: 0.5460 - val_accuracy: 0.7934 - 447ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5376 - accuracy: 0.7942 - val_loss: 0.5466 - val_accuracy: 0.7954 - 442ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5167 - accuracy: 0.8039 - val_loss: 0.5208 - val_accuracy: 0.7935 - 488ms/epoch - 20ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4984 - accuracy: 0.8117 - val_loss: 0.5598 - val_accuracy: 0.7834 - 465ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4883 - accuracy: 0.8158 - val_loss: 0.4912 - val_accuracy: 0.8149 - 466ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 1s - loss: 0.4820 - accuracy: 0.8179 - val_loss: 0.4914 - val_accuracy: 0.8147 - 555ms/epoch - 23ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4644 - accuracy: 0.8257 - val_loss: 0.4727 - val_accuracy: 0.8200 - 476ms/epoch - 20ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4565 - accuracy: 0.8287 - val_loss: 0.4656 - val_accuracy: 0.8338 - 450ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4489 - accuracy: 0.8332 - val_loss: 0.4395 - val_accuracy: 0.8406 - 448ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4383 - accuracy: 0.8361 - val_loss: 0.4647 - val_accuracy: 0.8208 - 451ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4319 - accuracy: 0.8415 - val_loss: 0.4859 - val_accuracy: 0.8114 - 437ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 2s - loss: 1.2604 - accuracy: 0.5368 - val_loss: 0.8781 - val_accuracy: 0.6724 - 2s/epoch - 66ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8114 - accuracy: 0.6843 - val_loss: 0.7443 - val_accuracy: 0.7141 - 445ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6915 - accuracy: 0.7359 - val_loss: 0.6596 - val_accuracy: 0.7525 - 472ms/epoch - 20ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6366 - accuracy: 0.7553 - val_loss: 0.6132 - val_accuracy: 0.7617 - 457ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5893 - accuracy: 0.7751 - val_loss: 0.5857 - val_accuracy: 0.7738 - 440ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5565 - accuracy: 0.7911 - val_loss: 0.5707 - val_accuracy: 0.7778 - 439ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5271 - accuracy: 0.8050 - val_loss: 0.5337 - val_accuracy: 0.7944 - 459ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5119 - accuracy: 0.8102 - val_loss: 0.5321 - val_accuracy: 0.8046 - 448ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4888 - accuracy: 0.8212 - val_loss: 0.4963 - val_accuracy: 0.8207 - 466ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4750 - accuracy: 0.8271 - val_loss: 0.4935 - val_accuracy: 0.8132 - 457ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4626 - accuracy: 0.8302 - val_loss: 0.4546 - val_accuracy: 0.8382 - 470ms/epoch - 20ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4494 - accuracy: 0.8368 - val_loss: 0.4512 - val_accuracy: 0.8376 - 444ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4395 - accuracy: 0.8406 - val_loss: 0.4722 - val_accuracy: 0.8226 - 462ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4276 - accuracy: 0.8453 - val_loss: 0.4338 - val_accuracy: 0.8440 - 461ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4191 - accuracy: 0.8487 - val_loss: 0.4399 - val_accuracy: 0.8381 - 463ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4105 - accuracy: 0.8517 - val_loss: 0.4285 - val_accuracy: 0.8423 - 444ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4002 - accuracy: 0.8552 - val_loss: 0.4360 - val_accuracy: 0.8389 - 446ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3975 - accuracy: 0.8567 - val_loss: 0.3966 - val_accuracy: 0.8565 - 487ms/epoch - 20ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3888 - accuracy: 0.8612 - val_loss: 0.3942 - val_accuracy: 0.8580 - 451ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3827 - accuracy: 0.8616 - val_loss: 0.3975 - val_accuracy: 0.8525 - 465ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.2162 - accuracy: 0.2476 - val_loss: 2.0510 - val_accuracy: 0.2234 - 1s/epoch - 205ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.8024 - accuracy: 0.4093 - val_loss: 1.5507 - val_accuracy: 0.4781 - 436ms/epoch - 73ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.4386 - accuracy: 0.5120 - val_loss: 1.1839 - val_accuracy: 0.6126 - 427ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.1986 - accuracy: 0.5888 - val_loss: 1.0866 - val_accuracy: 0.5996 - 425ms/epoch - 71ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.0816 - accuracy: 0.5836 - val_loss: 0.9944 - val_accuracy: 0.6306 - 421ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.9861 - accuracy: 0.6369 - val_loss: 0.9873 - val_accuracy: 0.6359 - 421ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.9993 - accuracy: 0.6163 - val_loss: 0.8909 - val_accuracy: 0.6850 - 416ms/epoch - 69ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.9430 - accuracy: 0.6321 - val_loss: 0.8901 - val_accuracy: 0.6649 - 437ms/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.8704 - accuracy: 0.6719 - val_loss: 0.9154 - val_accuracy: 0.6288 - 415ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.8838 - accuracy: 0.6525 - val_loss: 0.9220 - val_accuracy: 0.6378 - 419ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.8659 - accuracy: 0.6613 - val_loss: 0.7968 - val_accuracy: 0.6965 - 426ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.8329 - accuracy: 0.6856 - val_loss: 0.8709 - val_accuracy: 0.6836 - 423ms/epoch - 71ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.8177 - accuracy: 0.6855 - val_loss: 0.8418 - val_accuracy: 0.6441 - 437ms/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.8420 - accuracy: 0.6623 - val_loss: 0.7809 - val_accuracy: 0.6998 - 417ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.7831 - accuracy: 0.6933 - val_loss: 0.8132 - val_accuracy: 0.6841 - 417ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.7792 - accuracy: 0.6973 - val_loss: 0.7820 - val_accuracy: 0.6957 - 424ms/epoch - 71ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.7870 - accuracy: 0.6981 - val_loss: 0.7551 - val_accuracy: 0.7077 - 431ms/epoch - 72ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.7714 - accuracy: 0.6938 - val_loss: 0.7438 - val_accuracy: 0.6979 - 439ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.7409 - accuracy: 0.7004 - val_loss: 0.7931 - val_accuracy: 0.6877 - 438ms/epoch - 73ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.7626 - accuracy: 0.7042 - val_loss: 0.7583 - val_accuracy: 0.7008 - 431ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8359 - accuracy: 0.3956 - val_loss: 1.2833 - val_accuracy: 0.5396 - 1s/epoch - 223ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1808 - accuracy: 0.5781 - val_loss: 1.0517 - val_accuracy: 0.6010 - 419ms/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9817 - accuracy: 0.6349 - val_loss: 0.9237 - val_accuracy: 0.6406 - 418ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8849 - accuracy: 0.6618 - val_loss: 0.8359 - val_accuracy: 0.7043 - 410ms/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8263 - accuracy: 0.6846 - val_loss: 0.8016 - val_accuracy: 0.6965 - 422ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7685 - accuracy: 0.7183 - val_loss: 0.7315 - val_accuracy: 0.7306 - 444ms/epoch - 74ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7362 - accuracy: 0.7200 - val_loss: 0.7297 - val_accuracy: 0.7078 - 416ms/epoch - 69ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7130 - accuracy: 0.7248 - val_loss: 0.6859 - val_accuracy: 0.7443 - 446ms/epoch - 74ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6865 - accuracy: 0.7410 - val_loss: 0.7007 - val_accuracy: 0.7234 - 423ms/epoch - 70ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6513 - accuracy: 0.7571 - val_loss: 0.6680 - val_accuracy: 0.7472 - 439ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6480 - accuracy: 0.7523 - val_loss: 0.6499 - val_accuracy: 0.7596 - 411ms/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6341 - accuracy: 0.7641 - val_loss: 0.6418 - val_accuracy: 0.7643 - 438ms/epoch - 73ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6199 - accuracy: 0.7691 - val_loss: 0.6311 - val_accuracy: 0.7590 - 424ms/epoch - 71ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6067 - accuracy: 0.7688 - val_loss: 0.6136 - val_accuracy: 0.7701 - 435ms/epoch - 73ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6087 - accuracy: 0.7734 - val_loss: 0.5977 - val_accuracy: 0.7798 - 420ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5752 - accuracy: 0.7875 - val_loss: 0.5936 - val_accuracy: 0.7711 - 440ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5694 - accuracy: 0.7877 - val_loss: 0.6036 - val_accuracy: 0.7743 - 453ms/epoch - 75ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5798 - accuracy: 0.7845 - val_loss: 0.5663 - val_accuracy: 0.7939 - 409ms/epoch - 68ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5644 - accuracy: 0.7891 - val_loss: 0.5567 - val_accuracy: 0.7954 - 436ms/epoch - 73ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5494 - accuracy: 0.8000 - val_loss: 0.5711 - val_accuracy: 0.7822 - 420ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.4762 - accuracy: 0.8239 - val_loss: 0.3560 - val_accuracy: 0.8642 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.3342 - accuracy: 0.8805 - val_loss: 0.3290 - val_accuracy: 0.8779 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 7s - loss: 0.3091 - accuracy: 0.8907 - val_loss: 0.3053 - val_accuracy: 0.8889 - 7s/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 7s - loss: 0.2971 - accuracy: 0.8953 - val_loss: 0.3047 - val_accuracy: 0.8934 - 7s/epoch - 5ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 7s - loss: 0.2934 - accuracy: 0.8986 - val_loss: 0.3699 - val_accuracy: 0.8793 - 7s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 7s - loss: 0.2967 - accuracy: 0.8969 - val_loss: 0.3061 - val_accuracy: 0.8899 - 7s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.2957 - accuracy: 0.8976 - val_loss: 0.3482 - val_accuracy: 0.8898 - 7s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.2996 - accuracy: 0.8972 - val_loss: 0.3202 - val_accuracy: 0.8878 - 7s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3003 - accuracy: 0.8972 - val_loss: 0.3683 - val_accuracy: 0.8867 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3025 - accuracy: 0.8984 - val_loss: 0.3770 - val_accuracy: 0.8795 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3022 - accuracy: 0.8959 - val_loss: 0.4453 - val_accuracy: 0.8763 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.3075 - accuracy: 0.8980 - val_loss: 0.3381 - val_accuracy: 0.8884 - 7s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3072 - accuracy: 0.8957 - val_loss: 0.4436 - val_accuracy: 0.8532 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.3125 - accuracy: 0.8943 - val_loss: 0.3331 - val_accuracy: 0.8830 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 7s - loss: 0.3144 - accuracy: 0.8925 - val_loss: 0.4562 - val_accuracy: 0.8809 - 7s/epoch - 5ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.3249 - accuracy: 0.8918 - val_loss: 0.3597 - val_accuracy: 0.8819 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3219 - accuracy: 0.8914 - val_loss: 0.3966 - val_accuracy: 0.8880 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.3308 - accuracy: 0.8908 - val_loss: 0.3632 - val_accuracy: 0.8653 - 7s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 7s - loss: 0.3299 - accuracy: 0.8893 - val_loss: 0.3606 - val_accuracy: 0.8773 - 7s/epoch - 5ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.3247 - accuracy: 0.8888 - val_loss: 0.3635 - val_accuracy: 0.8812 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.5319 - accuracy: 0.8076 - val_loss: 0.4539 - val_accuracy: 0.8319 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4299 - accuracy: 0.8452 - val_loss: 0.4211 - val_accuracy: 0.8446 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4125 - accuracy: 0.8546 - val_loss: 0.4462 - val_accuracy: 0.8394 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 7s - loss: 0.4021 - accuracy: 0.8571 - val_loss: 0.4052 - val_accuracy: 0.8527 - 7s/epoch - 5ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 7s - loss: 0.3919 - accuracy: 0.8604 - val_loss: 0.3952 - val_accuracy: 0.8612 - 7s/epoch - 5ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3826 - accuracy: 0.8638 - val_loss: 0.3976 - val_accuracy: 0.8553 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.3702 - accuracy: 0.8683 - val_loss: 0.3781 - val_accuracy: 0.8634 - 7s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3661 - accuracy: 0.8692 - val_loss: 0.3788 - val_accuracy: 0.8607 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3596 - accuracy: 0.8732 - val_loss: 0.3860 - val_accuracy: 0.8626 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.3624 - accuracy: 0.8729 - val_loss: 0.4129 - val_accuracy: 0.8516 - 7s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 7s - loss: 0.3541 - accuracy: 0.8747 - val_loss: 0.3809 - val_accuracy: 0.8642 - 7s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.3526 - accuracy: 0.8748 - val_loss: 0.3864 - val_accuracy: 0.8672 - 7s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.3456 - accuracy: 0.8777 - val_loss: 0.3864 - val_accuracy: 0.8612 - 7s/epoch - 5ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.3421 - accuracy: 0.8786 - val_loss: 0.3687 - val_accuracy: 0.8695 - 7s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 7s - loss: 0.3371 - accuracy: 0.8816 - val_loss: 0.3790 - val_accuracy: 0.8650 - 7s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.3363 - accuracy: 0.8800 - val_loss: 0.3708 - val_accuracy: 0.8692 - 7s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.3433 - accuracy: 0.8776 - val_loss: 0.3836 - val_accuracy: 0.8640 - 7s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.3418 - accuracy: 0.8780 - val_loss: 0.3878 - val_accuracy: 0.8607 - 7s/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 7s - loss: 0.3329 - accuracy: 0.8826 - val_loss: 0.3805 - val_accuracy: 0.8649 - 7s/epoch - 5ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.3292 - accuracy: 0.8828 - val_loss: 0.3804 - val_accuracy: 0.8667 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.4300 - accuracy: 0.5058 - val_loss: 0.7971 - val_accuracy: 0.7092 - 1s/epoch - 55ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7775 - accuracy: 0.7076 - val_loss: 0.7367 - val_accuracy: 0.6990 - 478ms/epoch - 20ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6638 - accuracy: 0.7456 - val_loss: 0.7402 - val_accuracy: 0.7141 - 451ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5841 - accuracy: 0.7782 - val_loss: 0.5867 - val_accuracy: 0.7643 - 431ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5358 - accuracy: 0.7984 - val_loss: 0.4873 - val_accuracy: 0.8154 - 439ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4801 - accuracy: 0.8234 - val_loss: 0.4755 - val_accuracy: 0.8162 - 467ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4467 - accuracy: 0.8330 - val_loss: 0.4420 - val_accuracy: 0.8316 - 445ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4256 - accuracy: 0.8426 - val_loss: 0.4084 - val_accuracy: 0.8479 - 437ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.3965 - accuracy: 0.8528 - val_loss: 0.4304 - val_accuracy: 0.8361 - 427ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.3832 - accuracy: 0.8585 - val_loss: 0.3924 - val_accuracy: 0.8586 - 425ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3595 - accuracy: 0.8663 - val_loss: 0.3629 - val_accuracy: 0.8653 - 431ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3483 - accuracy: 0.8703 - val_loss: 0.3892 - val_accuracy: 0.8518 - 415ms/epoch - 17ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3418 - accuracy: 0.8731 - val_loss: 0.3651 - val_accuracy: 0.8632 - 445ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3212 - accuracy: 0.8817 - val_loss: 0.3453 - val_accuracy: 0.8696 - 421ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3094 - accuracy: 0.8839 - val_loss: 0.3436 - val_accuracy: 0.8713 - 446ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.2984 - accuracy: 0.8894 - val_loss: 0.3344 - val_accuracy: 0.8727 - 424ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.2896 - accuracy: 0.8924 - val_loss: 0.3533 - val_accuracy: 0.8662 - 414ms/epoch - 17ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.2774 - accuracy: 0.8973 - val_loss: 0.3131 - val_accuracy: 0.8831 - 430ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.2729 - accuracy: 0.8984 - val_loss: 0.3056 - val_accuracy: 0.8866 - 420ms/epoch - 18ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.2578 - accuracy: 0.9036 - val_loss: 0.3717 - val_accuracy: 0.8636 - 423ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.3086 - accuracy: 0.5351 - val_loss: 0.7872 - val_accuracy: 0.6937 - 1s/epoch - 54ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7322 - accuracy: 0.7191 - val_loss: 0.6406 - val_accuracy: 0.7638 - 428ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6378 - accuracy: 0.7644 - val_loss: 0.5703 - val_accuracy: 0.7888 - 444ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5658 - accuracy: 0.7872 - val_loss: 0.5610 - val_accuracy: 0.7886 - 438ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5187 - accuracy: 0.8053 - val_loss: 0.4853 - val_accuracy: 0.8205 - 441ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4781 - accuracy: 0.8248 - val_loss: 0.4886 - val_accuracy: 0.8205 - 431ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4500 - accuracy: 0.8355 - val_loss: 0.4532 - val_accuracy: 0.8349 - 422ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4263 - accuracy: 0.8450 - val_loss: 0.4602 - val_accuracy: 0.8302 - 435ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4101 - accuracy: 0.8493 - val_loss: 0.4435 - val_accuracy: 0.8351 - 439ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.3900 - accuracy: 0.8585 - val_loss: 0.4003 - val_accuracy: 0.8532 - 421ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3833 - accuracy: 0.8619 - val_loss: 0.4383 - val_accuracy: 0.8385 - 435ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3689 - accuracy: 0.8658 - val_loss: 0.4143 - val_accuracy: 0.8491 - 435ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3588 - accuracy: 0.8693 - val_loss: 0.4284 - val_accuracy: 0.8436 - 438ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3493 - accuracy: 0.8735 - val_loss: 0.3870 - val_accuracy: 0.8579 - 433ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3429 - accuracy: 0.8757 - val_loss: 0.3671 - val_accuracy: 0.8667 - 438ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3326 - accuracy: 0.8785 - val_loss: 0.4101 - val_accuracy: 0.8439 - 442ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3274 - accuracy: 0.8812 - val_loss: 0.3733 - val_accuracy: 0.8617 - 438ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3201 - accuracy: 0.8845 - val_loss: 0.3703 - val_accuracy: 0.8617 - 420ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3130 - accuracy: 0.8866 - val_loss: 0.3675 - val_accuracy: 0.8640 - 426ms/epoch - 18ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3077 - accuracy: 0.8875 - val_loss: 0.3919 - val_accuracy: 0.8600 - 427ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1241 - accuracy: 0.2501 - val_loss: 1.4152 - val_accuracy: 0.4484 - 1s/epoch - 198ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.5871 - accuracy: 0.4896 - val_loss: 1.0208 - val_accuracy: 0.6207 - 409ms/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1543 - accuracy: 0.5573 - val_loss: 0.9024 - val_accuracy: 0.6449 - 396ms/epoch - 66ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.9653 - accuracy: 0.6443 - val_loss: 0.7948 - val_accuracy: 0.6946 - 396ms/epoch - 66ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.0872 - accuracy: 0.6045 - val_loss: 0.7817 - val_accuracy: 0.7158 - 405ms/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8020 - accuracy: 0.7053 - val_loss: 0.7253 - val_accuracy: 0.7248 - 394ms/epoch - 66ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8033 - accuracy: 0.6886 - val_loss: 0.7269 - val_accuracy: 0.7262 - 404ms/epoch - 67ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7453 - accuracy: 0.7152 - val_loss: 0.6932 - val_accuracy: 0.7301 - 412ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.8105 - accuracy: 0.6922 - val_loss: 0.6740 - val_accuracy: 0.7396 - 406ms/epoch - 68ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6692 - accuracy: 0.7438 - val_loss: 0.6572 - val_accuracy: 0.7442 - 391ms/epoch - 65ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6776 - accuracy: 0.7397 - val_loss: 0.6856 - val_accuracy: 0.7253 - 418ms/epoch - 70ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6843 - accuracy: 0.7386 - val_loss: 0.7254 - val_accuracy: 0.7219 - 404ms/epoch - 67ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6438 - accuracy: 0.7554 - val_loss: 0.6047 - val_accuracy: 0.7801 - 410ms/epoch - 68ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6042 - accuracy: 0.7752 - val_loss: 0.6124 - val_accuracy: 0.7637 - 394ms/epoch - 66ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6815 - accuracy: 0.7404 - val_loss: 0.5607 - val_accuracy: 0.7752 - 393ms/epoch - 65ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5708 - accuracy: 0.7760 - val_loss: 0.5683 - val_accuracy: 0.7818 - 399ms/epoch - 66ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5514 - accuracy: 0.7878 - val_loss: 0.5514 - val_accuracy: 0.7920 - 394ms/epoch - 66ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5737 - accuracy: 0.7824 - val_loss: 0.5214 - val_accuracy: 0.8055 - 392ms/epoch - 65ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5330 - accuracy: 0.7960 - val_loss: 0.5242 - val_accuracy: 0.8018 - 398ms/epoch - 66ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5613 - accuracy: 0.7866 - val_loss: 0.6233 - val_accuracy: 0.7595 - 411ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.3873 - accuracy: 0.2755 - val_loss: 1.1910 - val_accuracy: 0.5774 - 1s/epoch - 214ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1151 - accuracy: 0.6061 - val_loss: 0.9420 - val_accuracy: 0.6581 - 391ms/epoch - 65ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.8877 - accuracy: 0.6802 - val_loss: 0.8741 - val_accuracy: 0.6836 - 416ms/epoch - 69ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8312 - accuracy: 0.6878 - val_loss: 0.7667 - val_accuracy: 0.7007 - 405ms/epoch - 67ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7257 - accuracy: 0.7269 - val_loss: 0.7317 - val_accuracy: 0.7280 - 409ms/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7197 - accuracy: 0.7257 - val_loss: 0.7064 - val_accuracy: 0.7336 - 404ms/epoch - 67ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7734 - accuracy: 0.7098 - val_loss: 0.7158 - val_accuracy: 0.7218 - 393ms/epoch - 66ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6377 - accuracy: 0.7606 - val_loss: 0.6155 - val_accuracy: 0.7622 - 425ms/epoch - 71ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6384 - accuracy: 0.7532 - val_loss: 0.5994 - val_accuracy: 0.7790 - 404ms/epoch - 67ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6071 - accuracy: 0.7715 - val_loss: 0.6138 - val_accuracy: 0.7631 - 416ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.5843 - accuracy: 0.7793 - val_loss: 0.5736 - val_accuracy: 0.7814 - 398ms/epoch - 66ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5919 - accuracy: 0.7740 - val_loss: 0.6871 - val_accuracy: 0.7616 - 394ms/epoch - 66ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5821 - accuracy: 0.7835 - val_loss: 0.5227 - val_accuracy: 0.8090 - 422ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5273 - accuracy: 0.8104 - val_loss: 0.5605 - val_accuracy: 0.7772 - 398ms/epoch - 66ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5653 - accuracy: 0.7857 - val_loss: 0.5322 - val_accuracy: 0.7954 - 392ms/epoch - 65ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.4934 - accuracy: 0.8190 - val_loss: 0.5438 - val_accuracy: 0.7915 - 415ms/epoch - 69ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5149 - accuracy: 0.8085 - val_loss: 0.5169 - val_accuracy: 0.8055 - 397ms/epoch - 66ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.4876 - accuracy: 0.8195 - val_loss: 0.4929 - val_accuracy: 0.8199 - 407ms/epoch - 68ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4954 - accuracy: 0.8177 - val_loss: 0.4915 - val_accuracy: 0.8221 - 398ms/epoch - 66ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4939 - accuracy: 0.8182 - val_loss: 0.5138 - val_accuracy: 0.8052 - 405ms/epoch - 68ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5006 - accuracy: 0.8117 - val_loss: 0.3970 - val_accuracy: 0.8537 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.3414 - accuracy: 0.8774 - val_loss: 0.3573 - val_accuracy: 0.8720 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3182 - accuracy: 0.8861 - val_loss: 0.3376 - val_accuracy: 0.8835 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 7s - loss: 0.3079 - accuracy: 0.8905 - val_loss: 0.3974 - val_accuracy: 0.8665 - 7s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3093 - accuracy: 0.8911 - val_loss: 0.3430 - val_accuracy: 0.8785 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3080 - accuracy: 0.8925 - val_loss: 0.3453 - val_accuracy: 0.8874 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3110 - accuracy: 0.8910 - val_loss: 0.3431 - val_accuracy: 0.8769 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3196 - accuracy: 0.8909 - val_loss: 0.3294 - val_accuracy: 0.8842 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3164 - accuracy: 0.8915 - val_loss: 0.3463 - val_accuracy: 0.8763 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3171 - accuracy: 0.8924 - val_loss: 0.4664 - val_accuracy: 0.8723 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3269 - accuracy: 0.8886 - val_loss: 0.3655 - val_accuracy: 0.8870 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3355 - accuracy: 0.8863 - val_loss: 0.3769 - val_accuracy: 0.8792 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3540 - accuracy: 0.8853 - val_loss: 0.3534 - val_accuracy: 0.8824 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.3428 - accuracy: 0.8864 - val_loss: 0.3772 - val_accuracy: 0.8692 - 7s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3420 - accuracy: 0.8846 - val_loss: 0.3570 - val_accuracy: 0.8774 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.3476 - accuracy: 0.8860 - val_loss: 0.4535 - val_accuracy: 0.8729 - 7s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3570 - accuracy: 0.8829 - val_loss: 0.3892 - val_accuracy: 0.8745 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3584 - accuracy: 0.8829 - val_loss: 0.3934 - val_accuracy: 0.8615 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3661 - accuracy: 0.8823 - val_loss: 0.4264 - val_accuracy: 0.8757 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3653 - accuracy: 0.8805 - val_loss: 0.3606 - val_accuracy: 0.8683 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5244 - accuracy: 0.8086 - val_loss: 0.4310 - val_accuracy: 0.8434 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.3840 - accuracy: 0.8609 - val_loss: 0.4124 - val_accuracy: 0.8499 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3476 - accuracy: 0.8748 - val_loss: 0.3675 - val_accuracy: 0.8692 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3236 - accuracy: 0.8832 - val_loss: 0.3640 - val_accuracy: 0.8707 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3070 - accuracy: 0.8907 - val_loss: 0.3747 - val_accuracy: 0.8621 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.2921 - accuracy: 0.8948 - val_loss: 0.3509 - val_accuracy: 0.8737 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.2827 - accuracy: 0.8983 - val_loss: 0.3461 - val_accuracy: 0.8764 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.2710 - accuracy: 0.9028 - val_loss: 0.3560 - val_accuracy: 0.8743 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.2598 - accuracy: 0.9063 - val_loss: 0.3434 - val_accuracy: 0.8805 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.2521 - accuracy: 0.9093 - val_loss: 0.3421 - val_accuracy: 0.8817 - 7s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.2483 - accuracy: 0.9105 - val_loss: 0.3347 - val_accuracy: 0.8828 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.2390 - accuracy: 0.9143 - val_loss: 0.3565 - val_accuracy: 0.8775 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.2344 - accuracy: 0.9156 - val_loss: 0.3300 - val_accuracy: 0.8895 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.2269 - accuracy: 0.9195 - val_loss: 0.3449 - val_accuracy: 0.8843 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2212 - accuracy: 0.9210 - val_loss: 0.3490 - val_accuracy: 0.8842 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2138 - accuracy: 0.9230 - val_loss: 0.3638 - val_accuracy: 0.8831 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.2081 - accuracy: 0.9263 - val_loss: 0.3522 - val_accuracy: 0.8818 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2041 - accuracy: 0.9279 - val_loss: 0.3422 - val_accuracy: 0.8868 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2012 - accuracy: 0.9288 - val_loss: 0.3597 - val_accuracy: 0.8843 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.1946 - accuracy: 0.9309 - val_loss: 0.3567 - val_accuracy: 0.8865 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.6493 - accuracy: 0.4183 - val_loss: 1.2829 - val_accuracy: 0.4857 - 1s/epoch - 56ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.9627 - accuracy: 0.6343 - val_loss: 0.8025 - val_accuracy: 0.6938 - 451ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.8098 - accuracy: 0.6908 - val_loss: 0.7019 - val_accuracy: 0.7207 - 445ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6898 - accuracy: 0.7323 - val_loss: 0.6299 - val_accuracy: 0.7638 - 462ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6273 - accuracy: 0.7576 - val_loss: 0.5689 - val_accuracy: 0.7732 - 465ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5657 - accuracy: 0.7831 - val_loss: 0.5799 - val_accuracy: 0.7663 - 461ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5353 - accuracy: 0.7963 - val_loss: 0.5602 - val_accuracy: 0.7869 - 442ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5010 - accuracy: 0.8123 - val_loss: 0.4753 - val_accuracy: 0.8222 - 456ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4766 - accuracy: 0.8220 - val_loss: 0.4603 - val_accuracy: 0.8286 - 444ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4506 - accuracy: 0.8329 - val_loss: 0.4465 - val_accuracy: 0.8401 - 443ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4367 - accuracy: 0.8365 - val_loss: 0.4291 - val_accuracy: 0.8359 - 444ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4101 - accuracy: 0.8480 - val_loss: 0.3917 - val_accuracy: 0.8544 - 447ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4002 - accuracy: 0.8538 - val_loss: 0.4133 - val_accuracy: 0.8450 - 445ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3861 - accuracy: 0.8585 - val_loss: 0.4039 - val_accuracy: 0.8500 - 444ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3766 - accuracy: 0.8602 - val_loss: 0.3643 - val_accuracy: 0.8638 - 457ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3620 - accuracy: 0.8664 - val_loss: 0.3887 - val_accuracy: 0.8547 - 467ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3516 - accuracy: 0.8689 - val_loss: 0.3953 - val_accuracy: 0.8579 - 444ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3503 - accuracy: 0.8697 - val_loss: 0.3568 - val_accuracy: 0.8683 - 449ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3349 - accuracy: 0.8758 - val_loss: 0.3500 - val_accuracy: 0.8707 - 458ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3291 - accuracy: 0.8784 - val_loss: 0.3510 - val_accuracy: 0.8676 - 460ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.3478 - accuracy: 0.5240 - val_loss: 0.9322 - val_accuracy: 0.6216 - 1s/epoch - 54ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7930 - accuracy: 0.6885 - val_loss: 0.6704 - val_accuracy: 0.7518 - 465ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6614 - accuracy: 0.7485 - val_loss: 0.5990 - val_accuracy: 0.7748 - 458ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5779 - accuracy: 0.7831 - val_loss: 0.5732 - val_accuracy: 0.7881 - 443ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5331 - accuracy: 0.8018 - val_loss: 0.5087 - val_accuracy: 0.8166 - 453ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4945 - accuracy: 0.8189 - val_loss: 0.4898 - val_accuracy: 0.8145 - 449ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4667 - accuracy: 0.8299 - val_loss: 0.4947 - val_accuracy: 0.8182 - 446ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4412 - accuracy: 0.8385 - val_loss: 0.4226 - val_accuracy: 0.8482 - 437ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4215 - accuracy: 0.8442 - val_loss: 0.4383 - val_accuracy: 0.8371 - 443ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4026 - accuracy: 0.8519 - val_loss: 0.4095 - val_accuracy: 0.8489 - 459ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3881 - accuracy: 0.8583 - val_loss: 0.4322 - val_accuracy: 0.8428 - 466ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3753 - accuracy: 0.8637 - val_loss: 0.4278 - val_accuracy: 0.8398 - 446ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3611 - accuracy: 0.8685 - val_loss: 0.3783 - val_accuracy: 0.8632 - 460ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3526 - accuracy: 0.8706 - val_loss: 0.3777 - val_accuracy: 0.8591 - 441ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3362 - accuracy: 0.8776 - val_loss: 0.3917 - val_accuracy: 0.8558 - 467ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3313 - accuracy: 0.8787 - val_loss: 0.3733 - val_accuracy: 0.8623 - 458ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3229 - accuracy: 0.8820 - val_loss: 0.3615 - val_accuracy: 0.8676 - 439ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3141 - accuracy: 0.8844 - val_loss: 0.3663 - val_accuracy: 0.8632 - 466ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3038 - accuracy: 0.8879 - val_loss: 0.3529 - val_accuracy: 0.8710 - 445ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.2985 - accuracy: 0.8906 - val_loss: 0.3390 - val_accuracy: 0.8731 - 451ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.2289 - accuracy: 0.2079 - val_loss: 1.6760 - val_accuracy: 0.4257 - 1s/epoch - 205ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.6536 - accuracy: 0.4285 - val_loss: 1.6521 - val_accuracy: 0.4200 - 435ms/epoch - 72ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.3426 - accuracy: 0.5133 - val_loss: 1.1696 - val_accuracy: 0.5678 - 428ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.9346 - accuracy: 0.6682 - val_loss: 0.9659 - val_accuracy: 0.6559 - 419ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9331 - accuracy: 0.6577 - val_loss: 0.8842 - val_accuracy: 0.6338 - 434ms/epoch - 72ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8347 - accuracy: 0.6683 - val_loss: 0.7712 - val_accuracy: 0.7052 - 422ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8349 - accuracy: 0.6788 - val_loss: 0.6925 - val_accuracy: 0.7392 - 412ms/epoch - 69ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7502 - accuracy: 0.7125 - val_loss: 0.8079 - val_accuracy: 0.6841 - 434ms/epoch - 72ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7476 - accuracy: 0.7048 - val_loss: 0.6832 - val_accuracy: 0.7254 - 413ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7871 - accuracy: 0.6896 - val_loss: 0.6624 - val_accuracy: 0.7493 - 416ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6668 - accuracy: 0.7362 - val_loss: 0.6892 - val_accuracy: 0.7420 - 405ms/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6455 - accuracy: 0.7499 - val_loss: 0.7838 - val_accuracy: 0.7106 - 414ms/epoch - 69ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.7138 - accuracy: 0.7248 - val_loss: 0.6582 - val_accuracy: 0.7488 - 418ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6516 - accuracy: 0.7453 - val_loss: 0.6554 - val_accuracy: 0.7354 - 441ms/epoch - 73ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6141 - accuracy: 0.7504 - val_loss: 0.6135 - val_accuracy: 0.7614 - 433ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6165 - accuracy: 0.7657 - val_loss: 0.5815 - val_accuracy: 0.7737 - 412ms/epoch - 69ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5837 - accuracy: 0.7750 - val_loss: 0.6227 - val_accuracy: 0.7469 - 440ms/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6314 - accuracy: 0.7472 - val_loss: 0.5702 - val_accuracy: 0.7831 - 439ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5737 - accuracy: 0.7744 - val_loss: 0.5663 - val_accuracy: 0.7814 - 416ms/epoch - 69ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5710 - accuracy: 0.7782 - val_loss: 0.5300 - val_accuracy: 0.7952 - 408ms/epoch - 68ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8402 - accuracy: 0.3245 - val_loss: 1.3373 - val_accuracy: 0.5404 - 1s/epoch - 215ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.0736 - accuracy: 0.6167 - val_loss: 0.9708 - val_accuracy: 0.6308 - 415ms/epoch - 69ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9285 - accuracy: 0.6425 - val_loss: 0.8430 - val_accuracy: 0.6815 - 426ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8383 - accuracy: 0.6756 - val_loss: 0.8731 - val_accuracy: 0.6658 - 415ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7820 - accuracy: 0.6948 - val_loss: 0.7459 - val_accuracy: 0.7028 - 430ms/epoch - 72ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7398 - accuracy: 0.7165 - val_loss: 0.6644 - val_accuracy: 0.7521 - 433ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7131 - accuracy: 0.7214 - val_loss: 0.6803 - val_accuracy: 0.7289 - 426ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6743 - accuracy: 0.7409 - val_loss: 0.6202 - val_accuracy: 0.7684 - 413ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6032 - accuracy: 0.7768 - val_loss: 0.6353 - val_accuracy: 0.7615 - 427ms/epoch - 71ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6423 - accuracy: 0.7533 - val_loss: 0.6612 - val_accuracy: 0.7496 - 437ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6236 - accuracy: 0.7675 - val_loss: 0.5851 - val_accuracy: 0.7738 - 413ms/epoch - 69ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5721 - accuracy: 0.7808 - val_loss: 0.5983 - val_accuracy: 0.7674 - 416ms/epoch - 69ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5826 - accuracy: 0.7780 - val_loss: 0.5809 - val_accuracy: 0.7749 - 435ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5409 - accuracy: 0.7960 - val_loss: 0.5763 - val_accuracy: 0.7872 - 414ms/epoch - 69ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5734 - accuracy: 0.7888 - val_loss: 0.5477 - val_accuracy: 0.7973 - 434ms/epoch - 72ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5406 - accuracy: 0.7963 - val_loss: 0.5580 - val_accuracy: 0.7910 - 420ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5125 - accuracy: 0.8155 - val_loss: 0.5617 - val_accuracy: 0.7814 - 426ms/epoch - 71ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5283 - accuracy: 0.8024 - val_loss: 0.5488 - val_accuracy: 0.7967 - 423ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5026 - accuracy: 0.8128 - val_loss: 0.5268 - val_accuracy: 0.8096 - 416ms/epoch - 69ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5029 - accuracy: 0.8122 - val_loss: 0.5167 - val_accuracy: 0.8070 - 420ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.5991 - accuracy: 0.7760 - val_loss: 0.4247 - val_accuracy: 0.8448 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.4143 - accuracy: 0.8474 - val_loss: 0.3678 - val_accuracy: 0.8637 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 7s - loss: 0.3692 - accuracy: 0.8653 - val_loss: 0.3336 - val_accuracy: 0.8778 - 7s/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 7s - loss: 0.3441 - accuracy: 0.8770 - val_loss: 0.3251 - val_accuracy: 0.8795 - 7s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3268 - accuracy: 0.8827 - val_loss: 0.3376 - val_accuracy: 0.8824 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 7s - loss: 0.3137 - accuracy: 0.8878 - val_loss: 0.3116 - val_accuracy: 0.8894 - 7s/epoch - 5ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.3033 - accuracy: 0.8913 - val_loss: 0.2941 - val_accuracy: 0.8938 - 7s/epoch - 5ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.2972 - accuracy: 0.8941 - val_loss: 0.2872 - val_accuracy: 0.8998 - 7s/epoch - 5ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.2885 - accuracy: 0.8962 - val_loss: 0.2800 - val_accuracy: 0.9022 - 7s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.2871 - accuracy: 0.8981 - val_loss: 0.2923 - val_accuracy: 0.8967 - 7s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 7s - loss: 0.2814 - accuracy: 0.8996 - val_loss: 0.3034 - val_accuracy: 0.8972 - 7s/epoch - 5ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.2764 - accuracy: 0.9018 - val_loss: 0.2905 - val_accuracy: 0.8962 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.2763 - accuracy: 0.9031 - val_loss: 0.2797 - val_accuracy: 0.9006 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.2736 - accuracy: 0.9028 - val_loss: 0.2840 - val_accuracy: 0.8980 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2732 - accuracy: 0.9029 - val_loss: 0.2837 - val_accuracy: 0.9008 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.2700 - accuracy: 0.9057 - val_loss: 0.2780 - val_accuracy: 0.9037 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.2670 - accuracy: 0.9068 - val_loss: 0.3048 - val_accuracy: 0.9019 - 7s/epoch - 5ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.2663 - accuracy: 0.9064 - val_loss: 0.2844 - val_accuracy: 0.8962 - 7s/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 7s - loss: 0.2674 - accuracy: 0.9053 - val_loss: 0.2800 - val_accuracy: 0.9024 - 7s/epoch - 5ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.2702 - accuracy: 0.9058 - val_loss: 0.2992 - val_accuracy: 0.8982 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.5760 - accuracy: 0.7872 - val_loss: 0.4466 - val_accuracy: 0.8326 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.4368 - accuracy: 0.8424 - val_loss: 0.4154 - val_accuracy: 0.8460 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4019 - accuracy: 0.8537 - val_loss: 0.3891 - val_accuracy: 0.8578 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3809 - accuracy: 0.8621 - val_loss: 0.3653 - val_accuracy: 0.8697 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3689 - accuracy: 0.8682 - val_loss: 0.3645 - val_accuracy: 0.8679 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3617 - accuracy: 0.8694 - val_loss: 0.3664 - val_accuracy: 0.8607 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3514 - accuracy: 0.8755 - val_loss: 0.3482 - val_accuracy: 0.8727 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3502 - accuracy: 0.8747 - val_loss: 0.3509 - val_accuracy: 0.8732 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.3433 - accuracy: 0.8768 - val_loss: 0.3484 - val_accuracy: 0.8734 - 7s/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 8s - loss: 0.3399 - accuracy: 0.8773 - val_loss: 0.3599 - val_accuracy: 0.8652 - 8s/epoch - 5ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 7s - loss: 0.3355 - accuracy: 0.8788 - val_loss: 0.3562 - val_accuracy: 0.8685 - 7s/epoch - 5ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.3309 - accuracy: 0.8837 - val_loss: 0.3382 - val_accuracy: 0.8755 - 7s/epoch - 5ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.3315 - accuracy: 0.8820 - val_loss: 0.3338 - val_accuracy: 0.8788 - 7s/epoch - 5ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.3282 - accuracy: 0.8842 - val_loss: 0.3373 - val_accuracy: 0.8764 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3252 - accuracy: 0.8845 - val_loss: 0.3320 - val_accuracy: 0.8806 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3243 - accuracy: 0.8852 - val_loss: 0.3411 - val_accuracy: 0.8767 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3199 - accuracy: 0.8858 - val_loss: 0.3440 - val_accuracy: 0.8764 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3175 - accuracy: 0.8878 - val_loss: 0.3238 - val_accuracy: 0.8815 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3172 - accuracy: 0.8876 - val_loss: 0.3378 - val_accuracy: 0.8795 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3141 - accuracy: 0.8877 - val_loss: 0.3219 - val_accuracy: 0.8843 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.4378 - accuracy: 0.5106 - val_loss: 0.9572 - val_accuracy: 0.6699 - 1s/epoch - 57ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8735 - accuracy: 0.6726 - val_loss: 0.7826 - val_accuracy: 0.7044 - 447ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7656 - accuracy: 0.7085 - val_loss: 0.7414 - val_accuracy: 0.7116 - 450ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.7043 - accuracy: 0.7321 - val_loss: 0.6278 - val_accuracy: 0.7613 - 444ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6468 - accuracy: 0.7539 - val_loss: 0.6096 - val_accuracy: 0.7703 - 453ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6146 - accuracy: 0.7680 - val_loss: 0.6281 - val_accuracy: 0.7669 - 458ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5814 - accuracy: 0.7823 - val_loss: 0.5837 - val_accuracy: 0.7829 - 444ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5564 - accuracy: 0.7928 - val_loss: 0.6007 - val_accuracy: 0.7825 - 451ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5388 - accuracy: 0.7987 - val_loss: 0.5521 - val_accuracy: 0.7915 - 439ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5144 - accuracy: 0.8093 - val_loss: 0.4940 - val_accuracy: 0.8170 - 451ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4978 - accuracy: 0.8160 - val_loss: 0.4743 - val_accuracy: 0.8266 - 449ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4818 - accuracy: 0.8212 - val_loss: 0.4590 - val_accuracy: 0.8299 - 450ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4673 - accuracy: 0.8282 - val_loss: 0.4599 - val_accuracy: 0.8311 - 466ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4589 - accuracy: 0.8322 - val_loss: 0.4520 - val_accuracy: 0.8326 - 436ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4466 - accuracy: 0.8352 - val_loss: 0.4380 - val_accuracy: 0.8414 - 445ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4331 - accuracy: 0.8413 - val_loss: 0.4220 - val_accuracy: 0.8450 - 454ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4260 - accuracy: 0.8433 - val_loss: 0.4172 - val_accuracy: 0.8423 - 448ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4199 - accuracy: 0.8453 - val_loss: 0.4012 - val_accuracy: 0.8514 - 444ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4072 - accuracy: 0.8512 - val_loss: 0.4001 - val_accuracy: 0.8578 - 445ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4033 - accuracy: 0.8521 - val_loss: 0.3885 - val_accuracy: 0.8581 - 438ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.1881 - accuracy: 0.5790 - val_loss: 0.7959 - val_accuracy: 0.7131 - 1s/epoch - 53ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7374 - accuracy: 0.7240 - val_loss: 0.6586 - val_accuracy: 0.7553 - 465ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6474 - accuracy: 0.7542 - val_loss: 0.6122 - val_accuracy: 0.7720 - 456ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5944 - accuracy: 0.7789 - val_loss: 0.5644 - val_accuracy: 0.7919 - 460ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5596 - accuracy: 0.7911 - val_loss: 0.5866 - val_accuracy: 0.7724 - 454ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5281 - accuracy: 0.8071 - val_loss: 0.5223 - val_accuracy: 0.8025 - 453ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5092 - accuracy: 0.8141 - val_loss: 0.5126 - val_accuracy: 0.8016 - 455ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4896 - accuracy: 0.8208 - val_loss: 0.4782 - val_accuracy: 0.8261 - 452ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4713 - accuracy: 0.8299 - val_loss: 0.4663 - val_accuracy: 0.8304 - 434ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4555 - accuracy: 0.8363 - val_loss: 0.4539 - val_accuracy: 0.8323 - 457ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4461 - accuracy: 0.8388 - val_loss: 0.4386 - val_accuracy: 0.8400 - 457ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4348 - accuracy: 0.8423 - val_loss: 0.4248 - val_accuracy: 0.8480 - 454ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4253 - accuracy: 0.8456 - val_loss: 0.4215 - val_accuracy: 0.8437 - 451ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4135 - accuracy: 0.8512 - val_loss: 0.4141 - val_accuracy: 0.8482 - 456ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4072 - accuracy: 0.8540 - val_loss: 0.4113 - val_accuracy: 0.8465 - 443ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3998 - accuracy: 0.8558 - val_loss: 0.4043 - val_accuracy: 0.8517 - 461ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3956 - accuracy: 0.8579 - val_loss: 0.3974 - val_accuracy: 0.8546 - 449ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3886 - accuracy: 0.8599 - val_loss: 0.3877 - val_accuracy: 0.8572 - 446ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3807 - accuracy: 0.8634 - val_loss: 0.3823 - val_accuracy: 0.8609 - 447ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3757 - accuracy: 0.8648 - val_loss: 0.3797 - val_accuracy: 0.8634 - 455ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0870 - accuracy: 0.2217 - val_loss: 1.6407 - val_accuracy: 0.5272 - 1s/epoch - 219ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.4735 - accuracy: 0.5376 - val_loss: 1.2045 - val_accuracy: 0.6165 - 411ms/epoch - 69ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1207 - accuracy: 0.6057 - val_loss: 0.9573 - val_accuracy: 0.6488 - 414ms/epoch - 69ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.0124 - accuracy: 0.6358 - val_loss: 0.9010 - val_accuracy: 0.6826 - 416ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9058 - accuracy: 0.6786 - val_loss: 0.8650 - val_accuracy: 0.6488 - 420ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8452 - accuracy: 0.6832 - val_loss: 0.7911 - val_accuracy: 0.6892 - 415ms/epoch - 69ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8274 - accuracy: 0.6893 - val_loss: 0.7628 - val_accuracy: 0.7114 - 428ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7845 - accuracy: 0.7068 - val_loss: 0.8673 - val_accuracy: 0.6755 - 417ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7713 - accuracy: 0.7104 - val_loss: 0.7493 - val_accuracy: 0.7273 - 440ms/epoch - 73ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7285 - accuracy: 0.7225 - val_loss: 0.7145 - val_accuracy: 0.7355 - 419ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.7081 - accuracy: 0.7308 - val_loss: 0.6941 - val_accuracy: 0.7392 - 428ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7265 - accuracy: 0.7284 - val_loss: 0.7525 - val_accuracy: 0.7070 - 431ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6774 - accuracy: 0.7402 - val_loss: 0.6978 - val_accuracy: 0.7332 - 436ms/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6622 - accuracy: 0.7479 - val_loss: 0.6416 - val_accuracy: 0.7543 - 433ms/epoch - 72ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6651 - accuracy: 0.7488 - val_loss: 0.6370 - val_accuracy: 0.7528 - 417ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6430 - accuracy: 0.7505 - val_loss: 0.6497 - val_accuracy: 0.7420 - 422ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.6282 - accuracy: 0.7606 - val_loss: 0.6127 - val_accuracy: 0.7690 - 446ms/epoch - 74ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6229 - accuracy: 0.7672 - val_loss: 0.6338 - val_accuracy: 0.7519 - 436ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6254 - accuracy: 0.7592 - val_loss: 0.5936 - val_accuracy: 0.7753 - 428ms/epoch - 71ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5932 - accuracy: 0.7750 - val_loss: 0.5679 - val_accuracy: 0.7885 - 430ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8287 - accuracy: 0.3824 - val_loss: 1.2528 - val_accuracy: 0.5743 - 1s/epoch - 208ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1556 - accuracy: 0.5783 - val_loss: 1.0099 - val_accuracy: 0.6202 - 417ms/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9410 - accuracy: 0.6503 - val_loss: 0.8616 - val_accuracy: 0.6993 - 419ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8641 - accuracy: 0.6819 - val_loss: 0.8912 - val_accuracy: 0.6456 - 427ms/epoch - 71ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7883 - accuracy: 0.7072 - val_loss: 0.7345 - val_accuracy: 0.7350 - 431ms/epoch - 72ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7700 - accuracy: 0.7063 - val_loss: 0.7409 - val_accuracy: 0.7081 - 435ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7171 - accuracy: 0.7297 - val_loss: 0.7075 - val_accuracy: 0.7355 - 427ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6930 - accuracy: 0.7424 - val_loss: 0.6706 - val_accuracy: 0.7534 - 420ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6837 - accuracy: 0.7426 - val_loss: 0.6297 - val_accuracy: 0.7676 - 422ms/epoch - 70ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6465 - accuracy: 0.7570 - val_loss: 0.6375 - val_accuracy: 0.7548 - 420ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6266 - accuracy: 0.7686 - val_loss: 0.6396 - val_accuracy: 0.7613 - 423ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6379 - accuracy: 0.7630 - val_loss: 0.5901 - val_accuracy: 0.7765 - 440ms/epoch - 73ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5981 - accuracy: 0.7787 - val_loss: 0.5886 - val_accuracy: 0.7808 - 420ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5857 - accuracy: 0.7817 - val_loss: 0.6102 - val_accuracy: 0.7671 - 417ms/epoch - 69ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5920 - accuracy: 0.7771 - val_loss: 0.5574 - val_accuracy: 0.7959 - 423ms/epoch - 71ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5915 - accuracy: 0.7796 - val_loss: 0.5566 - val_accuracy: 0.7938 - 432ms/epoch - 72ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5484 - accuracy: 0.8000 - val_loss: 0.5481 - val_accuracy: 0.7972 - 433ms/epoch - 72ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5665 - accuracy: 0.7861 - val_loss: 0.5367 - val_accuracy: 0.8013 - 410ms/epoch - 68ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5418 - accuracy: 0.7981 - val_loss: 0.5412 - val_accuracy: 0.8020 - 430ms/epoch - 72ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5389 - accuracy: 0.8042 - val_loss: 0.5418 - val_accuracy: 0.7981 - 416ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.7243 - accuracy: 0.7273 - val_loss: 0.4850 - val_accuracy: 0.8204 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4914 - accuracy: 0.8234 - val_loss: 0.4020 - val_accuracy: 0.8548 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4334 - accuracy: 0.8479 - val_loss: 0.3825 - val_accuracy: 0.8604 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.3960 - accuracy: 0.8617 - val_loss: 0.3585 - val_accuracy: 0.8720 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3735 - accuracy: 0.8695 - val_loss: 0.3455 - val_accuracy: 0.8752 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3630 - accuracy: 0.8737 - val_loss: 0.3434 - val_accuracy: 0.8743 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3541 - accuracy: 0.8797 - val_loss: 0.3578 - val_accuracy: 0.8747 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3516 - accuracy: 0.8786 - val_loss: 0.3391 - val_accuracy: 0.8798 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.3461 - accuracy: 0.8824 - val_loss: 0.3326 - val_accuracy: 0.8799 - 7s/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3475 - accuracy: 0.8803 - val_loss: 0.3472 - val_accuracy: 0.8822 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3478 - accuracy: 0.8827 - val_loss: 0.3199 - val_accuracy: 0.8868 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3486 - accuracy: 0.8841 - val_loss: 0.3511 - val_accuracy: 0.8820 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3472 - accuracy: 0.8823 - val_loss: 0.3227 - val_accuracy: 0.8815 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.3542 - accuracy: 0.8815 - val_loss: 0.3514 - val_accuracy: 0.8828 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3591 - accuracy: 0.8787 - val_loss: 0.3483 - val_accuracy: 0.8758 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3667 - accuracy: 0.8793 - val_loss: 0.3615 - val_accuracy: 0.8714 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3667 - accuracy: 0.8788 - val_loss: 0.4206 - val_accuracy: 0.8703 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3717 - accuracy: 0.8786 - val_loss: 0.3368 - val_accuracy: 0.8791 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3801 - accuracy: 0.8741 - val_loss: 0.3577 - val_accuracy: 0.8725 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3855 - accuracy: 0.8724 - val_loss: 0.3471 - val_accuracy: 0.8754 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6106 - accuracy: 0.7768 - val_loss: 0.4631 - val_accuracy: 0.8361 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.4695 - accuracy: 0.8331 - val_loss: 0.4216 - val_accuracy: 0.8454 - 7s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4285 - accuracy: 0.8476 - val_loss: 0.4142 - val_accuracy: 0.8508 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4025 - accuracy: 0.8564 - val_loss: 0.3835 - val_accuracy: 0.8626 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3846 - accuracy: 0.8632 - val_loss: 0.3625 - val_accuracy: 0.8669 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3678 - accuracy: 0.8699 - val_loss: 0.3555 - val_accuracy: 0.8713 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3601 - accuracy: 0.8724 - val_loss: 0.3478 - val_accuracy: 0.8723 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3439 - accuracy: 0.8778 - val_loss: 0.3372 - val_accuracy: 0.8743 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3363 - accuracy: 0.8802 - val_loss: 0.3370 - val_accuracy: 0.8754 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3287 - accuracy: 0.8835 - val_loss: 0.3388 - val_accuracy: 0.8798 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3229 - accuracy: 0.8854 - val_loss: 0.3285 - val_accuracy: 0.8821 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.3119 - accuracy: 0.8890 - val_loss: 0.3214 - val_accuracy: 0.8843 - 7s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3075 - accuracy: 0.8912 - val_loss: 0.3271 - val_accuracy: 0.8823 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.3030 - accuracy: 0.8929 - val_loss: 0.3191 - val_accuracy: 0.8848 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.2969 - accuracy: 0.8954 - val_loss: 0.3133 - val_accuracy: 0.8888 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.2913 - accuracy: 0.8966 - val_loss: 0.3160 - val_accuracy: 0.8868 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.2893 - accuracy: 0.8978 - val_loss: 0.3153 - val_accuracy: 0.8863 - 7s/epoch - 5ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.2863 - accuracy: 0.8999 - val_loss: 0.3115 - val_accuracy: 0.8908 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.2814 - accuracy: 0.9011 - val_loss: 0.3124 - val_accuracy: 0.8901 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.2783 - accuracy: 0.9021 - val_loss: 0.3159 - val_accuracy: 0.8914 - 7s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.6127 - accuracy: 0.3998 - val_loss: 1.0271 - val_accuracy: 0.6020 - 1s/epoch - 57ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 1.0548 - accuracy: 0.5982 - val_loss: 0.8587 - val_accuracy: 0.6889 - 447ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.9132 - accuracy: 0.6582 - val_loss: 0.7426 - val_accuracy: 0.7071 - 449ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.8281 - accuracy: 0.6864 - val_loss: 0.7147 - val_accuracy: 0.7347 - 461ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.7794 - accuracy: 0.7033 - val_loss: 0.6604 - val_accuracy: 0.7452 - 443ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.7338 - accuracy: 0.7205 - val_loss: 0.6239 - val_accuracy: 0.7581 - 446ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.6974 - accuracy: 0.7345 - val_loss: 0.6527 - val_accuracy: 0.7553 - 459ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.6802 - accuracy: 0.7410 - val_loss: 0.6306 - val_accuracy: 0.7455 - 446ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.6545 - accuracy: 0.7503 - val_loss: 0.6044 - val_accuracy: 0.7551 - 464ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.6254 - accuracy: 0.7628 - val_loss: 0.5627 - val_accuracy: 0.7854 - 471ms/epoch - 20ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.6094 - accuracy: 0.7700 - val_loss: 0.5899 - val_accuracy: 0.7681 - 456ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5951 - accuracy: 0.7760 - val_loss: 0.5436 - val_accuracy: 0.7967 - 461ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.5733 - accuracy: 0.7849 - val_loss: 0.5182 - val_accuracy: 0.7994 - 452ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.5650 - accuracy: 0.7894 - val_loss: 0.5765 - val_accuracy: 0.7894 - 461ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.5515 - accuracy: 0.7947 - val_loss: 0.5312 - val_accuracy: 0.7814 - 469ms/epoch - 20ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.5387 - accuracy: 0.8001 - val_loss: 0.5118 - val_accuracy: 0.8093 - 443ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.5262 - accuracy: 0.8052 - val_loss: 0.4935 - val_accuracy: 0.8161 - 468ms/epoch - 20ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.5197 - accuracy: 0.8113 - val_loss: 0.4634 - val_accuracy: 0.8245 - 461ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.5005 - accuracy: 0.8168 - val_loss: 0.4665 - val_accuracy: 0.8259 - 460ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4966 - accuracy: 0.8201 - val_loss: 0.4479 - val_accuracy: 0.8290 - 450ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.2691 - accuracy: 0.5572 - val_loss: 0.8346 - val_accuracy: 0.6744 - 1s/epoch - 54ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7981 - accuracy: 0.7003 - val_loss: 0.7402 - val_accuracy: 0.7216 - 462ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6859 - accuracy: 0.7435 - val_loss: 0.7072 - val_accuracy: 0.7321 - 453ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6381 - accuracy: 0.7619 - val_loss: 0.6755 - val_accuracy: 0.7399 - 452ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6085 - accuracy: 0.7753 - val_loss: 0.5973 - val_accuracy: 0.7811 - 465ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5804 - accuracy: 0.7851 - val_loss: 0.5576 - val_accuracy: 0.7919 - 446ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5590 - accuracy: 0.7955 - val_loss: 0.5621 - val_accuracy: 0.7901 - 453ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5442 - accuracy: 0.8013 - val_loss: 0.5124 - val_accuracy: 0.8074 - 447ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5296 - accuracy: 0.8071 - val_loss: 0.5121 - val_accuracy: 0.8127 - 467ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5162 - accuracy: 0.8117 - val_loss: 0.4912 - val_accuracy: 0.8214 - 462ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5046 - accuracy: 0.8180 - val_loss: 0.5184 - val_accuracy: 0.8125 - 451ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4960 - accuracy: 0.8216 - val_loss: 0.4813 - val_accuracy: 0.8209 - 472ms/epoch - 20ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4875 - accuracy: 0.8235 - val_loss: 0.4750 - val_accuracy: 0.8291 - 469ms/epoch - 20ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4804 - accuracy: 0.8277 - val_loss: 0.4609 - val_accuracy: 0.8301 - 468ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4712 - accuracy: 0.8309 - val_loss: 0.4695 - val_accuracy: 0.8270 - 455ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4644 - accuracy: 0.8321 - val_loss: 0.4557 - val_accuracy: 0.8324 - 459ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4588 - accuracy: 0.8340 - val_loss: 0.4324 - val_accuracy: 0.8435 - 463ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4480 - accuracy: 0.8405 - val_loss: 0.4486 - val_accuracy: 0.8324 - 462ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4430 - accuracy: 0.8395 - val_loss: 0.4369 - val_accuracy: 0.8391 - 446ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4406 - accuracy: 0.8430 - val_loss: 0.4392 - val_accuracy: 0.8393 - 462ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1274 - accuracy: 0.2398 - val_loss: 1.7921 - val_accuracy: 0.3868 - 1s/epoch - 209ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.6528 - accuracy: 0.4053 - val_loss: 1.2923 - val_accuracy: 0.5585 - 428ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.3496 - accuracy: 0.5055 - val_loss: 1.1293 - val_accuracy: 0.5747 - 440ms/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.1803 - accuracy: 0.5576 - val_loss: 1.0025 - val_accuracy: 0.6117 - 422ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.1131 - accuracy: 0.5857 - val_loss: 1.0028 - val_accuracy: 0.6220 - 426ms/epoch - 71ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 1.0369 - accuracy: 0.6103 - val_loss: 0.8976 - val_accuracy: 0.6553 - 445ms/epoch - 74ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.9895 - accuracy: 0.6315 - val_loss: 0.8467 - val_accuracy: 0.6866 - 425ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.9751 - accuracy: 0.6306 - val_loss: 0.8224 - val_accuracy: 0.6796 - 445ms/epoch - 74ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.9106 - accuracy: 0.6609 - val_loss: 0.7900 - val_accuracy: 0.7031 - 434ms/epoch - 72ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.8380 - accuracy: 0.6879 - val_loss: 0.8963 - val_accuracy: 0.6546 - 418ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.8945 - accuracy: 0.6635 - val_loss: 0.7687 - val_accuracy: 0.7067 - 414ms/epoch - 69ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.8304 - accuracy: 0.6872 - val_loss: 0.7123 - val_accuracy: 0.7293 - 422ms/epoch - 70ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.8177 - accuracy: 0.6862 - val_loss: 0.7506 - val_accuracy: 0.7264 - 434ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.8020 - accuracy: 0.6966 - val_loss: 0.7232 - val_accuracy: 0.7177 - 423ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.7862 - accuracy: 0.7005 - val_loss: 0.6970 - val_accuracy: 0.7241 - 437ms/epoch - 73ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.7628 - accuracy: 0.7091 - val_loss: 0.8345 - val_accuracy: 0.6724 - 441ms/epoch - 74ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.7710 - accuracy: 0.7081 - val_loss: 0.7002 - val_accuracy: 0.7297 - 413ms/epoch - 69ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.7466 - accuracy: 0.7136 - val_loss: 0.6948 - val_accuracy: 0.7206 - 437ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.7348 - accuracy: 0.7127 - val_loss: 0.6546 - val_accuracy: 0.7497 - 416ms/epoch - 69ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.7128 - accuracy: 0.7288 - val_loss: 0.7352 - val_accuracy: 0.7130 - 415ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8983 - accuracy: 0.3745 - val_loss: 1.3347 - val_accuracy: 0.5265 - 1s/epoch - 219ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.2293 - accuracy: 0.5715 - val_loss: 1.0853 - val_accuracy: 0.6245 - 417ms/epoch - 69ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.0418 - accuracy: 0.6293 - val_loss: 0.9295 - val_accuracy: 0.6617 - 420ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.9452 - accuracy: 0.6453 - val_loss: 0.8411 - val_accuracy: 0.6950 - 420ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8692 - accuracy: 0.6804 - val_loss: 0.8042 - val_accuracy: 0.6981 - 446ms/epoch - 74ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8202 - accuracy: 0.6925 - val_loss: 0.7844 - val_accuracy: 0.6969 - 443ms/epoch - 74ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7828 - accuracy: 0.7085 - val_loss: 0.7134 - val_accuracy: 0.7278 - 422ms/epoch - 70ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7394 - accuracy: 0.7228 - val_loss: 0.7015 - val_accuracy: 0.7398 - 440ms/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7233 - accuracy: 0.7264 - val_loss: 0.7304 - val_accuracy: 0.7144 - 415ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6975 - accuracy: 0.7386 - val_loss: 0.6417 - val_accuracy: 0.7594 - 417ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6847 - accuracy: 0.7435 - val_loss: 0.6463 - val_accuracy: 0.7553 - 428ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6652 - accuracy: 0.7503 - val_loss: 0.6189 - val_accuracy: 0.7692 - 443ms/epoch - 74ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6504 - accuracy: 0.7547 - val_loss: 0.6355 - val_accuracy: 0.7578 - 434ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6452 - accuracy: 0.7580 - val_loss: 0.6130 - val_accuracy: 0.7686 - 419ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6205 - accuracy: 0.7684 - val_loss: 0.6023 - val_accuracy: 0.7705 - 435ms/epoch - 73ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6254 - accuracy: 0.7634 - val_loss: 0.6017 - val_accuracy: 0.7778 - 446ms/epoch - 74ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.6191 - accuracy: 0.7664 - val_loss: 0.5960 - val_accuracy: 0.7710 - 466ms/epoch - 78ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6001 - accuracy: 0.7757 - val_loss: 0.5804 - val_accuracy: 0.7846 - 428ms/epoch - 71ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6055 - accuracy: 0.7753 - val_loss: 0.5713 - val_accuracy: 0.7856 - 451ms/epoch - 75ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5898 - accuracy: 0.7820 - val_loss: 0.6089 - val_accuracy: 0.7644 - 415ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.5279 - accuracy: 0.8060 - val_loss: 0.4136 - val_accuracy: 0.8463 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.3889 - accuracy: 0.8595 - val_loss: 0.3669 - val_accuracy: 0.8695 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.3722 - accuracy: 0.8680 - val_loss: 0.3313 - val_accuracy: 0.8790 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 7s - loss: 0.3647 - accuracy: 0.8719 - val_loss: 0.3417 - val_accuracy: 0.8828 - 7s/epoch - 5ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3608 - accuracy: 0.8748 - val_loss: 0.3315 - val_accuracy: 0.8854 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3645 - accuracy: 0.8745 - val_loss: 0.3456 - val_accuracy: 0.8807 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.3662 - accuracy: 0.8747 - val_loss: 0.3378 - val_accuracy: 0.8861 - 7s/epoch - 5ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.3717 - accuracy: 0.8729 - val_loss: 0.3226 - val_accuracy: 0.8863 - 7s/epoch - 5ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.3697 - accuracy: 0.8754 - val_loss: 0.3553 - val_accuracy: 0.8745 - 7s/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.3814 - accuracy: 0.8697 - val_loss: 0.3518 - val_accuracy: 0.8813 - 7s/epoch - 5ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 7s - loss: 0.3811 - accuracy: 0.8720 - val_loss: 0.3770 - val_accuracy: 0.8679 - 7s/epoch - 5ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3778 - accuracy: 0.8713 - val_loss: 0.3690 - val_accuracy: 0.8758 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3828 - accuracy: 0.8703 - val_loss: 0.3683 - val_accuracy: 0.8719 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.4041 - accuracy: 0.8663 - val_loss: 0.3192 - val_accuracy: 0.8892 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3871 - accuracy: 0.8688 - val_loss: 0.3433 - val_accuracy: 0.8734 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3922 - accuracy: 0.8677 - val_loss: 0.3380 - val_accuracy: 0.8796 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.4093 - accuracy: 0.8658 - val_loss: 0.3388 - val_accuracy: 0.8758 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.3930 - accuracy: 0.8683 - val_loss: 0.3750 - val_accuracy: 0.8732 - 7s/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3943 - accuracy: 0.8679 - val_loss: 0.3401 - val_accuracy: 0.8757 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.4073 - accuracy: 0.8639 - val_loss: 0.4144 - val_accuracy: 0.8790 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.5648 - accuracy: 0.7946 - val_loss: 0.4892 - val_accuracy: 0.8195 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.4631 - accuracy: 0.8349 - val_loss: 0.4333 - val_accuracy: 0.8451 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4432 - accuracy: 0.8427 - val_loss: 0.4382 - val_accuracy: 0.8416 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4360 - accuracy: 0.8460 - val_loss: 0.4330 - val_accuracy: 0.8451 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.4238 - accuracy: 0.8496 - val_loss: 0.4195 - val_accuracy: 0.8512 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 7s - loss: 0.4237 - accuracy: 0.8513 - val_loss: 0.4261 - val_accuracy: 0.8481 - 7s/epoch - 5ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.4178 - accuracy: 0.8525 - val_loss: 0.3985 - val_accuracy: 0.8582 - 7s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.4121 - accuracy: 0.8549 - val_loss: 0.4208 - val_accuracy: 0.8524 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.4036 - accuracy: 0.8573 - val_loss: 0.4017 - val_accuracy: 0.8568 - 7s/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.4070 - accuracy: 0.8550 - val_loss: 0.4000 - val_accuracy: 0.8599 - 7s/epoch - 5ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3980 - accuracy: 0.8590 - val_loss: 0.3923 - val_accuracy: 0.8606 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3999 - accuracy: 0.8594 - val_loss: 0.3989 - val_accuracy: 0.8543 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.3933 - accuracy: 0.8616 - val_loss: 0.3938 - val_accuracy: 0.8608 - 7s/epoch - 5ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.3899 - accuracy: 0.8645 - val_loss: 0.3888 - val_accuracy: 0.8624 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3912 - accuracy: 0.8613 - val_loss: 0.3727 - val_accuracy: 0.8663 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.3852 - accuracy: 0.8643 - val_loss: 0.3814 - val_accuracy: 0.8657 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.3865 - accuracy: 0.8642 - val_loss: 0.3931 - val_accuracy: 0.8611 - 7s/epoch - 5ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3889 - accuracy: 0.8634 - val_loss: 0.3854 - val_accuracy: 0.8641 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 7s - loss: 0.3810 - accuracy: 0.8662 - val_loss: 0.3829 - val_accuracy: 0.8652 - 7s/epoch - 5ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.3787 - accuracy: 0.8663 - val_loss: 0.3761 - val_accuracy: 0.8695 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.5162 - accuracy: 0.4829 - val_loss: 0.8978 - val_accuracy: 0.6605 - 1s/epoch - 57ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8346 - accuracy: 0.6799 - val_loss: 0.7979 - val_accuracy: 0.7086 - 451ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7113 - accuracy: 0.7298 - val_loss: 0.6025 - val_accuracy: 0.7673 - 445ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6300 - accuracy: 0.7583 - val_loss: 0.6022 - val_accuracy: 0.7663 - 449ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5810 - accuracy: 0.7803 - val_loss: 0.5512 - val_accuracy: 0.7937 - 448ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5342 - accuracy: 0.7985 - val_loss: 0.5180 - val_accuracy: 0.7993 - 455ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4936 - accuracy: 0.8163 - val_loss: 0.4815 - val_accuracy: 0.8197 - 451ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4682 - accuracy: 0.8260 - val_loss: 0.4513 - val_accuracy: 0.8311 - 458ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4460 - accuracy: 0.8352 - val_loss: 0.4129 - val_accuracy: 0.8471 - 455ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4276 - accuracy: 0.8413 - val_loss: 0.4028 - val_accuracy: 0.8522 - 453ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4078 - accuracy: 0.8493 - val_loss: 0.4343 - val_accuracy: 0.8431 - 438ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3925 - accuracy: 0.8556 - val_loss: 0.3833 - val_accuracy: 0.8604 - 454ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3820 - accuracy: 0.8607 - val_loss: 0.3554 - val_accuracy: 0.8640 - 439ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3648 - accuracy: 0.8635 - val_loss: 0.3668 - val_accuracy: 0.8639 - 451ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3581 - accuracy: 0.8668 - val_loss: 0.3448 - val_accuracy: 0.8758 - 439ms/epoch - 18ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3438 - accuracy: 0.8731 - val_loss: 0.3570 - val_accuracy: 0.8656 - 448ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3419 - accuracy: 0.8735 - val_loss: 0.3304 - val_accuracy: 0.8770 - 455ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3269 - accuracy: 0.8788 - val_loss: 0.3483 - val_accuracy: 0.8695 - 459ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3190 - accuracy: 0.8817 - val_loss: 0.3412 - val_accuracy: 0.8717 - 454ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3165 - accuracy: 0.8828 - val_loss: 0.3078 - val_accuracy: 0.8852 - 449ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.0964 - accuracy: 0.6017 - val_loss: 0.7100 - val_accuracy: 0.7211 - 1s/epoch - 53ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.6654 - accuracy: 0.7480 - val_loss: 0.7840 - val_accuracy: 0.7044 - 445ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6000 - accuracy: 0.7756 - val_loss: 0.5730 - val_accuracy: 0.7864 - 455ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.5411 - accuracy: 0.7980 - val_loss: 0.5273 - val_accuracy: 0.7984 - 439ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5023 - accuracy: 0.8149 - val_loss: 0.4577 - val_accuracy: 0.8328 - 457ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.4722 - accuracy: 0.8252 - val_loss: 0.4630 - val_accuracy: 0.8256 - 460ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.4454 - accuracy: 0.8373 - val_loss: 0.4452 - val_accuracy: 0.8356 - 456ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4298 - accuracy: 0.8421 - val_loss: 0.4172 - val_accuracy: 0.8427 - 460ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4088 - accuracy: 0.8508 - val_loss: 0.3928 - val_accuracy: 0.8558 - 441ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4060 - accuracy: 0.8516 - val_loss: 0.3952 - val_accuracy: 0.8567 - 442ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.3876 - accuracy: 0.8576 - val_loss: 0.3995 - val_accuracy: 0.8506 - 445ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.3793 - accuracy: 0.8628 - val_loss: 0.4080 - val_accuracy: 0.8467 - 451ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.3757 - accuracy: 0.8631 - val_loss: 0.3769 - val_accuracy: 0.8607 - 444ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.3624 - accuracy: 0.8684 - val_loss: 0.3975 - val_accuracy: 0.8475 - 465ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.3594 - accuracy: 0.8690 - val_loss: 0.3823 - val_accuracy: 0.8547 - 456ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3510 - accuracy: 0.8724 - val_loss: 0.3670 - val_accuracy: 0.8641 - 460ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3455 - accuracy: 0.8735 - val_loss: 0.3663 - val_accuracy: 0.8686 - 447ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3447 - accuracy: 0.8743 - val_loss: 0.3582 - val_accuracy: 0.8673 - 451ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3384 - accuracy: 0.8751 - val_loss: 0.3899 - val_accuracy: 0.8496 - 445ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3311 - accuracy: 0.8786 - val_loss: 0.3441 - val_accuracy: 0.8738 - 445ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0658 - accuracy: 0.2326 - val_loss: 1.7047 - val_accuracy: 0.4133 - 1s/epoch - 204ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.6943 - accuracy: 0.4460 - val_loss: 1.1001 - val_accuracy: 0.6100 - 409ms/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1006 - accuracy: 0.5888 - val_loss: 0.8861 - val_accuracy: 0.6862 - 427ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.9499 - accuracy: 0.6617 - val_loss: 0.8572 - val_accuracy: 0.6795 - 430ms/epoch - 72ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9952 - accuracy: 0.6296 - val_loss: 0.7617 - val_accuracy: 0.7296 - 417ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8463 - accuracy: 0.6860 - val_loss: 0.7495 - val_accuracy: 0.7257 - 421ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7810 - accuracy: 0.7039 - val_loss: 0.7114 - val_accuracy: 0.7377 - 435ms/epoch - 73ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.8160 - accuracy: 0.6897 - val_loss: 0.6760 - val_accuracy: 0.7537 - 431ms/epoch - 72ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7272 - accuracy: 0.7291 - val_loss: 0.8727 - val_accuracy: 0.6711 - 426ms/epoch - 71ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7118 - accuracy: 0.7385 - val_loss: 0.6670 - val_accuracy: 0.7377 - 424ms/epoch - 71ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6518 - accuracy: 0.7519 - val_loss: 0.8304 - val_accuracy: 0.6534 - 415ms/epoch - 69ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7374 - accuracy: 0.7162 - val_loss: 0.5924 - val_accuracy: 0.7791 - 410ms/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5999 - accuracy: 0.7721 - val_loss: 0.5754 - val_accuracy: 0.7809 - 434ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5993 - accuracy: 0.7781 - val_loss: 0.5688 - val_accuracy: 0.7786 - 419ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5765 - accuracy: 0.7770 - val_loss: 0.6530 - val_accuracy: 0.7446 - 426ms/epoch - 71ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6539 - accuracy: 0.7552 - val_loss: 0.5171 - val_accuracy: 0.8060 - 441ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5532 - accuracy: 0.7902 - val_loss: 0.5939 - val_accuracy: 0.7675 - 436ms/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5302 - accuracy: 0.7988 - val_loss: 0.5873 - val_accuracy: 0.7717 - 422ms/epoch - 70ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5416 - accuracy: 0.7976 - val_loss: 0.5362 - val_accuracy: 0.7984 - 442ms/epoch - 74ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5259 - accuracy: 0.8010 - val_loss: 0.5749 - val_accuracy: 0.7884 - 431ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8511 - accuracy: 0.3661 - val_loss: 1.3416 - val_accuracy: 0.5602 - 1s/epoch - 211ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.0592 - accuracy: 0.6261 - val_loss: 0.8334 - val_accuracy: 0.7021 - 434ms/epoch - 72ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.8406 - accuracy: 0.6845 - val_loss: 0.8141 - val_accuracy: 0.7116 - 450ms/epoch - 75ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8012 - accuracy: 0.7011 - val_loss: 0.7155 - val_accuracy: 0.7371 - 423ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7262 - accuracy: 0.7261 - val_loss: 0.6671 - val_accuracy: 0.7464 - 421ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7054 - accuracy: 0.7349 - val_loss: 0.7129 - val_accuracy: 0.7334 - 431ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6877 - accuracy: 0.7413 - val_loss: 0.6257 - val_accuracy: 0.7617 - 427ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6759 - accuracy: 0.7488 - val_loss: 0.6209 - val_accuracy: 0.7729 - 416ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6348 - accuracy: 0.7580 - val_loss: 0.5821 - val_accuracy: 0.7870 - 425ms/epoch - 71ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.5703 - accuracy: 0.7899 - val_loss: 0.6142 - val_accuracy: 0.7662 - 429ms/epoch - 72ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6145 - accuracy: 0.7684 - val_loss: 0.5464 - val_accuracy: 0.8026 - 424ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.5694 - accuracy: 0.7905 - val_loss: 0.6090 - val_accuracy: 0.7784 - 423ms/epoch - 70ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.5613 - accuracy: 0.7893 - val_loss: 0.5694 - val_accuracy: 0.7940 - 416ms/epoch - 69ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5680 - accuracy: 0.7895 - val_loss: 0.5501 - val_accuracy: 0.7924 - 427ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5269 - accuracy: 0.8058 - val_loss: 0.4997 - val_accuracy: 0.8139 - 441ms/epoch - 74ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5457 - accuracy: 0.7972 - val_loss: 0.5505 - val_accuracy: 0.7928 - 420ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5062 - accuracy: 0.8146 - val_loss: 0.4829 - val_accuracy: 0.8227 - 420ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5004 - accuracy: 0.8145 - val_loss: 0.5028 - val_accuracy: 0.8049 - 417ms/epoch - 69ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.4933 - accuracy: 0.8182 - val_loss: 0.4708 - val_accuracy: 0.8252 - 424ms/epoch - 71ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.4687 - accuracy: 0.8301 - val_loss: 0.4751 - val_accuracy: 0.8235 - 440ms/epoch - 73ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6374 - accuracy: 0.7650 - val_loss: 0.4044 - val_accuracy: 0.8475 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4484 - accuracy: 0.8461 - val_loss: 0.4002 - val_accuracy: 0.8529 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4122 - accuracy: 0.8593 - val_loss: 0.3714 - val_accuracy: 0.8639 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4117 - accuracy: 0.8625 - val_loss: 0.3899 - val_accuracy: 0.8598 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.4061 - accuracy: 0.8627 - val_loss: 0.3640 - val_accuracy: 0.8737 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.4071 - accuracy: 0.8647 - val_loss: 0.3672 - val_accuracy: 0.8690 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.4054 - accuracy: 0.8667 - val_loss: 0.3657 - val_accuracy: 0.8724 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.4099 - accuracy: 0.8661 - val_loss: 0.3986 - val_accuracy: 0.8619 - 7s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.4280 - accuracy: 0.8629 - val_loss: 0.4156 - val_accuracy: 0.8556 - 7s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.4276 - accuracy: 0.8633 - val_loss: 0.3851 - val_accuracy: 0.8679 - 7s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4249 - accuracy: 0.8620 - val_loss: 0.4104 - val_accuracy: 0.8561 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4385 - accuracy: 0.8570 - val_loss: 0.3824 - val_accuracy: 0.8717 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.4495 - accuracy: 0.8535 - val_loss: 0.3905 - val_accuracy: 0.8598 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.4523 - accuracy: 0.8550 - val_loss: 0.3945 - val_accuracy: 0.8602 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.4550 - accuracy: 0.8524 - val_loss: 0.4270 - val_accuracy: 0.8522 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.4700 - accuracy: 0.8507 - val_loss: 0.4032 - val_accuracy: 0.8571 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.4714 - accuracy: 0.8503 - val_loss: 0.4305 - val_accuracy: 0.8545 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.4841 - accuracy: 0.8445 - val_loss: 0.5070 - val_accuracy: 0.8402 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.4921 - accuracy: 0.8451 - val_loss: 0.4142 - val_accuracy: 0.8524 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.4903 - accuracy: 0.8428 - val_loss: 0.4886 - val_accuracy: 0.8281 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6043 - accuracy: 0.7788 - val_loss: 0.4596 - val_accuracy: 0.8369 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4724 - accuracy: 0.8321 - val_loss: 0.4130 - val_accuracy: 0.8496 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4353 - accuracy: 0.8455 - val_loss: 0.4003 - val_accuracy: 0.8567 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4108 - accuracy: 0.8550 - val_loss: 0.3938 - val_accuracy: 0.8582 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.3982 - accuracy: 0.8613 - val_loss: 0.3811 - val_accuracy: 0.8638 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3866 - accuracy: 0.8651 - val_loss: 0.3702 - val_accuracy: 0.8691 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.3753 - accuracy: 0.8684 - val_loss: 0.3553 - val_accuracy: 0.8779 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.3683 - accuracy: 0.8717 - val_loss: 0.3556 - val_accuracy: 0.8760 - 7s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.3650 - accuracy: 0.8730 - val_loss: 0.3680 - val_accuracy: 0.8760 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.3590 - accuracy: 0.8754 - val_loss: 0.3615 - val_accuracy: 0.8754 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3535 - accuracy: 0.8777 - val_loss: 0.3542 - val_accuracy: 0.8822 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.3492 - accuracy: 0.8786 - val_loss: 0.3515 - val_accuracy: 0.8792 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3427 - accuracy: 0.8832 - val_loss: 0.3546 - val_accuracy: 0.8788 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.3400 - accuracy: 0.8816 - val_loss: 0.3514 - val_accuracy: 0.8794 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3369 - accuracy: 0.8859 - val_loss: 0.3614 - val_accuracy: 0.8788 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.3366 - accuracy: 0.8855 - val_loss: 0.3425 - val_accuracy: 0.8848 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.3341 - accuracy: 0.8847 - val_loss: 0.3462 - val_accuracy: 0.8851 - 7s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3303 - accuracy: 0.8870 - val_loss: 0.3476 - val_accuracy: 0.8838 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3279 - accuracy: 0.8873 - val_loss: 0.3398 - val_accuracy: 0.8858 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3261 - accuracy: 0.8883 - val_loss: 0.3368 - val_accuracy: 0.8844 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.6307 - accuracy: 0.4186 - val_loss: 0.8791 - val_accuracy: 0.6874 - 1s/epoch - 57ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.9096 - accuracy: 0.6545 - val_loss: 0.7726 - val_accuracy: 0.7130 - 453ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7888 - accuracy: 0.6999 - val_loss: 0.6161 - val_accuracy: 0.7581 - 450ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.7005 - accuracy: 0.7314 - val_loss: 0.6070 - val_accuracy: 0.7537 - 444ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6506 - accuracy: 0.7485 - val_loss: 0.5757 - val_accuracy: 0.7568 - 451ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6018 - accuracy: 0.7705 - val_loss: 0.5560 - val_accuracy: 0.7779 - 448ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5609 - accuracy: 0.7869 - val_loss: 0.5105 - val_accuracy: 0.7929 - 446ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5517 - accuracy: 0.7930 - val_loss: 0.5365 - val_accuracy: 0.7785 - 460ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5137 - accuracy: 0.8085 - val_loss: 0.5297 - val_accuracy: 0.7985 - 449ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4971 - accuracy: 0.8138 - val_loss: 0.4338 - val_accuracy: 0.8359 - 455ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4756 - accuracy: 0.8252 - val_loss: 0.4720 - val_accuracy: 0.8088 - 443ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4640 - accuracy: 0.8306 - val_loss: 0.4665 - val_accuracy: 0.8240 - 453ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4408 - accuracy: 0.8388 - val_loss: 0.4008 - val_accuracy: 0.8509 - 457ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4333 - accuracy: 0.8395 - val_loss: 0.3820 - val_accuracy: 0.8569 - 448ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4197 - accuracy: 0.8470 - val_loss: 0.3943 - val_accuracy: 0.8520 - 449ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4102 - accuracy: 0.8502 - val_loss: 0.4113 - val_accuracy: 0.8412 - 468ms/epoch - 20ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4011 - accuracy: 0.8529 - val_loss: 0.3714 - val_accuracy: 0.8601 - 468ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3872 - accuracy: 0.8572 - val_loss: 0.3571 - val_accuracy: 0.8643 - 460ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3802 - accuracy: 0.8607 - val_loss: 0.3604 - val_accuracy: 0.8661 - 453ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3680 - accuracy: 0.8657 - val_loss: 0.3976 - val_accuracy: 0.8456 - 464ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.3407 - accuracy: 0.5234 - val_loss: 0.7882 - val_accuracy: 0.6867 - 1s/epoch - 53ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7762 - accuracy: 0.7030 - val_loss: 0.7166 - val_accuracy: 0.7238 - 456ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6606 - accuracy: 0.7515 - val_loss: 0.6099 - val_accuracy: 0.7767 - 449ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6048 - accuracy: 0.7736 - val_loss: 0.5509 - val_accuracy: 0.7944 - 438ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5632 - accuracy: 0.7922 - val_loss: 0.5838 - val_accuracy: 0.7846 - 450ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5386 - accuracy: 0.8023 - val_loss: 0.4945 - val_accuracy: 0.8185 - 453ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5177 - accuracy: 0.8094 - val_loss: 0.4824 - val_accuracy: 0.8214 - 448ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4929 - accuracy: 0.8215 - val_loss: 0.4521 - val_accuracy: 0.8348 - 451ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4753 - accuracy: 0.8255 - val_loss: 0.5089 - val_accuracy: 0.8080 - 473ms/epoch - 20ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4690 - accuracy: 0.8303 - val_loss: 0.4596 - val_accuracy: 0.8280 - 449ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4529 - accuracy: 0.8361 - val_loss: 0.4373 - val_accuracy: 0.8417 - 443ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4406 - accuracy: 0.8417 - val_loss: 0.4029 - val_accuracy: 0.8513 - 458ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4305 - accuracy: 0.8428 - val_loss: 0.4090 - val_accuracy: 0.8470 - 443ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4199 - accuracy: 0.8484 - val_loss: 0.4346 - val_accuracy: 0.8373 - 453ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4124 - accuracy: 0.8503 - val_loss: 0.4108 - val_accuracy: 0.8459 - 458ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4057 - accuracy: 0.8517 - val_loss: 0.3828 - val_accuracy: 0.8556 - 470ms/epoch - 20ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4031 - accuracy: 0.8539 - val_loss: 0.3889 - val_accuracy: 0.8541 - 465ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3911 - accuracy: 0.8596 - val_loss: 0.3616 - val_accuracy: 0.8679 - 452ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3840 - accuracy: 0.8603 - val_loss: 0.3669 - val_accuracy: 0.8670 - 471ms/epoch - 20ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3864 - accuracy: 0.8591 - val_loss: 0.3555 - val_accuracy: 0.8681 - 446ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 2s - loss: 2.1587 - accuracy: 0.2033 - val_loss: 1.8905 - val_accuracy: 0.4243 - 2s/epoch - 270ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.7507 - accuracy: 0.4215 - val_loss: 1.0959 - val_accuracy: 0.6453 - 419ms/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1848 - accuracy: 0.5549 - val_loss: 1.0065 - val_accuracy: 0.6385 - 419ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.0022 - accuracy: 0.6260 - val_loss: 0.8460 - val_accuracy: 0.6845 - 436ms/epoch - 73ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9748 - accuracy: 0.6280 - val_loss: 0.8545 - val_accuracy: 0.6354 - 440ms/epoch - 73ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.9025 - accuracy: 0.6476 - val_loss: 0.7133 - val_accuracy: 0.7343 - 420ms/epoch - 70ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8243 - accuracy: 0.6848 - val_loss: 0.7233 - val_accuracy: 0.7320 - 424ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7985 - accuracy: 0.6933 - val_loss: 0.6809 - val_accuracy: 0.7266 - 416ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7325 - accuracy: 0.7187 - val_loss: 0.8389 - val_accuracy: 0.6610 - 431ms/epoch - 72ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.8120 - accuracy: 0.6863 - val_loss: 0.6629 - val_accuracy: 0.7313 - 413ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6969 - accuracy: 0.7256 - val_loss: 0.6485 - val_accuracy: 0.7408 - 440ms/epoch - 73ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6816 - accuracy: 0.7304 - val_loss: 0.6372 - val_accuracy: 0.7508 - 418ms/epoch - 70ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6674 - accuracy: 0.7437 - val_loss: 0.6450 - val_accuracy: 0.7294 - 435ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6742 - accuracy: 0.7369 - val_loss: 0.6133 - val_accuracy: 0.7607 - 423ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6316 - accuracy: 0.7598 - val_loss: 0.5618 - val_accuracy: 0.7710 - 416ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6429 - accuracy: 0.7496 - val_loss: 0.5613 - val_accuracy: 0.7833 - 436ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5963 - accuracy: 0.7728 - val_loss: 0.5740 - val_accuracy: 0.7660 - 440ms/epoch - 73ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6191 - accuracy: 0.7592 - val_loss: 0.5449 - val_accuracy: 0.7919 - 432ms/epoch - 72ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5858 - accuracy: 0.7755 - val_loss: 0.5039 - val_accuracy: 0.8018 - 425ms/epoch - 71ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5944 - accuracy: 0.7724 - val_loss: 0.5216 - val_accuracy: 0.8042 - 422ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8988 - accuracy: 0.3280 - val_loss: 1.1718 - val_accuracy: 0.5526 - 1s/epoch - 209ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1117 - accuracy: 0.5895 - val_loss: 0.9773 - val_accuracy: 0.6070 - 424ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9561 - accuracy: 0.6327 - val_loss: 0.7931 - val_accuracy: 0.7151 - 441ms/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8208 - accuracy: 0.6886 - val_loss: 0.7690 - val_accuracy: 0.7034 - 421ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7676 - accuracy: 0.7107 - val_loss: 0.7088 - val_accuracy: 0.7240 - 443ms/epoch - 74ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7821 - accuracy: 0.6958 - val_loss: 0.6940 - val_accuracy: 0.7259 - 432ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.6894 - accuracy: 0.7325 - val_loss: 0.6492 - val_accuracy: 0.7409 - 421ms/epoch - 70ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6663 - accuracy: 0.7497 - val_loss: 0.7028 - val_accuracy: 0.7259 - 427ms/epoch - 71ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.6686 - accuracy: 0.7474 - val_loss: 0.6235 - val_accuracy: 0.7672 - 466ms/epoch - 78ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6690 - accuracy: 0.7488 - val_loss: 0.5848 - val_accuracy: 0.7811 - 477ms/epoch - 79ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6246 - accuracy: 0.7628 - val_loss: 0.6034 - val_accuracy: 0.7614 - 473ms/epoch - 79ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6057 - accuracy: 0.7713 - val_loss: 0.6337 - val_accuracy: 0.7651 - 449ms/epoch - 75ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6067 - accuracy: 0.7768 - val_loss: 0.5768 - val_accuracy: 0.7817 - 460ms/epoch - 77ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.5966 - accuracy: 0.7718 - val_loss: 0.5769 - val_accuracy: 0.7788 - 461ms/epoch - 77ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5776 - accuracy: 0.7861 - val_loss: 0.5355 - val_accuracy: 0.7944 - 470ms/epoch - 78ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5930 - accuracy: 0.7776 - val_loss: 0.5456 - val_accuracy: 0.7981 - 448ms/epoch - 75ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5570 - accuracy: 0.7913 - val_loss: 0.5397 - val_accuracy: 0.7941 - 422ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5512 - accuracy: 0.7965 - val_loss: 0.5396 - val_accuracy: 0.7954 - 420ms/epoch - 70ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5519 - accuracy: 0.7946 - val_loss: 0.5381 - val_accuracy: 0.7964 - 431ms/epoch - 72ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5459 - accuracy: 0.7987 - val_loss: 0.5037 - val_accuracy: 0.8185 - 416ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.6931 - accuracy: 0.7402 - val_loss: 0.4778 - val_accuracy: 0.8245 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.4916 - accuracy: 0.8200 - val_loss: 0.3935 - val_accuracy: 0.8549 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.4387 - accuracy: 0.8403 - val_loss: 0.3646 - val_accuracy: 0.8645 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4051 - accuracy: 0.8549 - val_loss: 0.3420 - val_accuracy: 0.8742 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 7s - loss: 0.3867 - accuracy: 0.8599 - val_loss: 0.3353 - val_accuracy: 0.8790 - 7s/epoch - 5ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.3753 - accuracy: 0.8672 - val_loss: 0.3129 - val_accuracy: 0.8871 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.3692 - accuracy: 0.8713 - val_loss: 0.3234 - val_accuracy: 0.8821 - 7s/epoch - 5ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.3634 - accuracy: 0.8723 - val_loss: 0.3304 - val_accuracy: 0.8813 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.3619 - accuracy: 0.8714 - val_loss: 0.3216 - val_accuracy: 0.8891 - 7s/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.3604 - accuracy: 0.8751 - val_loss: 0.2978 - val_accuracy: 0.8942 - 7s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 7s - loss: 0.3594 - accuracy: 0.8747 - val_loss: 0.3316 - val_accuracy: 0.8889 - 7s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.3537 - accuracy: 0.8762 - val_loss: 0.2922 - val_accuracy: 0.8953 - 7s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.3574 - accuracy: 0.8756 - val_loss: 0.3043 - val_accuracy: 0.8903 - 7s/epoch - 5ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.3539 - accuracy: 0.8777 - val_loss: 0.3179 - val_accuracy: 0.8824 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3533 - accuracy: 0.8774 - val_loss: 0.3177 - val_accuracy: 0.8879 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.3545 - accuracy: 0.8775 - val_loss: 0.3184 - val_accuracy: 0.8875 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.3580 - accuracy: 0.8764 - val_loss: 0.3174 - val_accuracy: 0.8869 - 7s/epoch - 5ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.3534 - accuracy: 0.8770 - val_loss: 0.3003 - val_accuracy: 0.8920 - 7s/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3570 - accuracy: 0.8781 - val_loss: 0.3204 - val_accuracy: 0.8889 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.3581 - accuracy: 0.8757 - val_loss: 0.3083 - val_accuracy: 0.8906 - 7s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.6404 - accuracy: 0.7613 - val_loss: 0.4929 - val_accuracy: 0.8164 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.5069 - accuracy: 0.8168 - val_loss: 0.4396 - val_accuracy: 0.8389 - 7s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 7s - loss: 0.4699 - accuracy: 0.8311 - val_loss: 0.4223 - val_accuracy: 0.8421 - 7s/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 7s - loss: 0.4481 - accuracy: 0.8393 - val_loss: 0.4022 - val_accuracy: 0.8538 - 7s/epoch - 5ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.4279 - accuracy: 0.8465 - val_loss: 0.3901 - val_accuracy: 0.8598 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 7s - loss: 0.4249 - accuracy: 0.8478 - val_loss: 0.3877 - val_accuracy: 0.8587 - 7s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.4116 - accuracy: 0.8509 - val_loss: 0.3731 - val_accuracy: 0.8646 - 7s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.4103 - accuracy: 0.8534 - val_loss: 0.3758 - val_accuracy: 0.8613 - 7s/epoch - 5ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.4066 - accuracy: 0.8547 - val_loss: 0.3782 - val_accuracy: 0.8590 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.3983 - accuracy: 0.8570 - val_loss: 0.3686 - val_accuracy: 0.8656 - 7s/epoch - 5ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.3997 - accuracy: 0.8585 - val_loss: 0.3703 - val_accuracy: 0.8670 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.3997 - accuracy: 0.8569 - val_loss: 0.3726 - val_accuracy: 0.8657 - 7s/epoch - 5ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.3997 - accuracy: 0.8577 - val_loss: 0.3700 - val_accuracy: 0.8696 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.3977 - accuracy: 0.8585 - val_loss: 0.3696 - val_accuracy: 0.8671 - 7s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.3968 - accuracy: 0.8593 - val_loss: 0.3688 - val_accuracy: 0.8648 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.3982 - accuracy: 0.8589 - val_loss: 0.3720 - val_accuracy: 0.8652 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3947 - accuracy: 0.8599 - val_loss: 0.3663 - val_accuracy: 0.8703 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3946 - accuracy: 0.8609 - val_loss: 0.3649 - val_accuracy: 0.8670 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 7s - loss: 0.3912 - accuracy: 0.8615 - val_loss: 0.3672 - val_accuracy: 0.8691 - 7s/epoch - 5ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3933 - accuracy: 0.8609 - val_loss: 0.3659 - val_accuracy: 0.8692 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.4178 - accuracy: 0.5172 - val_loss: 0.8593 - val_accuracy: 0.6841 - 1s/epoch - 55ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8595 - accuracy: 0.6743 - val_loss: 0.7541 - val_accuracy: 0.6998 - 442ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7650 - accuracy: 0.7086 - val_loss: 0.6630 - val_accuracy: 0.7472 - 440ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6942 - accuracy: 0.7352 - val_loss: 0.6087 - val_accuracy: 0.7708 - 450ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6596 - accuracy: 0.7486 - val_loss: 0.5937 - val_accuracy: 0.7672 - 462ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6188 - accuracy: 0.7656 - val_loss: 0.5552 - val_accuracy: 0.7940 - 444ms/epoch - 18ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5940 - accuracy: 0.7758 - val_loss: 0.5394 - val_accuracy: 0.8055 - 446ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5662 - accuracy: 0.7872 - val_loss: 0.5255 - val_accuracy: 0.8007 - 451ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5481 - accuracy: 0.7956 - val_loss: 0.4973 - val_accuracy: 0.8191 - 446ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5337 - accuracy: 0.8009 - val_loss: 0.4835 - val_accuracy: 0.8239 - 455ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5133 - accuracy: 0.8102 - val_loss: 0.4780 - val_accuracy: 0.8190 - 448ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5079 - accuracy: 0.8119 - val_loss: 0.4867 - val_accuracy: 0.8165 - 460ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4917 - accuracy: 0.8184 - val_loss: 0.4507 - val_accuracy: 0.8331 - 438ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4802 - accuracy: 0.8241 - val_loss: 0.4663 - val_accuracy: 0.8215 - 444ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4693 - accuracy: 0.8288 - val_loss: 0.4306 - val_accuracy: 0.8432 - 446ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4626 - accuracy: 0.8308 - val_loss: 0.4129 - val_accuracy: 0.8498 - 458ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4550 - accuracy: 0.8334 - val_loss: 0.4140 - val_accuracy: 0.8453 - 459ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4500 - accuracy: 0.8360 - val_loss: 0.4039 - val_accuracy: 0.8532 - 456ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4377 - accuracy: 0.8409 - val_loss: 0.3987 - val_accuracy: 0.8542 - 455ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4354 - accuracy: 0.8411 - val_loss: 0.4012 - val_accuracy: 0.8501 - 452ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.1800 - accuracy: 0.5833 - val_loss: 0.7841 - val_accuracy: 0.7149 - 1s/epoch - 54ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7636 - accuracy: 0.7132 - val_loss: 0.6648 - val_accuracy: 0.7555 - 462ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6762 - accuracy: 0.7463 - val_loss: 0.5991 - val_accuracy: 0.7728 - 455ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6249 - accuracy: 0.7659 - val_loss: 0.5789 - val_accuracy: 0.7826 - 451ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5922 - accuracy: 0.7785 - val_loss: 0.5482 - val_accuracy: 0.7987 - 438ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5674 - accuracy: 0.7877 - val_loss: 0.5240 - val_accuracy: 0.8029 - 459ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5489 - accuracy: 0.7948 - val_loss: 0.5048 - val_accuracy: 0.8128 - 459ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5303 - accuracy: 0.8031 - val_loss: 0.5373 - val_accuracy: 0.7901 - 442ms/epoch - 18ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5142 - accuracy: 0.8099 - val_loss: 0.4972 - val_accuracy: 0.8153 - 455ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5058 - accuracy: 0.8143 - val_loss: 0.4664 - val_accuracy: 0.8283 - 454ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4937 - accuracy: 0.8188 - val_loss: 0.4676 - val_accuracy: 0.8256 - 441ms/epoch - 18ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4851 - accuracy: 0.8217 - val_loss: 0.4503 - val_accuracy: 0.8350 - 442ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4734 - accuracy: 0.8249 - val_loss: 0.4462 - val_accuracy: 0.8360 - 468ms/epoch - 20ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4655 - accuracy: 0.8298 - val_loss: 0.4317 - val_accuracy: 0.8418 - 435ms/epoch - 18ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4581 - accuracy: 0.8318 - val_loss: 0.4364 - val_accuracy: 0.8410 - 445ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4538 - accuracy: 0.8332 - val_loss: 0.4241 - val_accuracy: 0.8434 - 444ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4446 - accuracy: 0.8378 - val_loss: 0.4348 - val_accuracy: 0.8349 - 454ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4407 - accuracy: 0.8381 - val_loss: 0.4138 - val_accuracy: 0.8451 - 436ms/epoch - 18ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4366 - accuracy: 0.8399 - val_loss: 0.4064 - val_accuracy: 0.8517 - 453ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4312 - accuracy: 0.8412 - val_loss: 0.4118 - val_accuracy: 0.8472 - 447ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0419 - accuracy: 0.2912 - val_loss: 1.5251 - val_accuracy: 0.5028 - 1s/epoch - 205ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.4087 - accuracy: 0.5270 - val_loss: 1.1055 - val_accuracy: 0.6162 - 428ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.1430 - accuracy: 0.5888 - val_loss: 0.9410 - val_accuracy: 0.6571 - 420ms/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.0144 - accuracy: 0.6282 - val_loss: 0.8864 - val_accuracy: 0.6799 - 433ms/epoch - 72ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9192 - accuracy: 0.6579 - val_loss: 0.8311 - val_accuracy: 0.6831 - 413ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8586 - accuracy: 0.6791 - val_loss: 0.8277 - val_accuracy: 0.6552 - 434ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8481 - accuracy: 0.6810 - val_loss: 0.7699 - val_accuracy: 0.7173 - 440ms/epoch - 73ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.8092 - accuracy: 0.6928 - val_loss: 0.7622 - val_accuracy: 0.7103 - 415ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7687 - accuracy: 0.7090 - val_loss: 0.6979 - val_accuracy: 0.7394 - 416ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7874 - accuracy: 0.7056 - val_loss: 0.6654 - val_accuracy: 0.7523 - 416ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.7429 - accuracy: 0.7190 - val_loss: 0.6643 - val_accuracy: 0.7533 - 436ms/epoch - 73ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7216 - accuracy: 0.7292 - val_loss: 0.6586 - val_accuracy: 0.7461 - 414ms/epoch - 69ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.7116 - accuracy: 0.7277 - val_loss: 0.6358 - val_accuracy: 0.7565 - 433ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6816 - accuracy: 0.7427 - val_loss: 0.6346 - val_accuracy: 0.7585 - 436ms/epoch - 73ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6996 - accuracy: 0.7314 - val_loss: 0.6418 - val_accuracy: 0.7568 - 413ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6790 - accuracy: 0.7441 - val_loss: 0.6077 - val_accuracy: 0.7767 - 438ms/epoch - 73ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.6491 - accuracy: 0.7539 - val_loss: 0.6114 - val_accuracy: 0.7638 - 416ms/epoch - 69ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6589 - accuracy: 0.7521 - val_loss: 0.6002 - val_accuracy: 0.7738 - 446ms/epoch - 74ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6236 - accuracy: 0.7655 - val_loss: 0.6013 - val_accuracy: 0.7730 - 425ms/epoch - 71ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.6364 - accuracy: 0.7602 - val_loss: 0.5989 - val_accuracy: 0.7713 - 424ms/epoch - 71ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.7989 - accuracy: 0.3736 - val_loss: 1.2730 - val_accuracy: 0.5543 - 1s/epoch - 219ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1554 - accuracy: 0.5897 - val_loss: 1.0069 - val_accuracy: 0.6110 - 411ms/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9634 - accuracy: 0.6459 - val_loss: 0.8421 - val_accuracy: 0.6928 - 427ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8590 - accuracy: 0.6815 - val_loss: 0.7869 - val_accuracy: 0.7137 - 412ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8226 - accuracy: 0.6894 - val_loss: 0.7304 - val_accuracy: 0.7290 - 436ms/epoch - 73ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7617 - accuracy: 0.7144 - val_loss: 0.7512 - val_accuracy: 0.7157 - 413ms/epoch - 69ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7344 - accuracy: 0.7238 - val_loss: 0.6673 - val_accuracy: 0.7442 - 441ms/epoch - 73ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6962 - accuracy: 0.7398 - val_loss: 0.7118 - val_accuracy: 0.7257 - 429ms/epoch - 72ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7181 - accuracy: 0.7259 - val_loss: 0.6864 - val_accuracy: 0.7355 - 423ms/epoch - 70ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6728 - accuracy: 0.7454 - val_loss: 0.6210 - val_accuracy: 0.7617 - 440ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6452 - accuracy: 0.7583 - val_loss: 0.6218 - val_accuracy: 0.7615 - 427ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6476 - accuracy: 0.7570 - val_loss: 0.6230 - val_accuracy: 0.7623 - 407ms/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6368 - accuracy: 0.7613 - val_loss: 0.5845 - val_accuracy: 0.7772 - 431ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6124 - accuracy: 0.7712 - val_loss: 0.5905 - val_accuracy: 0.7821 - 426ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6111 - accuracy: 0.7698 - val_loss: 0.5743 - val_accuracy: 0.7906 - 417ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5997 - accuracy: 0.7771 - val_loss: 0.5625 - val_accuracy: 0.7899 - 418ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5828 - accuracy: 0.7824 - val_loss: 0.5579 - val_accuracy: 0.7883 - 460ms/epoch - 77ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5768 - accuracy: 0.7868 - val_loss: 0.5790 - val_accuracy: 0.7806 - 459ms/epoch - 77ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5685 - accuracy: 0.7902 - val_loss: 0.5298 - val_accuracy: 0.8074 - 465ms/epoch - 78ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5722 - accuracy: 0.7894 - val_loss: 0.5301 - val_accuracy: 0.8029 - 440ms/epoch - 73ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.9754 - accuracy: 0.6301 - val_loss: 0.5643 - val_accuracy: 0.7700 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.6677 - accuracy: 0.7553 - val_loss: 0.4687 - val_accuracy: 0.8295 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.5749 - accuracy: 0.7967 - val_loss: 0.4268 - val_accuracy: 0.8452 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5374 - accuracy: 0.8131 - val_loss: 0.3927 - val_accuracy: 0.8592 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 7s - loss: 0.5193 - accuracy: 0.8226 - val_loss: 0.4007 - val_accuracy: 0.8602 - 7s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.5095 - accuracy: 0.8282 - val_loss: 0.3925 - val_accuracy: 0.8622 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.5017 - accuracy: 0.8312 - val_loss: 0.4167 - val_accuracy: 0.8591 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.4969 - accuracy: 0.8339 - val_loss: 0.3855 - val_accuracy: 0.8641 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.4955 - accuracy: 0.8343 - val_loss: 0.4014 - val_accuracy: 0.8632 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4948 - accuracy: 0.8325 - val_loss: 0.4027 - val_accuracy: 0.8572 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4970 - accuracy: 0.8347 - val_loss: 0.3924 - val_accuracy: 0.8600 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4961 - accuracy: 0.8341 - val_loss: 0.3901 - val_accuracy: 0.8589 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.4989 - accuracy: 0.8315 - val_loss: 0.4667 - val_accuracy: 0.8397 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.5040 - accuracy: 0.8303 - val_loss: 0.4716 - val_accuracy: 0.8525 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.5085 - accuracy: 0.8313 - val_loss: 0.3716 - val_accuracy: 0.8661 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.5120 - accuracy: 0.8326 - val_loss: 0.4357 - val_accuracy: 0.8504 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.5142 - accuracy: 0.8321 - val_loss: 0.4489 - val_accuracy: 0.8410 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.5175 - accuracy: 0.8303 - val_loss: 0.4098 - val_accuracy: 0.8517 - 7s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.5255 - accuracy: 0.8297 - val_loss: 0.4066 - val_accuracy: 0.8607 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.5333 - accuracy: 0.8262 - val_loss: 0.4070 - val_accuracy: 0.8570 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.7709 - accuracy: 0.7183 - val_loss: 0.5779 - val_accuracy: 0.7860 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.6081 - accuracy: 0.7826 - val_loss: 0.4951 - val_accuracy: 0.8221 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 7s - loss: 0.5587 - accuracy: 0.8035 - val_loss: 0.4833 - val_accuracy: 0.8259 - 7s/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5305 - accuracy: 0.8130 - val_loss: 0.4504 - val_accuracy: 0.8386 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 7s - loss: 0.5058 - accuracy: 0.8227 - val_loss: 0.4406 - val_accuracy: 0.8453 - 7s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.4939 - accuracy: 0.8274 - val_loss: 0.4508 - val_accuracy: 0.8428 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.4823 - accuracy: 0.8331 - val_loss: 0.4265 - val_accuracy: 0.8478 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.4689 - accuracy: 0.8372 - val_loss: 0.4161 - val_accuracy: 0.8529 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.4575 - accuracy: 0.8413 - val_loss: 0.4106 - val_accuracy: 0.8569 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4478 - accuracy: 0.8469 - val_loss: 0.4049 - val_accuracy: 0.8582 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4390 - accuracy: 0.8500 - val_loss: 0.3874 - val_accuracy: 0.8647 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4343 - accuracy: 0.8516 - val_loss: 0.3965 - val_accuracy: 0.8642 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 7s - loss: 0.4234 - accuracy: 0.8551 - val_loss: 0.3871 - val_accuracy: 0.8660 - 7s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.4203 - accuracy: 0.8575 - val_loss: 0.3896 - val_accuracy: 0.8676 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.4129 - accuracy: 0.8596 - val_loss: 0.3936 - val_accuracy: 0.8665 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.4048 - accuracy: 0.8623 - val_loss: 0.3777 - val_accuracy: 0.8717 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.3996 - accuracy: 0.8654 - val_loss: 0.3850 - val_accuracy: 0.8722 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.3944 - accuracy: 0.8669 - val_loss: 0.3752 - val_accuracy: 0.8714 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.3930 - accuracy: 0.8674 - val_loss: 0.3620 - val_accuracy: 0.8770 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.3899 - accuracy: 0.8675 - val_loss: 0.3667 - val_accuracy: 0.8765 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.7835 - accuracy: 0.3457 - val_loss: 1.0966 - val_accuracy: 0.6539 - 1s/epoch - 56ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 1.2344 - accuracy: 0.5369 - val_loss: 0.8579 - val_accuracy: 0.7202 - 441ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 1.0767 - accuracy: 0.5917 - val_loss: 0.7552 - val_accuracy: 0.7301 - 442ms/epoch - 18ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.9806 - accuracy: 0.6265 - val_loss: 0.7087 - val_accuracy: 0.7211 - 469ms/epoch - 20ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.9284 - accuracy: 0.6454 - val_loss: 0.6850 - val_accuracy: 0.7517 - 445ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.8850 - accuracy: 0.6612 - val_loss: 0.6635 - val_accuracy: 0.7385 - 463ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.8492 - accuracy: 0.6735 - val_loss: 0.6439 - val_accuracy: 0.7460 - 455ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.8153 - accuracy: 0.6864 - val_loss: 0.6029 - val_accuracy: 0.7665 - 463ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.7935 - accuracy: 0.6964 - val_loss: 0.5946 - val_accuracy: 0.7702 - 463ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.7654 - accuracy: 0.7091 - val_loss: 0.5812 - val_accuracy: 0.7809 - 466ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.7413 - accuracy: 0.7186 - val_loss: 0.6100 - val_accuracy: 0.7636 - 447ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.7241 - accuracy: 0.7248 - val_loss: 0.5809 - val_accuracy: 0.7738 - 440ms/epoch - 18ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.7027 - accuracy: 0.7327 - val_loss: 0.5851 - val_accuracy: 0.7761 - 451ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.6946 - accuracy: 0.7372 - val_loss: 0.5653 - val_accuracy: 0.7809 - 454ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.6767 - accuracy: 0.7414 - val_loss: 0.5307 - val_accuracy: 0.7957 - 469ms/epoch - 20ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.6650 - accuracy: 0.7475 - val_loss: 0.5126 - val_accuracy: 0.7985 - 464ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.6566 - accuracy: 0.7514 - val_loss: 0.5356 - val_accuracy: 0.7940 - 441ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.6442 - accuracy: 0.7560 - val_loss: 0.5081 - val_accuracy: 0.8114 - 455ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.6340 - accuracy: 0.7622 - val_loss: 0.5080 - val_accuracy: 0.8012 - 470ms/epoch - 20ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.6264 - accuracy: 0.7632 - val_loss: 0.5249 - val_accuracy: 0.8119 - 439ms/epoch - 18ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.3745 - accuracy: 0.5098 - val_loss: 0.8679 - val_accuracy: 0.6854 - 1s/epoch - 54ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.9198 - accuracy: 0.6616 - val_loss: 0.7123 - val_accuracy: 0.7369 - 443ms/epoch - 18ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7926 - accuracy: 0.7124 - val_loss: 0.6645 - val_accuracy: 0.7507 - 456ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.7295 - accuracy: 0.7322 - val_loss: 0.6266 - val_accuracy: 0.7590 - 468ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6966 - accuracy: 0.7440 - val_loss: 0.6112 - val_accuracy: 0.7602 - 460ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6644 - accuracy: 0.7562 - val_loss: 0.6039 - val_accuracy: 0.7748 - 457ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.6422 - accuracy: 0.7656 - val_loss: 0.5834 - val_accuracy: 0.7817 - 455ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.6311 - accuracy: 0.7705 - val_loss: 0.5704 - val_accuracy: 0.7834 - 461ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.6148 - accuracy: 0.7761 - val_loss: 0.5587 - val_accuracy: 0.7984 - 434ms/epoch - 18ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.6080 - accuracy: 0.7812 - val_loss: 0.5487 - val_accuracy: 0.7964 - 468ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5970 - accuracy: 0.7836 - val_loss: 0.5236 - val_accuracy: 0.8094 - 461ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5838 - accuracy: 0.7898 - val_loss: 0.5207 - val_accuracy: 0.8092 - 451ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.5757 - accuracy: 0.7895 - val_loss: 0.5048 - val_accuracy: 0.8162 - 463ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.5656 - accuracy: 0.7959 - val_loss: 0.5250 - val_accuracy: 0.8064 - 465ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.5560 - accuracy: 0.8014 - val_loss: 0.4965 - val_accuracy: 0.8159 - 446ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.5492 - accuracy: 0.8051 - val_loss: 0.5119 - val_accuracy: 0.8117 - 444ms/epoch - 18ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.5457 - accuracy: 0.8056 - val_loss: 0.4793 - val_accuracy: 0.8265 - 447ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.5361 - accuracy: 0.8092 - val_loss: 0.5046 - val_accuracy: 0.8215 - 461ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.5274 - accuracy: 0.8109 - val_loss: 0.4738 - val_accuracy: 0.8319 - 465ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.5231 - accuracy: 0.8128 - val_loss: 0.4664 - val_accuracy: 0.8315 - 445ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1251 - accuracy: 0.2313 - val_loss: 1.7123 - val_accuracy: 0.4587 - 1s/epoch - 232ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.7563 - accuracy: 0.3574 - val_loss: 1.3090 - val_accuracy: 0.6036 - 428ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.5211 - accuracy: 0.4287 - val_loss: 1.1455 - val_accuracy: 0.6075 - 413ms/epoch - 69ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.3572 - accuracy: 0.4869 - val_loss: 1.0472 - val_accuracy: 0.6318 - 410ms/epoch - 68ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.2577 - accuracy: 0.5231 - val_loss: 1.0383 - val_accuracy: 0.6013 - 431ms/epoch - 72ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 1.2400 - accuracy: 0.5293 - val_loss: 0.9239 - val_accuracy: 0.6769 - 424ms/epoch - 71ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 1.1472 - accuracy: 0.5679 - val_loss: 0.8991 - val_accuracy: 0.6654 - 411ms/epoch - 68ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 1.1193 - accuracy: 0.5776 - val_loss: 0.8757 - val_accuracy: 0.7053 - 441ms/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 1.0823 - accuracy: 0.5950 - val_loss: 0.8538 - val_accuracy: 0.7121 - 416ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 1.0668 - accuracy: 0.5938 - val_loss: 0.8214 - val_accuracy: 0.7033 - 440ms/epoch - 73ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 1.0084 - accuracy: 0.6222 - val_loss: 0.7700 - val_accuracy: 0.7242 - 410ms/epoch - 68ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 1.0045 - accuracy: 0.6193 - val_loss: 0.7588 - val_accuracy: 0.7293 - 437ms/epoch - 73ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.9754 - accuracy: 0.6308 - val_loss: 0.7608 - val_accuracy: 0.7011 - 418ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.9766 - accuracy: 0.6284 - val_loss: 0.7447 - val_accuracy: 0.7257 - 434ms/epoch - 72ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.9525 - accuracy: 0.6424 - val_loss: 0.7290 - val_accuracy: 0.7382 - 435ms/epoch - 73ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.9283 - accuracy: 0.6488 - val_loss: 0.7874 - val_accuracy: 0.6950 - 431ms/epoch - 72ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.9277 - accuracy: 0.6510 - val_loss: 0.6990 - val_accuracy: 0.7407 - 408ms/epoch - 68ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.8972 - accuracy: 0.6620 - val_loss: 0.7102 - val_accuracy: 0.7235 - 414ms/epoch - 69ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.9105 - accuracy: 0.6554 - val_loss: 0.7306 - val_accuracy: 0.7227 - 436ms/epoch - 73ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.8746 - accuracy: 0.6700 - val_loss: 0.6897 - val_accuracy: 0.7406 - 428ms/epoch - 71ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.0128 - accuracy: 0.2852 - val_loss: 1.4818 - val_accuracy: 0.4681 - 1s/epoch - 206ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.4369 - accuracy: 0.4844 - val_loss: 1.1399 - val_accuracy: 0.6172 - 429ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.2078 - accuracy: 0.5530 - val_loss: 1.0329 - val_accuracy: 0.6219 - 447ms/epoch - 74ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.1324 - accuracy: 0.5669 - val_loss: 0.9233 - val_accuracy: 0.6676 - 432ms/epoch - 72ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.0473 - accuracy: 0.6022 - val_loss: 0.8781 - val_accuracy: 0.6780 - 407ms/epoch - 68ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.9645 - accuracy: 0.6465 - val_loss: 0.8549 - val_accuracy: 0.6694 - 410ms/epoch - 68ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.9554 - accuracy: 0.6394 - val_loss: 0.7776 - val_accuracy: 0.7103 - 415ms/epoch - 69ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.8917 - accuracy: 0.6706 - val_loss: 0.7813 - val_accuracy: 0.7014 - 434ms/epoch - 72ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.8552 - accuracy: 0.6875 - val_loss: 0.7086 - val_accuracy: 0.7458 - 413ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.8257 - accuracy: 0.6973 - val_loss: 0.7182 - val_accuracy: 0.7234 - 420ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.8151 - accuracy: 0.6988 - val_loss: 0.6813 - val_accuracy: 0.7464 - 436ms/epoch - 73ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7629 - accuracy: 0.7259 - val_loss: 0.6703 - val_accuracy: 0.7500 - 424ms/epoch - 71ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.7673 - accuracy: 0.7221 - val_loss: 0.6481 - val_accuracy: 0.7633 - 416ms/epoch - 69ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.7668 - accuracy: 0.7183 - val_loss: 0.6424 - val_accuracy: 0.7577 - 448ms/epoch - 75ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.7221 - accuracy: 0.7376 - val_loss: 0.6443 - val_accuracy: 0.7536 - 423ms/epoch - 71ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.7236 - accuracy: 0.7362 - val_loss: 0.6388 - val_accuracy: 0.7613 - 422ms/epoch - 70ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.7133 - accuracy: 0.7390 - val_loss: 0.6108 - val_accuracy: 0.7775 - 445ms/epoch - 74ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6855 - accuracy: 0.7522 - val_loss: 0.6084 - val_accuracy: 0.7747 - 430ms/epoch - 72ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6983 - accuracy: 0.7435 - val_loss: 0.5958 - val_accuracy: 0.7790 - 441ms/epoch - 74ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.6780 - accuracy: 0.7538 - val_loss: 0.6358 - val_accuracy: 0.7595 - 416ms/epoch - 69ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.6267 - accuracy: 0.7681 - val_loss: 0.4306 - val_accuracy: 0.8466 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 8s - loss: 0.4859 - accuracy: 0.8265 - val_loss: 0.4052 - val_accuracy: 0.8572 - 8s/epoch - 5ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 7s - loss: 0.4743 - accuracy: 0.8338 - val_loss: 0.3696 - val_accuracy: 0.8656 - 7s/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.4672 - accuracy: 0.8367 - val_loss: 0.3814 - val_accuracy: 0.8626 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 7s - loss: 0.4654 - accuracy: 0.8395 - val_loss: 0.3877 - val_accuracy: 0.8609 - 7s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.4618 - accuracy: 0.8412 - val_loss: 0.3900 - val_accuracy: 0.8626 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.4690 - accuracy: 0.8408 - val_loss: 0.3667 - val_accuracy: 0.8662 - 7s/epoch - 5ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.4668 - accuracy: 0.8404 - val_loss: 0.3906 - val_accuracy: 0.8527 - 7s/epoch - 5ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 7s - loss: 0.4736 - accuracy: 0.8401 - val_loss: 0.4024 - val_accuracy: 0.8558 - 7s/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4835 - accuracy: 0.8377 - val_loss: 0.5762 - val_accuracy: 0.8438 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4927 - accuracy: 0.8344 - val_loss: 0.4614 - val_accuracy: 0.8245 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 7s - loss: 0.4905 - accuracy: 0.8369 - val_loss: 0.4226 - val_accuracy: 0.8508 - 7s/epoch - 5ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.4903 - accuracy: 0.8347 - val_loss: 0.4150 - val_accuracy: 0.8553 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.4919 - accuracy: 0.8343 - val_loss: 0.3902 - val_accuracy: 0.8644 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 7s - loss: 0.4959 - accuracy: 0.8341 - val_loss: 0.4839 - val_accuracy: 0.8664 - 7s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.5045 - accuracy: 0.8300 - val_loss: 0.4292 - val_accuracy: 0.8466 - 7s/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.5034 - accuracy: 0.8330 - val_loss: 0.4466 - val_accuracy: 0.8558 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 7s - loss: 0.5023 - accuracy: 0.8334 - val_loss: 0.4037 - val_accuracy: 0.8619 - 7s/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.5132 - accuracy: 0.8291 - val_loss: 0.4468 - val_accuracy: 0.8586 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.5268 - accuracy: 0.8281 - val_loss: 0.4909 - val_accuracy: 0.8458 - 7s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.6390 - accuracy: 0.7643 - val_loss: 0.4919 - val_accuracy: 0.8185 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.5459 - accuracy: 0.8031 - val_loss: 0.5037 - val_accuracy: 0.8154 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.5299 - accuracy: 0.8082 - val_loss: 0.4810 - val_accuracy: 0.8290 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5212 - accuracy: 0.8136 - val_loss: 0.4708 - val_accuracy: 0.8306 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 8s - loss: 0.5175 - accuracy: 0.8151 - val_loss: 0.4699 - val_accuracy: 0.8327 - 8s/epoch - 5ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 7s - loss: 0.5113 - accuracy: 0.8159 - val_loss: 0.4706 - val_accuracy: 0.8334 - 7s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.5085 - accuracy: 0.8195 - val_loss: 0.4505 - val_accuracy: 0.8383 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 7s - loss: 0.5038 - accuracy: 0.8216 - val_loss: 0.4586 - val_accuracy: 0.8376 - 7s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.5046 - accuracy: 0.8204 - val_loss: 0.4720 - val_accuracy: 0.8342 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 7s - loss: 0.4986 - accuracy: 0.8225 - val_loss: 0.4497 - val_accuracy: 0.8446 - 7s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4959 - accuracy: 0.8242 - val_loss: 0.4364 - val_accuracy: 0.8418 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4899 - accuracy: 0.8270 - val_loss: 0.4465 - val_accuracy: 0.8428 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.4892 - accuracy: 0.8273 - val_loss: 0.4577 - val_accuracy: 0.8340 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 7s - loss: 0.4834 - accuracy: 0.8289 - val_loss: 0.4393 - val_accuracy: 0.8436 - 7s/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.4831 - accuracy: 0.8294 - val_loss: 0.4389 - val_accuracy: 0.8454 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 7s - loss: 0.4789 - accuracy: 0.8302 - val_loss: 0.4341 - val_accuracy: 0.8519 - 7s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 7s - loss: 0.4805 - accuracy: 0.8319 - val_loss: 0.4402 - val_accuracy: 0.8435 - 7s/epoch - 5ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.4761 - accuracy: 0.8323 - val_loss: 0.4324 - val_accuracy: 0.8451 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.4698 - accuracy: 0.8355 - val_loss: 0.4294 - val_accuracy: 0.8500 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 7s - loss: 0.4701 - accuracy: 0.8336 - val_loss: 0.4374 - val_accuracy: 0.8464 - 7s/epoch - 5ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.5294 - accuracy: 0.4601 - val_loss: 0.8254 - val_accuracy: 0.6907 - 1s/epoch - 56ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8377 - accuracy: 0.6818 - val_loss: 0.7154 - val_accuracy: 0.7198 - 456ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7130 - accuracy: 0.7234 - val_loss: 0.6358 - val_accuracy: 0.7459 - 454ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6379 - accuracy: 0.7541 - val_loss: 0.5626 - val_accuracy: 0.7733 - 450ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5861 - accuracy: 0.7748 - val_loss: 0.5383 - val_accuracy: 0.7969 - 438ms/epoch - 18ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5595 - accuracy: 0.7874 - val_loss: 0.4742 - val_accuracy: 0.8179 - 461ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5233 - accuracy: 0.8023 - val_loss: 0.5006 - val_accuracy: 0.8125 - 447ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4995 - accuracy: 0.8102 - val_loss: 0.4716 - val_accuracy: 0.8266 - 445ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4803 - accuracy: 0.8213 - val_loss: 0.4359 - val_accuracy: 0.8371 - 453ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4618 - accuracy: 0.8270 - val_loss: 0.4074 - val_accuracy: 0.8481 - 453ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4544 - accuracy: 0.8293 - val_loss: 0.3962 - val_accuracy: 0.8486 - 447ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4379 - accuracy: 0.8390 - val_loss: 0.4552 - val_accuracy: 0.8251 - 451ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4270 - accuracy: 0.8413 - val_loss: 0.3819 - val_accuracy: 0.8577 - 452ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4138 - accuracy: 0.8462 - val_loss: 0.3760 - val_accuracy: 0.8571 - 459ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4015 - accuracy: 0.8514 - val_loss: 0.3688 - val_accuracy: 0.8601 - 461ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.3992 - accuracy: 0.8526 - val_loss: 0.3596 - val_accuracy: 0.8654 - 444ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.3819 - accuracy: 0.8580 - val_loss: 0.3568 - val_accuracy: 0.8627 - 445ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.3827 - accuracy: 0.8587 - val_loss: 0.3265 - val_accuracy: 0.8788 - 455ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3726 - accuracy: 0.8602 - val_loss: 0.3353 - val_accuracy: 0.8803 - 460ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3712 - accuracy: 0.8619 - val_loss: 0.3219 - val_accuracy: 0.8833 - 450ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.2689 - accuracy: 0.5672 - val_loss: 0.8179 - val_accuracy: 0.6700 - 1s/epoch - 54ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.7715 - accuracy: 0.7096 - val_loss: 0.6704 - val_accuracy: 0.7407 - 451ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.6796 - accuracy: 0.7443 - val_loss: 0.5960 - val_accuracy: 0.7867 - 452ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6098 - accuracy: 0.7724 - val_loss: 0.5318 - val_accuracy: 0.8031 - 461ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.5600 - accuracy: 0.7943 - val_loss: 0.5160 - val_accuracy: 0.8060 - 446ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.5245 - accuracy: 0.8076 - val_loss: 0.5546 - val_accuracy: 0.7839 - 446ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.5119 - accuracy: 0.8130 - val_loss: 0.4769 - val_accuracy: 0.8187 - 454ms/epoch - 19ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.4866 - accuracy: 0.8210 - val_loss: 0.4510 - val_accuracy: 0.8324 - 457ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.4768 - accuracy: 0.8241 - val_loss: 0.4344 - val_accuracy: 0.8404 - 450ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.4573 - accuracy: 0.8331 - val_loss: 0.4160 - val_accuracy: 0.8487 - 443ms/epoch - 18ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.4517 - accuracy: 0.8354 - val_loss: 0.4159 - val_accuracy: 0.8481 - 446ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.4393 - accuracy: 0.8379 - val_loss: 0.4249 - val_accuracy: 0.8419 - 449ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.4397 - accuracy: 0.8388 - val_loss: 0.4092 - val_accuracy: 0.8484 - 441ms/epoch - 18ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.4222 - accuracy: 0.8453 - val_loss: 0.4028 - val_accuracy: 0.8520 - 463ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.4175 - accuracy: 0.8476 - val_loss: 0.3856 - val_accuracy: 0.8582 - 460ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.4131 - accuracy: 0.8488 - val_loss: 0.3870 - val_accuracy: 0.8597 - 463ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.4074 - accuracy: 0.8517 - val_loss: 0.3831 - val_accuracy: 0.8603 - 454ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4022 - accuracy: 0.8533 - val_loss: 0.3753 - val_accuracy: 0.8612 - 449ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.3972 - accuracy: 0.8555 - val_loss: 0.3796 - val_accuracy: 0.8619 - 454ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.3959 - accuracy: 0.8549 - val_loss: 0.3696 - val_accuracy: 0.8654 - 462ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 2s - loss: 2.4367 - accuracy: 0.2402 - val_loss: 1.8234 - val_accuracy: 0.4141 - 2s/epoch - 263ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.7811 - accuracy: 0.4185 - val_loss: 1.2678 - val_accuracy: 0.5683 - 426ms/epoch - 71ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.2349 - accuracy: 0.5805 - val_loss: 1.0032 - val_accuracy: 0.6794 - 427ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.0883 - accuracy: 0.6156 - val_loss: 0.9034 - val_accuracy: 0.6770 - 430ms/epoch - 72ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.9197 - accuracy: 0.6590 - val_loss: 0.8225 - val_accuracy: 0.6860 - 414ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.8586 - accuracy: 0.6695 - val_loss: 0.8890 - val_accuracy: 0.6740 - 428ms/epoch - 71ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.8913 - accuracy: 0.6677 - val_loss: 0.7023 - val_accuracy: 0.7331 - 433ms/epoch - 72ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7978 - accuracy: 0.7006 - val_loss: 0.7621 - val_accuracy: 0.7049 - 420ms/epoch - 70ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7484 - accuracy: 0.7136 - val_loss: 0.7503 - val_accuracy: 0.7201 - 421ms/epoch - 70ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.7584 - accuracy: 0.7082 - val_loss: 0.6911 - val_accuracy: 0.7522 - 435ms/epoch - 72ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.7056 - accuracy: 0.7281 - val_loss: 0.6942 - val_accuracy: 0.7406 - 420ms/epoch - 70ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.7012 - accuracy: 0.7363 - val_loss: 0.5837 - val_accuracy: 0.7706 - 428ms/epoch - 71ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6631 - accuracy: 0.7468 - val_loss: 0.6087 - val_accuracy: 0.7482 - 414ms/epoch - 69ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6311 - accuracy: 0.7543 - val_loss: 0.5720 - val_accuracy: 0.7819 - 433ms/epoch - 72ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6482 - accuracy: 0.7566 - val_loss: 0.5715 - val_accuracy: 0.7769 - 415ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6121 - accuracy: 0.7662 - val_loss: 0.5822 - val_accuracy: 0.7764 - 430ms/epoch - 72ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5891 - accuracy: 0.7745 - val_loss: 0.5306 - val_accuracy: 0.7905 - 425ms/epoch - 71ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5977 - accuracy: 0.7720 - val_loss: 0.5102 - val_accuracy: 0.8089 - 430ms/epoch - 72ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5635 - accuracy: 0.7829 - val_loss: 0.6017 - val_accuracy: 0.7603 - 415ms/epoch - 69ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5765 - accuracy: 0.7840 - val_loss: 0.5050 - val_accuracy: 0.8058 - 421ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.3929 - accuracy: 0.2884 - val_loss: 1.3940 - val_accuracy: 0.4519 - 1s/epoch - 207ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1240 - accuracy: 0.5982 - val_loss: 0.9005 - val_accuracy: 0.6678 - 406ms/epoch - 68ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9438 - accuracy: 0.6422 - val_loss: 0.7968 - val_accuracy: 0.6996 - 426ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8935 - accuracy: 0.6583 - val_loss: 0.7705 - val_accuracy: 0.7051 - 414ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.7952 - accuracy: 0.6958 - val_loss: 0.7579 - val_accuracy: 0.7055 - 413ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7471 - accuracy: 0.7120 - val_loss: 0.7431 - val_accuracy: 0.7096 - 431ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7538 - accuracy: 0.7128 - val_loss: 0.6657 - val_accuracy: 0.7456 - 428ms/epoch - 71ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.6879 - accuracy: 0.7414 - val_loss: 0.6315 - val_accuracy: 0.7527 - 414ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7038 - accuracy: 0.7302 - val_loss: 0.7309 - val_accuracy: 0.7271 - 414ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6846 - accuracy: 0.7441 - val_loss: 0.6053 - val_accuracy: 0.7683 - 409ms/epoch - 68ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6047 - accuracy: 0.7749 - val_loss: 0.6484 - val_accuracy: 0.7570 - 428ms/epoch - 71ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6398 - accuracy: 0.7571 - val_loss: 0.5537 - val_accuracy: 0.7934 - 432ms/epoch - 72ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6058 - accuracy: 0.7701 - val_loss: 0.6162 - val_accuracy: 0.7658 - 418ms/epoch - 70ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6015 - accuracy: 0.7777 - val_loss: 0.5567 - val_accuracy: 0.7929 - 424ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.5873 - accuracy: 0.7798 - val_loss: 0.5520 - val_accuracy: 0.7921 - 416ms/epoch - 69ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.5508 - accuracy: 0.7942 - val_loss: 0.5774 - val_accuracy: 0.7850 - 424ms/epoch - 71ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.5660 - accuracy: 0.7880 - val_loss: 0.5693 - val_accuracy: 0.7783 - 422ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.5535 - accuracy: 0.7913 - val_loss: 0.5138 - val_accuracy: 0.8103 - 435ms/epoch - 73ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.5587 - accuracy: 0.7933 - val_loss: 0.4975 - val_accuracy: 0.8132 - 432ms/epoch - 72ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.5020 - accuracy: 0.8169 - val_loss: 0.4767 - val_accuracy: 0.8250 - 417ms/epoch - 70ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 8s - loss: 0.8506 - accuracy: 0.6844 - val_loss: 0.5341 - val_accuracy: 0.8062 - 8s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 7s - loss: 0.6334 - accuracy: 0.7732 - val_loss: 0.4635 - val_accuracy: 0.8374 - 7s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.6152 - accuracy: 0.7881 - val_loss: 0.4753 - val_accuracy: 0.8356 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.6166 - accuracy: 0.7947 - val_loss: 0.4869 - val_accuracy: 0.8267 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.6199 - accuracy: 0.8000 - val_loss: 0.4944 - val_accuracy: 0.8343 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.6263 - accuracy: 0.7961 - val_loss: 0.5069 - val_accuracy: 0.8278 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 6s - loss: 0.6392 - accuracy: 0.7966 - val_loss: 0.4771 - val_accuracy: 0.8486 - 6s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.6661 - accuracy: 0.7937 - val_loss: 0.5672 - val_accuracy: 0.8065 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.6723 - accuracy: 0.7895 - val_loss: 0.5436 - val_accuracy: 0.8350 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.6774 - accuracy: 0.7893 - val_loss: 0.5095 - val_accuracy: 0.8342 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.6821 - accuracy: 0.7893 - val_loss: 0.7684 - val_accuracy: 0.7560 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.6899 - accuracy: 0.7861 - val_loss: 0.5233 - val_accuracy: 0.8159 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.7104 - accuracy: 0.7828 - val_loss: 0.5766 - val_accuracy: 0.8067 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.7258 - accuracy: 0.7805 - val_loss: 0.5701 - val_accuracy: 0.8018 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.7280 - accuracy: 0.7793 - val_loss: 0.5476 - val_accuracy: 0.8089 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.7197 - accuracy: 0.7814 - val_loss: 0.6452 - val_accuracy: 0.8143 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.7387 - accuracy: 0.7782 - val_loss: 0.5459 - val_accuracy: 0.8250 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.7362 - accuracy: 0.7772 - val_loss: 0.5611 - val_accuracy: 0.8096 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.7445 - accuracy: 0.7781 - val_loss: 0.5814 - val_accuracy: 0.7922 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.7422 - accuracy: 0.7772 - val_loss: 0.6054 - val_accuracy: 0.7944 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "1499/1499 - 7s - loss: 0.7495 - accuracy: 0.7279 - val_loss: 0.5450 - val_accuracy: 0.7972 - 7s/epoch - 5ms/step\n",
      "Epoch 2/20\n",
      "1499/1499 - 6s - loss: 0.6061 - accuracy: 0.7868 - val_loss: 0.4841 - val_accuracy: 0.8286 - 6s/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "1499/1499 - 6s - loss: 0.5638 - accuracy: 0.8052 - val_loss: 0.4631 - val_accuracy: 0.8380 - 6s/epoch - 4ms/step\n",
      "Epoch 4/20\n",
      "1499/1499 - 6s - loss: 0.5315 - accuracy: 0.8168 - val_loss: 0.4650 - val_accuracy: 0.8386 - 6s/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "1499/1499 - 6s - loss: 0.5136 - accuracy: 0.8256 - val_loss: 0.4650 - val_accuracy: 0.8438 - 6s/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "1499/1499 - 6s - loss: 0.5013 - accuracy: 0.8306 - val_loss: 0.4505 - val_accuracy: 0.8542 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1499/1499 - 7s - loss: 0.4914 - accuracy: 0.8354 - val_loss: 0.4209 - val_accuracy: 0.8591 - 7s/epoch - 4ms/step\n",
      "Epoch 8/20\n",
      "1499/1499 - 6s - loss: 0.4816 - accuracy: 0.8384 - val_loss: 0.4257 - val_accuracy: 0.8596 - 6s/epoch - 4ms/step\n",
      "Epoch 9/20\n",
      "1499/1499 - 6s - loss: 0.4737 - accuracy: 0.8414 - val_loss: 0.4198 - val_accuracy: 0.8602 - 6s/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "1499/1499 - 6s - loss: 0.4631 - accuracy: 0.8449 - val_loss: 0.4162 - val_accuracy: 0.8622 - 6s/epoch - 4ms/step\n",
      "Epoch 11/20\n",
      "1499/1499 - 6s - loss: 0.4596 - accuracy: 0.8489 - val_loss: 0.4410 - val_accuracy: 0.8627 - 6s/epoch - 4ms/step\n",
      "Epoch 12/20\n",
      "1499/1499 - 6s - loss: 0.4542 - accuracy: 0.8502 - val_loss: 0.4074 - val_accuracy: 0.8652 - 6s/epoch - 4ms/step\n",
      "Epoch 13/20\n",
      "1499/1499 - 6s - loss: 0.4456 - accuracy: 0.8529 - val_loss: 0.4071 - val_accuracy: 0.8613 - 6s/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "1499/1499 - 6s - loss: 0.4460 - accuracy: 0.8519 - val_loss: 0.4197 - val_accuracy: 0.8625 - 6s/epoch - 4ms/step\n",
      "Epoch 15/20\n",
      "1499/1499 - 6s - loss: 0.4415 - accuracy: 0.8549 - val_loss: 0.4254 - val_accuracy: 0.8658 - 6s/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "1499/1499 - 6s - loss: 0.4383 - accuracy: 0.8543 - val_loss: 0.4296 - val_accuracy: 0.8642 - 6s/epoch - 4ms/step\n",
      "Epoch 17/20\n",
      "1499/1499 - 6s - loss: 0.4410 - accuracy: 0.8556 - val_loss: 0.4407 - val_accuracy: 0.8611 - 6s/epoch - 4ms/step\n",
      "Epoch 18/20\n",
      "1499/1499 - 6s - loss: 0.4321 - accuracy: 0.8594 - val_loss: 0.4093 - val_accuracy: 0.8669 - 6s/epoch - 4ms/step\n",
      "Epoch 19/20\n",
      "1499/1499 - 6s - loss: 0.4281 - accuracy: 0.8605 - val_loss: 0.4162 - val_accuracy: 0.8685 - 6s/epoch - 4ms/step\n",
      "Epoch 20/20\n",
      "1499/1499 - 6s - loss: 0.4267 - accuracy: 0.8602 - val_loss: 0.4071 - val_accuracy: 0.8701 - 6s/epoch - 4ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.6330 - accuracy: 0.3790 - val_loss: 0.9232 - val_accuracy: 0.6699 - 1s/epoch - 56ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 1.0303 - accuracy: 0.6015 - val_loss: 0.7494 - val_accuracy: 0.7073 - 454ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.8788 - accuracy: 0.6565 - val_loss: 0.6750 - val_accuracy: 0.7461 - 464ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.7983 - accuracy: 0.6935 - val_loss: 0.6255 - val_accuracy: 0.7568 - 442ms/epoch - 18ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.7322 - accuracy: 0.7189 - val_loss: 0.6816 - val_accuracy: 0.7119 - 449ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.7100 - accuracy: 0.7291 - val_loss: 0.5772 - val_accuracy: 0.7688 - 462ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.6589 - accuracy: 0.7490 - val_loss: 0.5793 - val_accuracy: 0.7683 - 444ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.6421 - accuracy: 0.7561 - val_loss: 0.5438 - val_accuracy: 0.8019 - 446ms/epoch - 19ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.6158 - accuracy: 0.7667 - val_loss: 0.4831 - val_accuracy: 0.8193 - 448ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5954 - accuracy: 0.7766 - val_loss: 0.5226 - val_accuracy: 0.7776 - 445ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5760 - accuracy: 0.7849 - val_loss: 0.4639 - val_accuracy: 0.8250 - 470ms/epoch - 20ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5650 - accuracy: 0.7908 - val_loss: 0.4839 - val_accuracy: 0.8038 - 447ms/epoch - 19ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.5480 - accuracy: 0.7975 - val_loss: 0.4557 - val_accuracy: 0.8295 - 461ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.5378 - accuracy: 0.8029 - val_loss: 0.4684 - val_accuracy: 0.8266 - 470ms/epoch - 20ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.5263 - accuracy: 0.8070 - val_loss: 0.4680 - val_accuracy: 0.8292 - 453ms/epoch - 19ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.5161 - accuracy: 0.8109 - val_loss: 0.4498 - val_accuracy: 0.8316 - 447ms/epoch - 19ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.5023 - accuracy: 0.8164 - val_loss: 0.4430 - val_accuracy: 0.8193 - 465ms/epoch - 19ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4917 - accuracy: 0.8190 - val_loss: 0.4362 - val_accuracy: 0.8471 - 452ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4893 - accuracy: 0.8218 - val_loss: 0.4626 - val_accuracy: 0.8277 - 472ms/epoch - 20ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4800 - accuracy: 0.8246 - val_loss: 0.4467 - val_accuracy: 0.8351 - 454ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "24/24 - 1s - loss: 1.3329 - accuracy: 0.4989 - val_loss: 0.7541 - val_accuracy: 0.7240 - 1s/epoch - 53ms/step\n",
      "Epoch 2/20\n",
      "24/24 - 0s - loss: 0.8354 - accuracy: 0.6869 - val_loss: 0.6479 - val_accuracy: 0.7579 - 450ms/epoch - 19ms/step\n",
      "Epoch 3/20\n",
      "24/24 - 0s - loss: 0.7322 - accuracy: 0.7262 - val_loss: 0.6051 - val_accuracy: 0.7724 - 450ms/epoch - 19ms/step\n",
      "Epoch 4/20\n",
      "24/24 - 0s - loss: 0.6800 - accuracy: 0.7483 - val_loss: 0.5739 - val_accuracy: 0.7838 - 464ms/epoch - 19ms/step\n",
      "Epoch 5/20\n",
      "24/24 - 0s - loss: 0.6464 - accuracy: 0.7637 - val_loss: 0.5750 - val_accuracy: 0.7868 - 458ms/epoch - 19ms/step\n",
      "Epoch 6/20\n",
      "24/24 - 0s - loss: 0.6231 - accuracy: 0.7729 - val_loss: 0.5590 - val_accuracy: 0.7914 - 449ms/epoch - 19ms/step\n",
      "Epoch 7/20\n",
      "24/24 - 0s - loss: 0.6054 - accuracy: 0.7840 - val_loss: 0.5273 - val_accuracy: 0.8064 - 438ms/epoch - 18ms/step\n",
      "Epoch 8/20\n",
      "24/24 - 0s - loss: 0.5840 - accuracy: 0.7883 - val_loss: 0.5596 - val_accuracy: 0.7976 - 476ms/epoch - 20ms/step\n",
      "Epoch 9/20\n",
      "24/24 - 0s - loss: 0.5808 - accuracy: 0.7907 - val_loss: 0.5236 - val_accuracy: 0.8024 - 468ms/epoch - 19ms/step\n",
      "Epoch 10/20\n",
      "24/24 - 0s - loss: 0.5584 - accuracy: 0.8003 - val_loss: 0.4925 - val_accuracy: 0.8197 - 459ms/epoch - 19ms/step\n",
      "Epoch 11/20\n",
      "24/24 - 0s - loss: 0.5513 - accuracy: 0.8017 - val_loss: 0.4798 - val_accuracy: 0.8297 - 464ms/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "24/24 - 0s - loss: 0.5378 - accuracy: 0.8099 - val_loss: 0.4643 - val_accuracy: 0.8285 - 468ms/epoch - 20ms/step\n",
      "Epoch 13/20\n",
      "24/24 - 0s - loss: 0.5223 - accuracy: 0.8166 - val_loss: 0.5032 - val_accuracy: 0.8187 - 467ms/epoch - 19ms/step\n",
      "Epoch 14/20\n",
      "24/24 - 0s - loss: 0.5221 - accuracy: 0.8156 - val_loss: 0.4687 - val_accuracy: 0.8321 - 452ms/epoch - 19ms/step\n",
      "Epoch 15/20\n",
      "24/24 - 0s - loss: 0.5144 - accuracy: 0.8193 - val_loss: 0.4865 - val_accuracy: 0.8309 - 468ms/epoch - 20ms/step\n",
      "Epoch 16/20\n",
      "24/24 - 0s - loss: 0.5070 - accuracy: 0.8218 - val_loss: 0.4605 - val_accuracy: 0.8344 - 468ms/epoch - 20ms/step\n",
      "Epoch 17/20\n",
      "24/24 - 0s - loss: 0.5038 - accuracy: 0.8218 - val_loss: 0.4510 - val_accuracy: 0.8382 - 444ms/epoch - 18ms/step\n",
      "Epoch 18/20\n",
      "24/24 - 0s - loss: 0.4911 - accuracy: 0.8272 - val_loss: 0.4612 - val_accuracy: 0.8335 - 449ms/epoch - 19ms/step\n",
      "Epoch 19/20\n",
      "24/24 - 0s - loss: 0.4891 - accuracy: 0.8267 - val_loss: 0.4156 - val_accuracy: 0.8521 - 460ms/epoch - 19ms/step\n",
      "Epoch 20/20\n",
      "24/24 - 0s - loss: 0.4801 - accuracy: 0.8313 - val_loss: 0.4223 - val_accuracy: 0.8507 - 446ms/epoch - 19ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 2.1894 - accuracy: 0.1867 - val_loss: 1.5692 - val_accuracy: 0.4669 - 1s/epoch - 210ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.6993 - accuracy: 0.3625 - val_loss: 1.1437 - val_accuracy: 0.6525 - 440ms/epoch - 73ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 1.3437 - accuracy: 0.4734 - val_loss: 1.2644 - val_accuracy: 0.4496 - 423ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 1.2138 - accuracy: 0.5335 - val_loss: 0.9358 - val_accuracy: 0.6439 - 420ms/epoch - 70ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 1.1062 - accuracy: 0.5721 - val_loss: 0.8759 - val_accuracy: 0.6649 - 416ms/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 1.1040 - accuracy: 0.5754 - val_loss: 0.8038 - val_accuracy: 0.7062 - 413ms/epoch - 69ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.9869 - accuracy: 0.6237 - val_loss: 0.9463 - val_accuracy: 0.5963 - 404ms/epoch - 67ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 1.0045 - accuracy: 0.6189 - val_loss: 0.7529 - val_accuracy: 0.7067 - 415ms/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.9216 - accuracy: 0.6450 - val_loss: 0.7598 - val_accuracy: 0.6919 - 437ms/epoch - 73ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.8902 - accuracy: 0.6622 - val_loss: 0.7014 - val_accuracy: 0.7197 - 414ms/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.8789 - accuracy: 0.6609 - val_loss: 0.6631 - val_accuracy: 0.7468 - 417ms/epoch - 69ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.8269 - accuracy: 0.6871 - val_loss: 0.6719 - val_accuracy: 0.7463 - 436ms/epoch - 73ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.8458 - accuracy: 0.6835 - val_loss: 0.6598 - val_accuracy: 0.7508 - 430ms/epoch - 72ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.7959 - accuracy: 0.6982 - val_loss: 0.6495 - val_accuracy: 0.7499 - 418ms/epoch - 70ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.7935 - accuracy: 0.6959 - val_loss: 0.6273 - val_accuracy: 0.7614 - 423ms/epoch - 70ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.8022 - accuracy: 0.6963 - val_loss: 0.5957 - val_accuracy: 0.7704 - 424ms/epoch - 71ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.7394 - accuracy: 0.7204 - val_loss: 0.6480 - val_accuracy: 0.7253 - 428ms/epoch - 71ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.7451 - accuracy: 0.7138 - val_loss: 0.6220 - val_accuracy: 0.7441 - 422ms/epoch - 70ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.7554 - accuracy: 0.7075 - val_loss: 0.5709 - val_accuracy: 0.7800 - 413ms/epoch - 69ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.7053 - accuracy: 0.7321 - val_loss: 0.5953 - val_accuracy: 0.7510 - 430ms/epoch - 72ms/step\n",
      "Epoch 1/20\n",
      "6/6 - 1s - loss: 1.8379 - accuracy: 0.3384 - val_loss: 1.2287 - val_accuracy: 0.5050 - 1s/epoch - 208ms/step\n",
      "Epoch 2/20\n",
      "6/6 - 0s - loss: 1.1436 - accuracy: 0.5869 - val_loss: 0.8975 - val_accuracy: 0.6543 - 417ms/epoch - 69ms/step\n",
      "Epoch 3/20\n",
      "6/6 - 0s - loss: 0.9920 - accuracy: 0.6276 - val_loss: 0.7561 - val_accuracy: 0.7211 - 429ms/epoch - 71ms/step\n",
      "Epoch 4/20\n",
      "6/6 - 0s - loss: 0.8846 - accuracy: 0.6740 - val_loss: 0.6993 - val_accuracy: 0.7277 - 413ms/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "6/6 - 0s - loss: 0.8143 - accuracy: 0.6944 - val_loss: 0.7125 - val_accuracy: 0.7195 - 421ms/epoch - 70ms/step\n",
      "Epoch 6/20\n",
      "6/6 - 0s - loss: 0.7942 - accuracy: 0.7044 - val_loss: 0.6789 - val_accuracy: 0.7311 - 434ms/epoch - 72ms/step\n",
      "Epoch 7/20\n",
      "6/6 - 0s - loss: 0.7431 - accuracy: 0.7241 - val_loss: 0.6301 - val_accuracy: 0.7658 - 435ms/epoch - 73ms/step\n",
      "Epoch 8/20\n",
      "6/6 - 0s - loss: 0.7177 - accuracy: 0.7392 - val_loss: 0.7476 - val_accuracy: 0.7215 - 407ms/epoch - 68ms/step\n",
      "Epoch 9/20\n",
      "6/6 - 0s - loss: 0.7506 - accuracy: 0.7236 - val_loss: 0.6492 - val_accuracy: 0.7528 - 411ms/epoch - 69ms/step\n",
      "Epoch 10/20\n",
      "6/6 - 0s - loss: 0.6924 - accuracy: 0.7477 - val_loss: 0.6756 - val_accuracy: 0.7338 - 418ms/epoch - 70ms/step\n",
      "Epoch 11/20\n",
      "6/6 - 0s - loss: 0.6751 - accuracy: 0.7478 - val_loss: 0.6385 - val_accuracy: 0.7609 - 422ms/epoch - 70ms/step\n",
      "Epoch 12/20\n",
      "6/6 - 0s - loss: 0.6806 - accuracy: 0.7510 - val_loss: 0.6132 - val_accuracy: 0.7583 - 427ms/epoch - 71ms/step\n",
      "Epoch 13/20\n",
      "6/6 - 0s - loss: 0.6532 - accuracy: 0.7644 - val_loss: 0.5918 - val_accuracy: 0.7813 - 437ms/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "6/6 - 0s - loss: 0.6672 - accuracy: 0.7542 - val_loss: 0.6152 - val_accuracy: 0.7748 - 425ms/epoch - 71ms/step\n",
      "Epoch 15/20\n",
      "6/6 - 0s - loss: 0.6628 - accuracy: 0.7604 - val_loss: 0.5820 - val_accuracy: 0.7864 - 438ms/epoch - 73ms/step\n",
      "Epoch 16/20\n",
      "6/6 - 0s - loss: 0.6424 - accuracy: 0.7648 - val_loss: 0.5533 - val_accuracy: 0.7942 - 444ms/epoch - 74ms/step\n",
      "Epoch 17/20\n",
      "6/6 - 0s - loss: 0.6073 - accuracy: 0.7782 - val_loss: 0.5535 - val_accuracy: 0.7893 - 422ms/epoch - 70ms/step\n",
      "Epoch 18/20\n",
      "6/6 - 0s - loss: 0.6281 - accuracy: 0.7682 - val_loss: 0.5663 - val_accuracy: 0.7882 - 412ms/epoch - 69ms/step\n",
      "Epoch 19/20\n",
      "6/6 - 0s - loss: 0.6031 - accuracy: 0.7842 - val_loss: 0.5637 - val_accuracy: 0.8003 - 438ms/epoch - 73ms/step\n",
      "Epoch 20/20\n",
      "6/6 - 0s - loss: 0.6397 - accuracy: 0.7689 - val_loss: 0.5548 - val_accuracy: 0.7959 - 420ms/epoch - 70ms/step\n"
     ]
    }
   ],
   "source": [
    "for optimizer in optimizers:\n",
    "    for dropout in dropouts:\n",
    "        for learning_rate in learning_rates:\n",
    "            for kernel_size in kernel_sizes:\n",
    "                for batch_size in batch_sizes:\n",
    "                    for activation in activations:\n",
    "                        NAME = f\"optimizer {optimizer}- dropout {dropout}- learning rate {learning_rate}- kernel size {kernel_size}- stride {stride} batch size {batch_size} activation {activation}-{int(time.time())}\" \n",
    "                        tensorboard = TensorBoard(log_dir=\"Logs\\{}\".format(NAME))\n",
    "                        model = Sequential([\n",
    "                            keras.layers.Conv2D(6, kernel_size=kernel_size, strides=1,  activation=activation, input_shape=x_train[0].shape, padding='same'), #C1\n",
    "                            keras.layers.AveragePooling2D(), #S2\n",
    "                            keras.layers.Conv2D(16, kernel_size=kernel_size, strides=1, activation=activation, padding='valid'), #C3\n",
    "                            keras.layers.AveragePooling2D(), #S4\n",
    "                            keras.layers.Conv2D(120, kernel_size=kernel_size, strides=1, activation=activation, padding='valid'), #C5\n",
    "                            keras.layers.Flatten(), #Flatten\n",
    "                            keras.layers.Dropout(dropout),\n",
    "                            keras.layers.Dense(84, activation=activation), #F6\n",
    "                            keras.layers.Dense(10, activation='softmax') #Output layer\n",
    "                        ])\n",
    "                        if optimizer == \"sgd\":\n",
    "                            opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "                        elif optimizer == \"adam\":\n",
    "                            opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "                        elif optimizer == 'rmsprop':\n",
    "                            opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "                        model.compile(loss='CategoricalCrossentropy',optimizer=opt,metrics='accuracy')\n",
    "                        model.fit(x_train,y_train_encoded,epochs = 20,batch_size = batch_size,callbacks=[tensorboard],validation_data=(x_cv,y_cv_encoded),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ixeqwjfOvZta",
   "metadata": {
    "id": "ixeqwjfOvZta"
   },
   "outputs": [],
   "source": [
    "! cd ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DY3tvry0vq-I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DY3tvry0vq-I",
    "outputId": "02cde2da-824e-4439-f88d-4d7b150138da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '. ls'\n",
      "/content/gdrive/MyDrive/Datasets/fashionMnist\n"
     ]
    }
   ],
   "source": [
    "%cd . ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ODnWd1mv7es",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ODnWd1mv7es",
    "outputId": "e4cbb385-edbd-44b7-e609-33f574c3bdf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/Datasets\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97hdoEU0wRHB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97hdoEU0wRHB",
    "outputId": "2b7906d0-b1bf-4641-e32b-49b300a439f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: fashionMnist/ (stored 0%)\n",
      "  adding: fashionMnist/t10k-labels-idx1-ubyte (deflated 49%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation relu-1672671964/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation relu-1672671964/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation relu-1672671964/train/events.out.tfevents.1672671965.ce8d0207b981.377.12.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation relu-1672671964/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation relu-1672671964/validation/events.out.tfevents.1672671970.ce8d0207b981.377.13.v2 (deflated 76%)\n",
      "  adding: fashionMnist/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation tanh-1672672079/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation tanh-1672672079/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation tanh-1672672079/train/events.out.tfevents.1672672080.ce8d0207b981.377.14.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation tanh-1672672079/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 32 activation tanh-1672672079/validation/events.out.tfevents.1672672085.ce8d0207b981.377.15.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 2048 activation tanh-1672672233/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 2048 activation tanh-1672672233/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 2048 activation tanh-1672672233/train/events.out.tfevents.1672672233.ce8d0207b981.377.18.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 2048 activation tanh-1672672233/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 2048 activation tanh-1672672233/validation/events.out.tfevents.1672672234.ce8d0207b981.377.19.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 8192 activation relu-1672672242/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 8192 activation relu-1672672242/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 8192 activation relu-1672672242/train/events.out.tfevents.1672672243.ce8d0207b981.377.20.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 8192 activation relu-1672672242/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs1\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 1 batch size 8192 activation relu-1672672242/validation/events.out.tfevents.1672672243.ce8d0207b981.377.21.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672672547/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672672547/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672672547/train/events.out.tfevents.1672672547.ce8d0207b981.377.25.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672672547/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672672547/validation/events.out.tfevents.1672672552.ce8d0207b981.377.26.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672672665/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672672665/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672672665/train/events.out.tfevents.1672672665.ce8d0207b981.377.27.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672672665/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672672665/validation/events.out.tfevents.1672672670.ce8d0207b981.377.28.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672672780/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672672780/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672672780/train/events.out.tfevents.1672672780.ce8d0207b981.377.29.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672672780/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672672780/validation/events.out.tfevents.1672672781.ce8d0207b981.377.30.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672672790/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672672790/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672672790/train/events.out.tfevents.1672672791.ce8d0207b981.377.31.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672672790/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672672790/validation/events.out.tfevents.1672672791.ce8d0207b981.377.32.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672672801/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672672801/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672672801/train/events.out.tfevents.1672672801.ce8d0207b981.377.33.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672672801/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672672801/validation/events.out.tfevents.1672672802.ce8d0207b981.377.34.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672672812/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672672812/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672672812/train/events.out.tfevents.1672672812.ce8d0207b981.377.35.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672672812/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672672812/validation/events.out.tfevents.1672672813.ce8d0207b981.377.36.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672672821/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672672821/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672672821/train/events.out.tfevents.1672672821.ce8d0207b981.377.37.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672672821/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672672821/validation/events.out.tfevents.1672672826.ce8d0207b981.377.38.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672672963/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672672963/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672672963/train/events.out.tfevents.1672672964.ce8d0207b981.377.39.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672672963/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672672963/validation/events.out.tfevents.1672672968.ce8d0207b981.377.40.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672673106/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672673106/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672673106/train/events.out.tfevents.1672673106.ce8d0207b981.377.41.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672673106/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672673106/validation/events.out.tfevents.1672673107.ce8d0207b981.377.42.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672673117/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672673117/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672673117/train/events.out.tfevents.1672673117.ce8d0207b981.377.43.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672673117/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672673117/validation/events.out.tfevents.1672673118.ce8d0207b981.377.44.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672673126/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672673126/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672673126/train/events.out.tfevents.1672673127.ce8d0207b981.377.45.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672673126/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672673126/validation/events.out.tfevents.1672673130.ce8d0207b981.377.46.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672673148/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672673148/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672673148/train/events.out.tfevents.1672673148.ce8d0207b981.377.47.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672673148/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672673148/validation/events.out.tfevents.1672673149.ce8d0207b981.377.48.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672673157/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672673157/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672673157/train/events.out.tfevents.1672673157.ce8d0207b981.377.49.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672673157/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672673157/validation/events.out.tfevents.1672673162.ce8d0207b981.377.50.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672673299/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672673299/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672673299/train/events.out.tfevents.1672673300.ce8d0207b981.377.51.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672673299/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672673299/validation/events.out.tfevents.1672673305.ce8d0207b981.377.52.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672673442/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672673442/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672673442/train/events.out.tfevents.1672673443.ce8d0207b981.377.53.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672673442/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672673442/validation/events.out.tfevents.1672673443.ce8d0207b981.377.54.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672673452/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672673452/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672673452/train/events.out.tfevents.1672673452.ce8d0207b981.377.55.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672673452/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672673452/validation/events.out.tfevents.1672673453.ce8d0207b981.377.56.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672673463/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672673463/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672673463/train/events.out.tfevents.1672673463.ce8d0207b981.377.57.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672673463/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672673463/validation/events.out.tfevents.1672673464.ce8d0207b981.377.58.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672673473/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672673473/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672673473/train/events.out.tfevents.1672673474.ce8d0207b981.377.59.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672673473/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672673473/validation/events.out.tfevents.1672673474.ce8d0207b981.377.60.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672673484/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672673484/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672673484/train/events.out.tfevents.1672673484.ce8d0207b981.377.61.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672673484/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672673484/validation/events.out.tfevents.1672673489.ce8d0207b981.377.62.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672673590/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672673590/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672673590/train/events.out.tfevents.1672673590.ce8d0207b981.377.63.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672673590/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672673590/validation/events.out.tfevents.1672673595.ce8d0207b981.377.64.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672673732/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672673732/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672673732/train/events.out.tfevents.1672673733.ce8d0207b981.377.65.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672673732/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672673732/validation/events.out.tfevents.1672673734.ce8d0207b981.377.66.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672673743/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672673743/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672673743/train/events.out.tfevents.1672673743.ce8d0207b981.377.67.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672673743/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672673743/validation/events.out.tfevents.1672673744.ce8d0207b981.377.68.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672673754/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672673754/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672673754/train/events.out.tfevents.1672673754.ce8d0207b981.377.69.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672673754/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672673754/validation/events.out.tfevents.1672673755.ce8d0207b981.377.70.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672673765/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672673765/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672673765/train/events.out.tfevents.1672673765.ce8d0207b981.377.71.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672673765/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672673765/validation/events.out.tfevents.1672673766.ce8d0207b981.377.72.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672673774/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672673774/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672673774/train/events.out.tfevents.1672673774.ce8d0207b981.377.73.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672673774/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672673774/validation/events.out.tfevents.1672673779.ce8d0207b981.377.74.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672673888/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672673888/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672673888/train/events.out.tfevents.1672673888.ce8d0207b981.377.75.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672673888/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672673888/validation/events.out.tfevents.1672673893.ce8d0207b981.377.76.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672674002/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672674002/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672674002/train/events.out.tfevents.1672674003.ce8d0207b981.377.77.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672674002/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672674002/validation/events.out.tfevents.1672674004.ce8d0207b981.377.78.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672674012/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672674012/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672674012/train/events.out.tfevents.1672674013.ce8d0207b981.377.79.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672674012/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672674012/validation/events.out.tfevents.1672674013.ce8d0207b981.377.80.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672674022/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672674022/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672674022/train/events.out.tfevents.1672674022.ce8d0207b981.377.81.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672674022/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672674022/validation/events.out.tfevents.1672674023.ce8d0207b981.377.82.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672674033/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672674033/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672674033/train/events.out.tfevents.1672674033.ce8d0207b981.377.83.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672674033/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672674033/validation/events.out.tfevents.1672674034.ce8d0207b981.377.84.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672674044/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672674044/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672674044/train/events.out.tfevents.1672674044.ce8d0207b981.377.85.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672674044/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672674044/validation/events.out.tfevents.1672674049.ce8d0207b981.377.86.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672674186/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672674186/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672674186/train/events.out.tfevents.1672674186.ce8d0207b981.377.87.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672674186/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672674186/validation/events.out.tfevents.1672674191.ce8d0207b981.377.88.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672674329/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672674329/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672674329/train/events.out.tfevents.1672674329.ce8d0207b981.377.89.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672674329/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672674329/validation/events.out.tfevents.1672674330.ce8d0207b981.377.90.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672674340/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672674340/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672674340/train/events.out.tfevents.1672674340.ce8d0207b981.377.91.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672674340/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672674340/validation/events.out.tfevents.1672674341.ce8d0207b981.377.92.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672674350/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672674350/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672674350/train/events.out.tfevents.1672674351.ce8d0207b981.377.93.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672674350/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672674350/validation/events.out.tfevents.1672674351.ce8d0207b981.377.94.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672674361/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672674361/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672674361/train/events.out.tfevents.1672674361.ce8d0207b981.377.95.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672674361/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672674361/validation/events.out.tfevents.1672674362.ce8d0207b981.377.96.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672674372/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672674372/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672674372/train/events.out.tfevents.1672674372.ce8d0207b981.377.97.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672674372/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672674372/validation/events.out.tfevents.1672674377.ce8d0207b981.377.98.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672674514/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672674514/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672674514/train/events.out.tfevents.1672674515.ce8d0207b981.377.99.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672674514/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672674514/validation/events.out.tfevents.1672674520.ce8d0207b981.377.100.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672674628/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672674628/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672674628/train/events.out.tfevents.1672674628.ce8d0207b981.377.101.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672674628/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672674628/validation/events.out.tfevents.1672674629.ce8d0207b981.377.102.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672674638/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672674638/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672674638/train/events.out.tfevents.1672674638.ce8d0207b981.377.103.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672674638/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672674638/validation/events.out.tfevents.1672674639.ce8d0207b981.377.104.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672674649/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672674649/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672674649/train/events.out.tfevents.1672674649.ce8d0207b981.377.105.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672674649/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672674649/validation/events.out.tfevents.1672674650.ce8d0207b981.377.106.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672674659/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672674659/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672674659/train/events.out.tfevents.1672674660.ce8d0207b981.377.107.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672674659/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672674659/validation/events.out.tfevents.1672674661.ce8d0207b981.377.108.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672674670/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672674670/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672674670/train/events.out.tfevents.1672674670.ce8d0207b981.377.109.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672674670/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672674670/validation/events.out.tfevents.1672674675.ce8d0207b981.377.110.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672674778/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672674778/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672674778/train/events.out.tfevents.1672674778.ce8d0207b981.377.111.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672674778/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672674778/validation/events.out.tfevents.1672674783.ce8d0207b981.377.112.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672674920/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672674920/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672674920/train/events.out.tfevents.1672674920.ce8d0207b981.377.113.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672674920/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672674920/validation/events.out.tfevents.1672674921.ce8d0207b981.377.114.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672674931/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672674931/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672674931/train/events.out.tfevents.1672674931.ce8d0207b981.377.115.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672674931/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672674931/validation/events.out.tfevents.1672674932.ce8d0207b981.377.116.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672674942/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672674942/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672674942/train/events.out.tfevents.1672674942.ce8d0207b981.377.117.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672674942/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672674942/validation/events.out.tfevents.1672674943.ce8d0207b981.377.118.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672674951/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672674951/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672674951/train/events.out.tfevents.1672674952.ce8d0207b981.377.119.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672674951/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672674951/validation/events.out.tfevents.1672674952.ce8d0207b981.377.120.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672674962/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672674962/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672674962/train/events.out.tfevents.1672674962.ce8d0207b981.377.121.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672674962/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672674962/validation/events.out.tfevents.1672674968.ce8d0207b981.377.122.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672675076/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672675076/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672675076/train/events.out.tfevents.1672675076.ce8d0207b981.377.123.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672675076/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672675076/validation/events.out.tfevents.1672675081.ce8d0207b981.377.124.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672675190/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672675190/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672675190/train/events.out.tfevents.1672675190.ce8d0207b981.377.125.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672675190/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672675190/validation/events.out.tfevents.1672675191.ce8d0207b981.377.126.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672675200/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672675200/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672675200/train/events.out.tfevents.1672675201.ce8d0207b981.377.127.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672675200/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672675200/validation/events.out.tfevents.1672675201.ce8d0207b981.377.128.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672675210/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672675210/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672675210/train/events.out.tfevents.1672675210.ce8d0207b981.377.129.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672675210/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672675210/validation/events.out.tfevents.1672675211.ce8d0207b981.377.130.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672675221/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672675221/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672675221/train/events.out.tfevents.1672675221.ce8d0207b981.377.131.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672675221/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672675221/validation/events.out.tfevents.1672675222.ce8d0207b981.377.132.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672675231/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672675231/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672675231/train/events.out.tfevents.1672675231.ce8d0207b981.377.133.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672675231/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672675231/validation/events.out.tfevents.1672675235.ce8d0207b981.377.134.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672675339/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672675339/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672675339/train/events.out.tfevents.1672675339.ce8d0207b981.377.135.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672675339/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672675339/validation/events.out.tfevents.1672675344.ce8d0207b981.377.136.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672675482/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672675482/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672675482/train/events.out.tfevents.1672675482.ce8d0207b981.377.137.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672675482/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672675482/validation/events.out.tfevents.1672675483.ce8d0207b981.377.138.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672675492/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672675492/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672675492/train/events.out.tfevents.1672675492.ce8d0207b981.377.139.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672675492/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672675492/validation/events.out.tfevents.1672675493.ce8d0207b981.377.140.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672675502/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672675502/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672675502/train/events.out.tfevents.1672675503.ce8d0207b981.377.141.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672675502/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672675502/validation/events.out.tfevents.1672675503.ce8d0207b981.377.142.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672675513/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672675513/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672675513/train/events.out.tfevents.1672675513.ce8d0207b981.377.143.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672675513/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672675513/validation/events.out.tfevents.1672675514.ce8d0207b981.377.144.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672675523/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672675523/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672675523/train/events.out.tfevents.1672675523.ce8d0207b981.377.145.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672675523/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672675523/validation/events.out.tfevents.1672675528.ce8d0207b981.377.146.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672675665/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672675665/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672675665/train/events.out.tfevents.1672675665.ce8d0207b981.377.147.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672675665/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672675665/validation/events.out.tfevents.1672675670.ce8d0207b981.377.148.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672675808/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672675808/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672675808/train/events.out.tfevents.1672675808.ce8d0207b981.377.149.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672675808/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672675808/validation/events.out.tfevents.1672675809.ce8d0207b981.377.150.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672675818/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672675818/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672675818/train/events.out.tfevents.1672675818.ce8d0207b981.377.151.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672675818/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672675818/validation/events.out.tfevents.1672675819.ce8d0207b981.377.152.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672675829/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672675829/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672675829/train/events.out.tfevents.1672675829.ce8d0207b981.377.153.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672675829/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672675829/validation/events.out.tfevents.1672675830.ce8d0207b981.377.154.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672675840/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672675840/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672675840/train/events.out.tfevents.1672675840.ce8d0207b981.377.155.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672675840/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672675840/validation/events.out.tfevents.1672675841.ce8d0207b981.377.156.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672675851/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672675851/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672675851/train/events.out.tfevents.1672675851.ce8d0207b981.377.157.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672675851/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672675851/validation/events.out.tfevents.1672675855.ce8d0207b981.377.158.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672675957/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672675957/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672675957/train/events.out.tfevents.1672675957.ce8d0207b981.377.159.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672675957/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672675957/validation/events.out.tfevents.1672675962.ce8d0207b981.377.160.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672676100/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672676100/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672676100/train/events.out.tfevents.1672676100.ce8d0207b981.377.161.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672676100/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672676100/validation/events.out.tfevents.1672676101.ce8d0207b981.377.162.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672676110/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672676110/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672676110/train/events.out.tfevents.1672676111.ce8d0207b981.377.163.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672676110/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672676110/validation/events.out.tfevents.1672676111.ce8d0207b981.377.164.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672676120/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672676120/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672676120/train/events.out.tfevents.1672676120.ce8d0207b981.377.165.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672676120/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672676120/validation/events.out.tfevents.1672676121.ce8d0207b981.377.166.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672676130/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672676130/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672676130/train/events.out.tfevents.1672676130.ce8d0207b981.377.167.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672676130/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer sgd- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672676130/validation/events.out.tfevents.1672676131.ce8d0207b981.377.168.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672676139/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672676139/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672676139/train/events.out.tfevents.1672676139.ce8d0207b981.377.169.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672676139/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672676139/validation/events.out.tfevents.1672676144.ce8d0207b981.377.170.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672676255/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672676255/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672676255/train/events.out.tfevents.1672676255.ce8d0207b981.377.171.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672676255/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672676255/validation/events.out.tfevents.1672676260.ce8d0207b981.377.172.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672676371/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672676371/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672676371/train/events.out.tfevents.1672676372.ce8d0207b981.377.173.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672676371/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672676371/validation/events.out.tfevents.1672676373.ce8d0207b981.377.174.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672676381/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672676381/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672676381/train/events.out.tfevents.1672676381.ce8d0207b981.377.175.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672676381/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672676381/validation/events.out.tfevents.1672676382.ce8d0207b981.377.176.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672676392/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672676392/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672676392/train/events.out.tfevents.1672676392.ce8d0207b981.377.177.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672676392/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672676392/validation/events.out.tfevents.1672676393.ce8d0207b981.377.178.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672676402/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672676402/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672676402/train/events.out.tfevents.1672676402.ce8d0207b981.377.179.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672676402/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672676402/validation/events.out.tfevents.1672676403.ce8d0207b981.377.180.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672676411/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672676411/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672676411/train/events.out.tfevents.1672676411.ce8d0207b981.377.181.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672676411/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672676411/validation/events.out.tfevents.1672676416.ce8d0207b981.377.182.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672676521/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672676521/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672676521/train/events.out.tfevents.1672676521.ce8d0207b981.377.183.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672676521/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672676521/validation/events.out.tfevents.1672676526.ce8d0207b981.377.184.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672676632/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672676632/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672676632/train/events.out.tfevents.1672676632.ce8d0207b981.377.185.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672676632/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672676632/validation/events.out.tfevents.1672676633.ce8d0207b981.377.186.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672676643/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672676643/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672676643/train/events.out.tfevents.1672676643.ce8d0207b981.377.187.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672676643/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672676643/validation/events.out.tfevents.1672676644.ce8d0207b981.377.188.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672676654/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672676654/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672676654/train/events.out.tfevents.1672676654.ce8d0207b981.377.189.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672676654/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672676654/validation/events.out.tfevents.1672676655.ce8d0207b981.377.190.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672676665/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672676665/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672676665/train/events.out.tfevents.1672676665.ce8d0207b981.377.191.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672676665/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672676665/validation/events.out.tfevents.1672676666.ce8d0207b981.377.192.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672676675/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672676675/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672676675/train/events.out.tfevents.1672676676.ce8d0207b981.377.193.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672676675/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672676675/validation/events.out.tfevents.1672676681.ce8d0207b981.377.194.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672676818/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672676818/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672676818/train/events.out.tfevents.1672676818.ce8d0207b981.377.195.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672676818/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672676818/validation/events.out.tfevents.1672676823.ce8d0207b981.377.196.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672676936/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672676936/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672676936/train/events.out.tfevents.1672676937.ce8d0207b981.377.197.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672676936/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672676936/validation/events.out.tfevents.1672676938.ce8d0207b981.377.198.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672676946/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672676946/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672676946/train/events.out.tfevents.1672676946.ce8d0207b981.377.199.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672676946/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672676946/validation/events.out.tfevents.1672676948.ce8d0207b981.377.200.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672676957/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672676957/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672676957/train/events.out.tfevents.1672676958.ce8d0207b981.377.201.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672676957/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672676957/validation/events.out.tfevents.1672676958.ce8d0207b981.377.202.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672676968/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672676968/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672676968/train/events.out.tfevents.1672676968.ce8d0207b981.377.203.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672676968/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672676968/validation/events.out.tfevents.1672676969.ce8d0207b981.377.204.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672676977/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672676977/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672676977/train/events.out.tfevents.1672676978.ce8d0207b981.377.205.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672676977/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672676977/validation/events.out.tfevents.1672676983.ce8d0207b981.377.206.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672677087/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672677087/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672677087/train/events.out.tfevents.1672677088.ce8d0207b981.377.207.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672677087/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672677087/validation/events.out.tfevents.1672677092.ce8d0207b981.377.208.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672677197/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672677197/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672677197/train/events.out.tfevents.1672677198.ce8d0207b981.377.209.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672677197/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672677197/validation/events.out.tfevents.1672677199.ce8d0207b981.377.210.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672677208/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672677208/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672677208/train/events.out.tfevents.1672677209.ce8d0207b981.377.211.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672677208/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672677208/validation/events.out.tfevents.1672677209.ce8d0207b981.377.212.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672677218/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672677218/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672677218/train/events.out.tfevents.1672677218.ce8d0207b981.377.213.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672677218/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672677218/validation/events.out.tfevents.1672677219.ce8d0207b981.377.214.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672677229/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672677229/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672677229/train/events.out.tfevents.1672677229.ce8d0207b981.377.215.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672677229/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672677229/validation/events.out.tfevents.1672677230.ce8d0207b981.377.216.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672677240/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672677240/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672677240/train/events.out.tfevents.1672677240.ce8d0207b981.377.217.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672677240/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672677240/validation/events.out.tfevents.1672677246.ce8d0207b981.377.218.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672677383/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672677383/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672677383/train/events.out.tfevents.1672677383.ce8d0207b981.377.219.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672677383/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672677383/validation/events.out.tfevents.1672677388.ce8d0207b981.377.220.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672677502/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672677502/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672677502/train/events.out.tfevents.1672677502.ce8d0207b981.377.221.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672677502/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672677502/validation/events.out.tfevents.1672677503.ce8d0207b981.377.222.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672677512/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672677512/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672677512/train/events.out.tfevents.1672677512.ce8d0207b981.377.223.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672677512/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672677512/validation/events.out.tfevents.1672677513.ce8d0207b981.377.224.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672677523/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672677523/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672677523/train/events.out.tfevents.1672677523.ce8d0207b981.377.225.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672677523/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672677523/validation/events.out.tfevents.1672677524.ce8d0207b981.377.226.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672677534/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672677534/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672677534/train/events.out.tfevents.1672677534.ce8d0207b981.377.227.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672677534/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672677534/validation/events.out.tfevents.1672677535.ce8d0207b981.377.228.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672677543/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672677543/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672677543/train/events.out.tfevents.1672677544.ce8d0207b981.377.229.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672677543/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672677543/validation/events.out.tfevents.1672677549.ce8d0207b981.377.230.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672677686/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672677686/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672677686/train/events.out.tfevents.1672677686.ce8d0207b981.377.231.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672677686/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672677686/validation/events.out.tfevents.1672677691.ce8d0207b981.377.232.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672677829/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672677829/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672677829/train/events.out.tfevents.1672677829.ce8d0207b981.377.233.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672677829/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672677829/validation/events.out.tfevents.1672677830.ce8d0207b981.377.234.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672677839/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672677839/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672677839/train/events.out.tfevents.1672677840.ce8d0207b981.377.235.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672677839/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672677839/validation/events.out.tfevents.1672677840.ce8d0207b981.377.236.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672677850/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672677850/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672677850/train/events.out.tfevents.1672677850.ce8d0207b981.377.237.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672677850/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672677850/validation/events.out.tfevents.1672677851.ce8d0207b981.377.238.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672677860/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672677860/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672677860/train/events.out.tfevents.1672677860.ce8d0207b981.377.239.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672677860/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672677860/validation/events.out.tfevents.1672677861.ce8d0207b981.377.240.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672677871/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672677871/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672677871/train/events.out.tfevents.1672677871.ce8d0207b981.377.241.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672677871/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672677871/validation/events.out.tfevents.1672677876.ce8d0207b981.377.242.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672678013/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672678013/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672678013/train/events.out.tfevents.1672678013.ce8d0207b981.377.243.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672678013/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672678013/validation/events.out.tfevents.1672678019.ce8d0207b981.377.244.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672678131/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672678131/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672678131/train/events.out.tfevents.1672678131.ce8d0207b981.377.245.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672678131/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672678131/validation/events.out.tfevents.1672678133.ce8d0207b981.377.246.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672678142/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672678142/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672678142/train/events.out.tfevents.1672678143.ce8d0207b981.377.247.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672678142/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672678142/validation/events.out.tfevents.1672678144.ce8d0207b981.377.248.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672678153/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672678153/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672678153/train/events.out.tfevents.1672678153.ce8d0207b981.377.249.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672678153/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672678153/validation/events.out.tfevents.1672678154.ce8d0207b981.377.250.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672678163/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672678163/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672678163/train/events.out.tfevents.1672678164.ce8d0207b981.377.251.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672678163/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672678163/validation/events.out.tfevents.1672678165.ce8d0207b981.377.252.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672678173/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672678173/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672678173/train/events.out.tfevents.1672678173.ce8d0207b981.377.253.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672678173/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672678173/validation/events.out.tfevents.1672678178.ce8d0207b981.377.254.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672678286/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672678286/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672678286/train/events.out.tfevents.1672678286.ce8d0207b981.377.255.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672678286/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672678286/validation/events.out.tfevents.1672678291.ce8d0207b981.377.256.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672678396/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672678396/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672678396/train/events.out.tfevents.1672678396.ce8d0207b981.377.257.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672678396/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672678396/validation/events.out.tfevents.1672678397.ce8d0207b981.377.258.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672678407/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672678407/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672678407/train/events.out.tfevents.1672678407.ce8d0207b981.377.259.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672678407/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672678407/validation/events.out.tfevents.1672678408.ce8d0207b981.377.260.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672678418/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672678418/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672678418/train/events.out.tfevents.1672678418.ce8d0207b981.377.261.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672678418/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672678418/validation/events.out.tfevents.1672678419.ce8d0207b981.377.262.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672678429/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672678429/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672678429/train/events.out.tfevents.1672678429.ce8d0207b981.377.263.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672678429/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672678429/validation/events.out.tfevents.1672678430.ce8d0207b981.377.264.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672678439/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672678439/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672678439/train/events.out.tfevents.1672678439.ce8d0207b981.377.265.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672678439/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672678439/validation/events.out.tfevents.1672678444.ce8d0207b981.377.266.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672678581/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672678581/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672678581/train/events.out.tfevents.1672678581.ce8d0207b981.377.267.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672678581/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672678581/validation/events.out.tfevents.1672678587.ce8d0207b981.377.268.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672678703/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672678703/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672678703/train/events.out.tfevents.1672678704.ce8d0207b981.377.269.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672678703/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672678703/validation/events.out.tfevents.1672678705.ce8d0207b981.377.270.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672678714/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672678714/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672678714/train/events.out.tfevents.1672678714.ce8d0207b981.377.271.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672678714/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672678714/validation/events.out.tfevents.1672678715.ce8d0207b981.377.272.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672678724/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672678724/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672678724/train/events.out.tfevents.1672678724.ce8d0207b981.377.273.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672678724/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672678724/validation/events.out.tfevents.1672678725.ce8d0207b981.377.274.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672678734/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672678734/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672678734/train/events.out.tfevents.1672678735.ce8d0207b981.377.275.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672678734/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672678734/validation/events.out.tfevents.1672678736.ce8d0207b981.377.276.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672678744/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672678744/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672678744/train/events.out.tfevents.1672678744.ce8d0207b981.377.277.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672678744/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672678744/validation/events.out.tfevents.1672678749.ce8d0207b981.377.278.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672678857/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672678857/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672678857/train/events.out.tfevents.1672678857.ce8d0207b981.377.279.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672678857/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672678857/validation/events.out.tfevents.1672678863.ce8d0207b981.377.280.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672679000/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672679000/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672679000/train/events.out.tfevents.1672679000.ce8d0207b981.377.281.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672679000/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672679000/validation/events.out.tfevents.1672679001.ce8d0207b981.377.282.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672679010/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672679010/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672679010/train/events.out.tfevents.1672679010.ce8d0207b981.377.283.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672679010/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672679010/validation/events.out.tfevents.1672679011.ce8d0207b981.377.284.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672679020/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672679020/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672679020/train/events.out.tfevents.1672679020.ce8d0207b981.377.285.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672679020/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672679020/validation/events.out.tfevents.1672679021.ce8d0207b981.377.286.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672679031/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672679031/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672679031/train/events.out.tfevents.1672679031.ce8d0207b981.377.287.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672679031/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672679031/validation/events.out.tfevents.1672679032.ce8d0207b981.377.288.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672679041/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672679041/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672679041/train/events.out.tfevents.1672679041.ce8d0207b981.377.289.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672679041/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672679041/validation/events.out.tfevents.1672679047.ce8d0207b981.377.290.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672679184/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672679184/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672679184/train/events.out.tfevents.1672679184.ce8d0207b981.377.291.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672679184/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672679184/validation/events.out.tfevents.1672679189.ce8d0207b981.377.292.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672679305/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672679305/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672679305/train/events.out.tfevents.1672679305.ce8d0207b981.377.293.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672679305/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672679305/validation/events.out.tfevents.1672679306.ce8d0207b981.377.294.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672679315/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672679315/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672679315/train/events.out.tfevents.1672679315.ce8d0207b981.377.295.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672679315/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672679315/validation/events.out.tfevents.1672679316.ce8d0207b981.377.296.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672679326/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672679326/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672679326/train/events.out.tfevents.1672679326.ce8d0207b981.377.297.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672679326/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672679326/validation/events.out.tfevents.1672679327.ce8d0207b981.377.298.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672679337/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672679337/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672679337/train/events.out.tfevents.1672679337.ce8d0207b981.377.299.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672679337/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672679337/validation/events.out.tfevents.1672679338.ce8d0207b981.377.300.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672679346/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672679346/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672679346/train/events.out.tfevents.1672679347.ce8d0207b981.377.301.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672679346/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672679346/validation/events.out.tfevents.1672679352.ce8d0207b981.377.302.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672679460/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672679460/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672679460/train/events.out.tfevents.1672679460.ce8d0207b981.377.303.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672679460/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672679460/validation/events.out.tfevents.1672679465.ce8d0207b981.377.304.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672679571/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672679571/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672679571/train/events.out.tfevents.1672679571.ce8d0207b981.377.305.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672679571/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672679571/validation/events.out.tfevents.1672679572.ce8d0207b981.377.306.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672679581/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672679581/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672679581/train/events.out.tfevents.1672679581.ce8d0207b981.377.307.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672679581/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672679581/validation/events.out.tfevents.1672679582.ce8d0207b981.377.308.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672679592/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672679592/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672679592/train/events.out.tfevents.1672679592.ce8d0207b981.377.309.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672679592/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672679592/validation/events.out.tfevents.1672679593.ce8d0207b981.377.310.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672679603/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672679603/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672679603/train/events.out.tfevents.1672679603.ce8d0207b981.377.311.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672679603/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer adam- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672679603/validation/events.out.tfevents.1672679605.ce8d0207b981.377.312.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672679613/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672679613/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672679613/train/events.out.tfevents.1672679613.ce8d0207b981.377.313.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672679613/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672679613/validation/events.out.tfevents.1672679620.ce8d0207b981.377.314.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672679747/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672679747/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672679747/train/events.out.tfevents.1672679748.ce8d0207b981.377.315.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672679747/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672679747/validation/events.out.tfevents.1672679754.ce8d0207b981.377.316.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672679881/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672679881/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672679881/train/events.out.tfevents.1672679881.ce8d0207b981.377.317.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672679881/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672679881/validation/events.out.tfevents.1672679883.ce8d0207b981.377.318.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672679892/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672679892/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672679892/train/events.out.tfevents.1672679892.ce8d0207b981.377.319.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672679892/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672679892/validation/events.out.tfevents.1672679894.ce8d0207b981.377.320.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672679902/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672679902/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672679902/train/events.out.tfevents.1672679902.ce8d0207b981.377.321.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672679902/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672679902/validation/events.out.tfevents.1672679903.ce8d0207b981.377.322.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672679911/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672679911/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672679911/train/events.out.tfevents.1672679911.ce8d0207b981.377.323.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672679911/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672679911/validation/events.out.tfevents.1672679912.ce8d0207b981.377.324.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672679922/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672679922/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672679922/train/events.out.tfevents.1672679922.ce8d0207b981.377.325.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672679922/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672679922/validation/events.out.tfevents.1672679928.ce8d0207b981.377.326.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672680065/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672680065/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672680065/train/events.out.tfevents.1672680065.ce8d0207b981.377.327.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672680065/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672680065/validation/events.out.tfevents.1672680071.ce8d0207b981.377.328.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672680208/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672680208/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672680208/train/events.out.tfevents.1672680208.ce8d0207b981.377.329.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672680208/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672680208/validation/events.out.tfevents.1672680209.ce8d0207b981.377.330.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672680218/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672680218/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672680218/train/events.out.tfevents.1672680218.ce8d0207b981.377.331.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672680218/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672680218/validation/events.out.tfevents.1672680220.ce8d0207b981.377.332.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672680230/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672680230/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672680230/train/events.out.tfevents.1672680230.ce8d0207b981.377.333.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672680230/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672680230/validation/events.out.tfevents.1672680231.ce8d0207b981.377.334.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672680241/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672680241/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672680241/train/events.out.tfevents.1672680241.ce8d0207b981.377.335.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672680241/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672680241/validation/events.out.tfevents.1672680242.ce8d0207b981.377.336.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672680252/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672680252/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672680252/train/events.out.tfevents.1672680252.ce8d0207b981.377.337.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672680252/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672680252/validation/events.out.tfevents.1672680258.ce8d0207b981.377.338.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672680386/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672680386/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672680386/train/events.out.tfevents.1672680386.ce8d0207b981.377.339.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672680386/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672680386/validation/events.out.tfevents.1672680392.ce8d0207b981.377.340.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672680529/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672680529/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672680529/train/events.out.tfevents.1672680529.ce8d0207b981.377.341.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672680529/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672680529/validation/events.out.tfevents.1672680530.ce8d0207b981.377.342.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672680540/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672680540/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672680540/train/events.out.tfevents.1672680540.ce8d0207b981.377.343.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672680540/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672680540/validation/events.out.tfevents.1672680541.ce8d0207b981.377.344.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672680550/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672680550/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672680550/train/events.out.tfevents.1672680550.ce8d0207b981.377.345.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672680550/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672680550/validation/events.out.tfevents.1672680551.ce8d0207b981.377.346.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672680561/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672680561/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672680561/train/events.out.tfevents.1672680561.ce8d0207b981.377.347.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672680561/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672680561/validation/events.out.tfevents.1672680562.ce8d0207b981.377.348.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672680572/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672680572/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672680572/train/events.out.tfevents.1672680572.ce8d0207b981.377.349.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672680572/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672680572/validation/events.out.tfevents.1672680578.ce8d0207b981.377.350.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672680715/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672680715/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672680715/train/events.out.tfevents.1672680715.ce8d0207b981.377.351.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672680715/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672680715/validation/events.out.tfevents.1672680721.ce8d0207b981.377.352.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672680858/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672680858/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672680858/train/events.out.tfevents.1672680858.ce8d0207b981.377.353.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672680858/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672680858/validation/events.out.tfevents.1672680859.ce8d0207b981.377.354.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672680869/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672680869/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672680869/train/events.out.tfevents.1672680869.ce8d0207b981.377.355.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672680869/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672680869/validation/events.out.tfevents.1672680870.ce8d0207b981.377.356.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672680879/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672680879/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672680879/train/events.out.tfevents.1672680879.ce8d0207b981.377.357.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672680879/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672680879/validation/events.out.tfevents.1672680880.ce8d0207b981.377.358.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672680890/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672680890/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672680890/train/events.out.tfevents.1672680890.ce8d0207b981.377.359.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672680890/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672680890/validation/events.out.tfevents.1672680891.ce8d0207b981.377.360.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672680900/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672680900/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672680900/train/events.out.tfevents.1672680900.ce8d0207b981.377.361.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672680900/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672680900/validation/events.out.tfevents.1672680906.ce8d0207b981.377.362.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672681035/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672681035/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672681035/train/events.out.tfevents.1672681035.ce8d0207b981.377.363.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672681035/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672681035/validation/events.out.tfevents.1672681041.ce8d0207b981.377.364.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672681177/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672681177/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672681177/train/events.out.tfevents.1672681178.ce8d0207b981.377.365.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672681177/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672681177/validation/events.out.tfevents.1672681179.ce8d0207b981.377.366.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672681189/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672681189/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672681189/train/events.out.tfevents.1672681189.ce8d0207b981.377.367.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672681189/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672681189/validation/events.out.tfevents.1672681190.ce8d0207b981.377.368.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672681200/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672681200/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672681200/train/events.out.tfevents.1672681200.ce8d0207b981.377.369.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672681200/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672681200/validation/events.out.tfevents.1672681201.ce8d0207b981.377.370.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672681210/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672681210/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672681210/train/events.out.tfevents.1672681210.ce8d0207b981.377.371.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672681210/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672681210/validation/events.out.tfevents.1672681211.ce8d0207b981.377.372.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672681221/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672681221/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672681221/train/events.out.tfevents.1672681221.ce8d0207b981.377.373.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672681221/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672681221/validation/events.out.tfevents.1672681227.ce8d0207b981.377.374.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672681344/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672681344/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672681344/train/events.out.tfevents.1672681345.ce8d0207b981.377.375.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672681344/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672681344/validation/events.out.tfevents.1672681350.ce8d0207b981.377.376.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672681487/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672681487/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672681487/train/events.out.tfevents.1672681487.ce8d0207b981.377.377.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672681487/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672681487/validation/events.out.tfevents.1672681488.ce8d0207b981.377.378.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672681498/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672681498/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672681498/train/events.out.tfevents.1672681498.ce8d0207b981.377.379.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672681498/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672681498/validation/events.out.tfevents.1672681499.ce8d0207b981.377.380.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672681508/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672681508/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672681508/train/events.out.tfevents.1672681509.ce8d0207b981.377.381.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672681508/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672681508/validation/events.out.tfevents.1672681510.ce8d0207b981.377.382.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672681520/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672681520/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672681520/train/events.out.tfevents.1672681520.ce8d0207b981.377.383.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672681520/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672681520/validation/events.out.tfevents.1672681521.ce8d0207b981.377.384.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672681531/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672681531/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672681531/train/events.out.tfevents.1672681531.ce8d0207b981.377.385.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672681531/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672681531/validation/events.out.tfevents.1672681537.ce8d0207b981.377.386.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672681664/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672681664/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672681664/train/events.out.tfevents.1672681664.ce8d0207b981.377.387.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672681664/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672681664/validation/events.out.tfevents.1672681671.ce8d0207b981.377.388.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672681798/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672681798/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672681798/train/events.out.tfevents.1672681799.ce8d0207b981.377.389.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672681798/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672681798/validation/events.out.tfevents.1672681800.ce8d0207b981.377.390.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672681809/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672681809/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672681809/train/events.out.tfevents.1672681809.ce8d0207b981.377.391.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672681809/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672681809/validation/events.out.tfevents.1672681810.ce8d0207b981.377.392.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672681819/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672681819/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672681819/train/events.out.tfevents.1672681819.ce8d0207b981.377.393.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672681819/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672681819/validation/events.out.tfevents.1672681820.ce8d0207b981.377.394.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672681829/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672681829/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672681829/train/events.out.tfevents.1672681829.ce8d0207b981.377.395.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672681829/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672681829/validation/events.out.tfevents.1672681830.ce8d0207b981.377.396.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672681840/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672681840/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672681840/train/events.out.tfevents.1672681840.ce8d0207b981.377.397.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672681840/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672681840/validation/events.out.tfevents.1672681846.ce8d0207b981.377.398.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672681966/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672681966/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672681966/train/events.out.tfevents.1672681966.ce8d0207b981.377.399.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672681966/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672681966/validation/events.out.tfevents.1672681972.ce8d0207b981.377.400.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672682092/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672682092/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672682092/train/events.out.tfevents.1672682092.ce8d0207b981.377.401.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672682092/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672682092/validation/events.out.tfevents.1672682093.ce8d0207b981.377.402.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672682102/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672682102/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672682102/train/events.out.tfevents.1672682102.ce8d0207b981.377.403.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672682102/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672682102/validation/events.out.tfevents.1672682103.ce8d0207b981.377.404.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672682112/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672682112/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672682112/train/events.out.tfevents.1672682112.ce8d0207b981.377.405.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672682112/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672682112/validation/events.out.tfevents.1672682114.ce8d0207b981.377.406.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672682122/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672682122/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672682122/train/events.out.tfevents.1672682123.ce8d0207b981.377.407.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672682122/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.5- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672682122/validation/events.out.tfevents.1672682124.ce8d0207b981.377.408.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672682132/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672682132/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672682132/train/events.out.tfevents.1672682132.ce8d0207b981.377.409.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672682132/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation relu-1672682132/validation/events.out.tfevents.1672682139.ce8d0207b981.377.410.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672682275/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672682275/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672682275/train/events.out.tfevents.1672682275.ce8d0207b981.377.411.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672682275/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 32 activation tanh-1672682275/validation/events.out.tfevents.1672682282.ce8d0207b981.377.412.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672682418/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672682418/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672682418/train/events.out.tfevents.1672682418.ce8d0207b981.377.413.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672682418/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation relu-1672682418/validation/events.out.tfevents.1672682419.ce8d0207b981.377.414.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672682428/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672682428/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672682428/train/events.out.tfevents.1672682428.ce8d0207b981.377.415.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672682428/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 2048 activation tanh-1672682428/validation/events.out.tfevents.1672682429.ce8d0207b981.377.416.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672682438/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672682438/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672682438/train/events.out.tfevents.1672682438.ce8d0207b981.377.417.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672682438/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation relu-1672682438/validation/events.out.tfevents.1672682439.ce8d0207b981.377.418.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672682449/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672682449/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672682449/train/events.out.tfevents.1672682449.ce8d0207b981.377.419.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672682449/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 3- stride 2 batch size 8192 activation tanh-1672682449/validation/events.out.tfevents.1672682451.ce8d0207b981.377.420.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672682459/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672682459/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672682459/train/events.out.tfevents.1672682459.ce8d0207b981.377.421.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672682459/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation relu-1672682459/validation/events.out.tfevents.1672682466.ce8d0207b981.377.422.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672682602/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672682602/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672682602/train/events.out.tfevents.1672682602.ce8d0207b981.377.423.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672682602/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 32 activation tanh-1672682602/validation/events.out.tfevents.1672682609.ce8d0207b981.377.424.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672682730/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672682730/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672682730/train/events.out.tfevents.1672682731.ce8d0207b981.377.425.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672682730/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation relu-1672682730/validation/events.out.tfevents.1672682732.ce8d0207b981.377.426.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672682741/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672682741/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672682741/train/events.out.tfevents.1672682741.ce8d0207b981.377.427.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672682741/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 2048 activation tanh-1672682741/validation/events.out.tfevents.1672682742.ce8d0207b981.377.428.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672682751/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672682751/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672682751/train/events.out.tfevents.1672682751.ce8d0207b981.377.429.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672682751/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation relu-1672682751/validation/events.out.tfevents.1672682752.ce8d0207b981.377.430.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672682761/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672682761/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672682761/train/events.out.tfevents.1672682761.ce8d0207b981.377.431.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672682761/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.001- kernel size 5- stride 2 batch size 8192 activation tanh-1672682761/validation/events.out.tfevents.1672682762.ce8d0207b981.377.432.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672682772/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672682772/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672682772/train/events.out.tfevents.1672682772.ce8d0207b981.377.433.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672682772/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation relu-1672682772/validation/events.out.tfevents.1672682778.ce8d0207b981.377.434.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672682915/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672682915/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672682915/train/events.out.tfevents.1672682915.ce8d0207b981.377.435.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672682915/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 32 activation tanh-1672682915/validation/events.out.tfevents.1672682921.ce8d0207b981.377.436.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672683057/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672683057/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672683057/train/events.out.tfevents.1672683058.ce8d0207b981.377.437.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672683057/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation relu-1672683057/validation/events.out.tfevents.1672683059.ce8d0207b981.377.438.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672683068/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672683068/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672683068/train/events.out.tfevents.1672683069.ce8d0207b981.377.439.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672683068/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 2048 activation tanh-1672683068/validation/events.out.tfevents.1672683070.ce8d0207b981.377.440.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672683079/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672683079/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672683079/train/events.out.tfevents.1672683079.ce8d0207b981.377.441.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672683079/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation relu-1672683079/validation/events.out.tfevents.1672683080.ce8d0207b981.377.442.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672683089/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672683089/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672683089/train/events.out.tfevents.1672683089.ce8d0207b981.377.443.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672683089/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 3- stride 2 batch size 8192 activation tanh-1672683089/validation/events.out.tfevents.1672683090.ce8d0207b981.377.444.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672683098/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672683098/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672683098/train/events.out.tfevents.1672683098.ce8d0207b981.377.445.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672683098/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation relu-1672683098/validation/events.out.tfevents.1672683105.ce8d0207b981.377.446.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672683224/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672683224/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672683224/train/events.out.tfevents.1672683224.ce8d0207b981.377.447.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672683224/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 32 activation tanh-1672683224/validation/events.out.tfevents.1672683230.ce8d0207b981.377.448.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672683349/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672683349/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672683349/train/events.out.tfevents.1672683349.ce8d0207b981.377.449.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672683349/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation relu-1672683349/validation/events.out.tfevents.1672683350.ce8d0207b981.377.450.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672683360/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672683360/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672683360/train/events.out.tfevents.1672683360.ce8d0207b981.377.451.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672683360/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 2048 activation tanh-1672683360/validation/events.out.tfevents.1672683361.ce8d0207b981.377.452.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672683370/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672683370/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672683370/train/events.out.tfevents.1672683370.ce8d0207b981.377.453.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672683370/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation relu-1672683370/validation/events.out.tfevents.1672683371.ce8d0207b981.377.454.v2 (deflated 77%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672683380/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672683380/train/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672683380/train/events.out.tfevents.1672683380.ce8d0207b981.377.455.v2 (deflated 76%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672683380/validation/ (stored 0%)\n",
      "  adding: fashionMnist/Logs\\optimizer rmsprop- dropout 0.8- learning rate 0.003- kernel size 5- stride 2 batch size 8192 activation tanh-1672683380/validation/events.out.tfevents.1672683381.ce8d0207b981.377.456.v2 (deflated 77%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r fashionMnistt.zip fashionMnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmcpJmSHxuHv",
   "metadata": {
    "id": "mmcpJmSHxuHv"
   },
   "outputs": [],
   "source": [
    "files.download('fashionMnistt.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u8soalw0_Vkl",
   "metadata": {
    "id": "u8soalw0_Vkl"
   },
   "outputs": [],
   "source": [
    "### the best model parameters are dropout = 0.5 kernel size 5 batch size 32 learning rate 0.001 activation tanh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VbxesfZq-_FY",
   "metadata": {
    "id": "VbxesfZq-_FY"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9,\n",
    "                              patience=20, min_lr=0.0001, min_delta = 0)\n",
    "\n",
    "# we import the early stopping call back and tell it to monitor the accuracy of the validation set\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "model = Sequential([\n",
    "                            keras.layers.Conv2D(6, kernel_size=3, strides=1,  activation='tanh', input_shape=x_train[0].shape, padding='same'), #C1\n",
    "                            keras.layers.AveragePooling2D(), #S2\n",
    "                            keras.layers.Conv2D(16, kernel_size=3, strides=1, activation='tanh', padding='valid'), #C3\n",
    "                            keras.layers.AveragePooling2D(), #S4\n",
    "                            keras.layers.Conv2D(120, kernel_size=3, strides=1, activation='tanh', padding='valid'), #C5\n",
    "                            keras.layers.Flatten(), #Flatten\n",
    "                            keras.layers.Dropout(0.5),\n",
    "                            keras.layers.Dense(84, activation='tanh'), #F6\n",
    "                            keras.layers.Dense(10, activation='softmax') #Output layer\n",
    "                        ])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='CategoricalCrossentropy',optimizer=opt,metrics='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m2Ae00QE_2P2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2Ae00QE_2P2",
    "outputId": "4d29ce00-2949-43aa-c187-4e7db72e469c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1499/1499 - 7s - loss: 0.5815 - accuracy: 0.7857 - val_loss: 0.4487 - val_accuracy: 0.8333 - 7s/epoch - 5ms/step\n",
      "Epoch 2/50\n",
      "1499/1499 - 6s - loss: 0.4241 - accuracy: 0.8467 - val_loss: 0.3935 - val_accuracy: 0.8552 - 6s/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "1499/1499 - 6s - loss: 0.3882 - accuracy: 0.8590 - val_loss: 0.3710 - val_accuracy: 0.8612 - 6s/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "1499/1499 - 6s - loss: 0.3684 - accuracy: 0.8665 - val_loss: 0.3660 - val_accuracy: 0.8643 - 6s/epoch - 4ms/step\n",
      "Epoch 5/50\n",
      "1499/1499 - 6s - loss: 0.3536 - accuracy: 0.8708 - val_loss: 0.3519 - val_accuracy: 0.8683 - 6s/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "1499/1499 - 7s - loss: 0.3421 - accuracy: 0.8758 - val_loss: 0.3375 - val_accuracy: 0.8749 - 7s/epoch - 5ms/step\n",
      "Epoch 7/50\n",
      "1499/1499 - 6s - loss: 0.3302 - accuracy: 0.8789 - val_loss: 0.3278 - val_accuracy: 0.8783 - 6s/epoch - 4ms/step\n",
      "Epoch 8/50\n",
      "1499/1499 - 6s - loss: 0.3251 - accuracy: 0.8809 - val_loss: 0.3344 - val_accuracy: 0.8769 - 6s/epoch - 4ms/step\n",
      "Epoch 9/50\n",
      "1499/1499 - 6s - loss: 0.3191 - accuracy: 0.8825 - val_loss: 0.3391 - val_accuracy: 0.8724 - 6s/epoch - 4ms/step\n",
      "Epoch 10/50\n",
      "1499/1499 - 6s - loss: 0.3178 - accuracy: 0.8823 - val_loss: 0.3514 - val_accuracy: 0.8692 - 6s/epoch - 4ms/step\n",
      "Epoch 11/50\n",
      "1499/1499 - 8s - loss: 0.3142 - accuracy: 0.8834 - val_loss: 0.3361 - val_accuracy: 0.8765 - 8s/epoch - 5ms/step\n",
      "Epoch 12/50\n",
      "1499/1499 - 6s - loss: 0.3093 - accuracy: 0.8877 - val_loss: 0.3271 - val_accuracy: 0.8788 - 6s/epoch - 4ms/step\n",
      "Epoch 13/50\n",
      "1499/1499 - 6s - loss: 0.3049 - accuracy: 0.8877 - val_loss: 0.3233 - val_accuracy: 0.8782 - 6s/epoch - 4ms/step\n",
      "Epoch 14/50\n",
      "1499/1499 - 6s - loss: 0.3014 - accuracy: 0.8881 - val_loss: 0.3317 - val_accuracy: 0.8768 - 6s/epoch - 4ms/step\n",
      "Epoch 15/50\n",
      "1499/1499 - 6s - loss: 0.3008 - accuracy: 0.8901 - val_loss: 0.3281 - val_accuracy: 0.8798 - 6s/epoch - 4ms/step\n",
      "Epoch 16/50\n",
      "1499/1499 - 5s - loss: 0.3002 - accuracy: 0.8881 - val_loss: 0.3225 - val_accuracy: 0.8817 - 5s/epoch - 4ms/step\n",
      "Epoch 17/50\n",
      "1499/1499 - 6s - loss: 0.2927 - accuracy: 0.8927 - val_loss: 0.3203 - val_accuracy: 0.8841 - 6s/epoch - 4ms/step\n",
      "Epoch 18/50\n",
      "1499/1499 - 6s - loss: 0.2963 - accuracy: 0.8918 - val_loss: 0.3247 - val_accuracy: 0.8828 - 6s/epoch - 4ms/step\n",
      "Epoch 19/50\n",
      "1499/1499 - 6s - loss: 0.2953 - accuracy: 0.8919 - val_loss: 0.3120 - val_accuracy: 0.8850 - 6s/epoch - 4ms/step\n",
      "Epoch 20/50\n",
      "1499/1499 - 6s - loss: 0.2884 - accuracy: 0.8927 - val_loss: 0.3162 - val_accuracy: 0.8817 - 6s/epoch - 4ms/step\n",
      "Epoch 21/50\n",
      "1499/1499 - 6s - loss: 0.2875 - accuracy: 0.8947 - val_loss: 0.3251 - val_accuracy: 0.8800 - 6s/epoch - 4ms/step\n",
      "Epoch 22/50\n",
      "1499/1499 - 6s - loss: 0.2900 - accuracy: 0.8925 - val_loss: 0.3203 - val_accuracy: 0.8819 - 6s/epoch - 4ms/step\n",
      "Epoch 23/50\n",
      "1499/1499 - 6s - loss: 0.2892 - accuracy: 0.8947 - val_loss: 0.3249 - val_accuracy: 0.8816 - 6s/epoch - 4ms/step\n",
      "Epoch 24/50\n",
      "1499/1499 - 6s - loss: 0.2866 - accuracy: 0.8940 - val_loss: 0.3227 - val_accuracy: 0.8824 - 6s/epoch - 4ms/step\n",
      "Epoch 25/50\n",
      "1499/1499 - 6s - loss: 0.2907 - accuracy: 0.8933 - val_loss: 0.3185 - val_accuracy: 0.8788 - 6s/epoch - 4ms/step\n",
      "Epoch 26/50\n",
      "1499/1499 - 6s - loss: 0.2840 - accuracy: 0.8948 - val_loss: 0.3158 - val_accuracy: 0.8856 - 6s/epoch - 4ms/step\n",
      "Epoch 27/50\n",
      "1499/1499 - 6s - loss: 0.2819 - accuracy: 0.8959 - val_loss: 0.3261 - val_accuracy: 0.8807 - 6s/epoch - 4ms/step\n",
      "Epoch 28/50\n",
      "1499/1499 - 6s - loss: 0.2834 - accuracy: 0.8956 - val_loss: 0.3140 - val_accuracy: 0.8838 - 6s/epoch - 4ms/step\n",
      "Epoch 29/50\n",
      "1499/1499 - 6s - loss: 0.2844 - accuracy: 0.8954 - val_loss: 0.3170 - val_accuracy: 0.8851 - 6s/epoch - 4ms/step\n",
      "Epoch 30/50\n",
      "1499/1499 - 6s - loss: 0.2790 - accuracy: 0.8973 - val_loss: 0.3154 - val_accuracy: 0.8865 - 6s/epoch - 4ms/step\n",
      "Epoch 31/50\n",
      "1499/1499 - 6s - loss: 0.2790 - accuracy: 0.8971 - val_loss: 0.3155 - val_accuracy: 0.8839 - 6s/epoch - 4ms/step\n",
      "Epoch 32/50\n",
      "1499/1499 - 6s - loss: 0.2774 - accuracy: 0.8976 - val_loss: 0.3155 - val_accuracy: 0.8853 - 6s/epoch - 4ms/step\n",
      "Epoch 33/50\n",
      "1499/1499 - 6s - loss: 0.2791 - accuracy: 0.8965 - val_loss: 0.3169 - val_accuracy: 0.8858 - 6s/epoch - 4ms/step\n",
      "Epoch 34/50\n",
      "1499/1499 - 6s - loss: 0.2807 - accuracy: 0.8955 - val_loss: 0.3213 - val_accuracy: 0.8809 - 6s/epoch - 4ms/step\n",
      "Epoch 35/50\n",
      "1499/1499 - 6s - loss: 0.2778 - accuracy: 0.8966 - val_loss: 0.3197 - val_accuracy: 0.8809 - 6s/epoch - 4ms/step\n",
      "Epoch 36/50\n",
      "1499/1499 - 6s - loss: 0.2762 - accuracy: 0.8966 - val_loss: 0.3135 - val_accuracy: 0.8849 - 6s/epoch - 4ms/step\n",
      "Epoch 37/50\n",
      "1499/1499 - 6s - loss: 0.2752 - accuracy: 0.8981 - val_loss: 0.3130 - val_accuracy: 0.8856 - 6s/epoch - 4ms/step\n",
      "Epoch 38/50\n",
      "1499/1499 - 6s - loss: 0.2749 - accuracy: 0.8984 - val_loss: 0.3064 - val_accuracy: 0.8895 - 6s/epoch - 4ms/step\n",
      "Epoch 39/50\n",
      "1499/1499 - 6s - loss: 0.2749 - accuracy: 0.8990 - val_loss: 0.3180 - val_accuracy: 0.8841 - 6s/epoch - 4ms/step\n",
      "Epoch 40/50\n",
      "1499/1499 - 6s - loss: 0.2725 - accuracy: 0.9001 - val_loss: 0.3169 - val_accuracy: 0.8830 - 6s/epoch - 4ms/step\n",
      "Epoch 41/50\n",
      "1499/1499 - 6s - loss: 0.2698 - accuracy: 0.8997 - val_loss: 0.3195 - val_accuracy: 0.8843 - 6s/epoch - 4ms/step\n",
      "Epoch 42/50\n",
      "1499/1499 - 6s - loss: 0.2712 - accuracy: 0.8999 - val_loss: 0.3169 - val_accuracy: 0.8859 - 6s/epoch - 4ms/step\n",
      "Epoch 43/50\n",
      "1499/1499 - 6s - loss: 0.2708 - accuracy: 0.8994 - val_loss: 0.3126 - val_accuracy: 0.8827 - 6s/epoch - 4ms/step\n",
      "Epoch 44/50\n",
      "1499/1499 - 6s - loss: 0.2698 - accuracy: 0.8996 - val_loss: 0.3067 - val_accuracy: 0.8868 - 6s/epoch - 4ms/step\n",
      "Epoch 45/50\n",
      "1499/1499 - 6s - loss: 0.2691 - accuracy: 0.9008 - val_loss: 0.3137 - val_accuracy: 0.8882 - 6s/epoch - 4ms/step\n",
      "Epoch 46/50\n",
      "1499/1499 - 6s - loss: 0.2667 - accuracy: 0.9002 - val_loss: 0.3139 - val_accuracy: 0.8859 - 6s/epoch - 4ms/step\n",
      "Epoch 47/50\n",
      "1499/1499 - 6s - loss: 0.2691 - accuracy: 0.8998 - val_loss: 0.3104 - val_accuracy: 0.8881 - 6s/epoch - 4ms/step\n",
      "Epoch 48/50\n",
      "1499/1499 - 6s - loss: 0.2679 - accuracy: 0.9008 - val_loss: 0.3081 - val_accuracy: 0.8869 - 6s/epoch - 4ms/step\n",
      "Epoch 49/50\n",
      "1499/1499 - 6s - loss: 0.2676 - accuracy: 0.9014 - val_loss: 0.3088 - val_accuracy: 0.8871 - 6s/epoch - 4ms/step\n",
      "Epoch 50/50\n",
      "1499/1499 - 6s - loss: 0.2678 - accuracy: 0.8997 - val_loss: 0.3075 - val_accuracy: 0.8903 - 6s/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train_encoded,epochs = 50,batch_size = 32,callbacks=[early_stop],validation_data=(x_cv,y_cv_encoded),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L2Hm5FEAHWgK",
   "metadata": {
    "id": "L2Hm5FEAHWgK"
   },
   "source": [
    "## results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "RL1p2pFcHbv3",
   "metadata": {
    "id": "RL1p2pFcHbv3"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "SLSQez_EIOFW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "SLSQez_EIOFW",
    "outputId": "9d808fdb-c74e-4f8d-e155-cb3e6ccca664"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-deed04b3-ed87-4a7d-9bd6-a34a3a5c3416\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deed04b3-ed87-4a7d-9bd6-a34a3a5c3416')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-deed04b3-ed87-4a7d-9bd6-a34a3a5c3416 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-deed04b3-ed87-4a7d-9bd6-a34a3a5c3416');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       9   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0      14      53   \n",
       "3      2       0       0       0       0       0       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       8  ...       103        87        56         0         0         0   \n",
       "1       0  ...        34         0         0         0         0         0   \n",
       "2      99  ...         0         0         0         0        63        53   \n",
       "3       0  ...       137       126       140         0       133       224   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2        31         0         0         0  \n",
       "3       222        56         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "AUKc4AknMM4V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUKc4AknMM4V",
    "outputId": "9111c851-a312-4e2e-bd3e-07c25c0cc08f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "VHe4j-LJMQu9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHe4j-LJMQu9",
    "outputId": "20ddd72f-4e00-4c87-a7d9-f318a5009953"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 8, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = df_test['label']\n",
    "Y_test = Y_test.to_numpy()\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "Y0WE1oIBNhYO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0WE1oIBNhYO",
    "outputId": "415d7e36-ed17-43b0-da7d-9bb05d94370d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 3, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df_test[pixels]\n",
    "X_test = X_test.to_numpy()\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4AMpNSPNNoiQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AMpNSPNNoiQ",
    "outputId": "dd0e9542-73bc-417a-9147-5c922fd7181a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000,))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3AelnTCmNv_N",
   "metadata": {
    "id": "3AelnTCmNv_N"
   },
   "outputs": [],
   "source": [
    "m_test = X_test.shape[0]\n",
    "x_test = X_test.reshape(m_test,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "s2esMo5pN86j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2esMo5pN86j",
    "outputId": "d38ba61b-9b33-42e8-d063-05d9658d2468"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "oQN_MtXvOAi_",
   "metadata": {
    "id": "oQN_MtXvOAi_"
   },
   "outputs": [],
   "source": [
    "x_test = np.expand_dims(x_test.astype('float32'), -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "Dqg9RgInOEFQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dqg9RgInOEFQ",
    "outputId": "2bb4f0cc-60d6-4a89-e190-d2f87c5bc562"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "-iN_3dweOEY8",
   "metadata": {
    "id": "-iN_3dweOEY8"
   },
   "outputs": [],
   "source": [
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "lRZCLpSFOEl5",
   "metadata": {
    "id": "lRZCLpSFOEl5"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_test_encoded = np_utils.to_categorical(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "Yj8m9z--OUdu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yj8m9z--OUdu",
    "outputId": "b06f35a3-18a7-4aed-d9bb-f4630c34d302"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "21SePIngObtF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21SePIngObtF",
    "outputId": "9b98cfbf-af1d-46d5-e347-02a024fa12c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2979 - accuracy: 0.8911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.297888845205307, 0.8910999894142151]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZfAqEX8ZOjxa",
   "metadata": {
    "id": "ZfAqEX8ZOjxa"
   },
   "source": [
    "# comment \n",
    "- we see that the accuracy of the LeNet5 model is barely overcoming 90%\n",
    "- when grid search was used, it turned out there were modifications that led to better performance (like using relu instead of tanh)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
